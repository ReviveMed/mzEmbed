{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from  study_alignment.utils import get_dropbox_dir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "\n",
    "from ml.prep import SimpleDataset\n",
    "\n",
    "from study_alignment.standardize import fill_na_by_cohort, standardize_across_cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dropbox_dir = get_dropbox_dir()\n",
    "base_dir = os.path.join(dropbox_dir, 'development_CohortCombination','alignment_RCC_2024_Feb_27')\n",
    "data_dir = os.path.join(base_dir, 'alignment_id_36', 'grid_search_index_1')\n",
    "# rcc_metadata_file = os.path.join(dropbox_dir, 'development_CohortCombination','clean_rcc_metadata.csv')\n",
    "subset_id = \"subset_robust_Freq, Cohort Log Size Weighted_0.2_rem_['549', '551', '547']_recompute\"\n",
    "matt_ft_dir = os.path.join(base_dir, 'matt_top_fts')\n",
    "\n",
    "subset_dir = os.path.join(data_dir, subset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the key features\n",
    "def get_key_ft_dct(ref_freq=0.6,matt_ft_dir=matt_ft_dir):\n",
    "\n",
    "    matt_ft_files = os.listdir(matt_ft_dir)\n",
    "    matt_ft_files = [f for f in matt_ft_files if f.endswith('.txt')]\n",
    "\n",
    "    matt_ft_dict = {}\n",
    "    for f in matt_ft_files:\n",
    "        ft_name = f.split('_feats')[0]\n",
    "        # with open(os.path.join(matt_ft_dir, f), 'r') as file:\n",
    "        #     ft = file.read().split(', ')\n",
    "        # if len(ft) == 1:\n",
    "        with open(os.path.join(matt_ft_dir, f), 'r') as file:\n",
    "            ft = file.read().splitlines()\n",
    "            # print(file.read()\n",
    "        # remove all of the ', and commas from the strings in the list\n",
    "        ft = [x.strip(',').strip(' ').strip('\"').strip(\"'\").strip('\\n').strip('\\t') for x in ft]\n",
    "        matt_ft_dict[ft_name] = ft\n",
    "        # break\n",
    "        print(ft_name + ': ' + str(len(ft)))\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ### RCC Target Metabolites\n",
    "\n",
    "    # %%\n",
    "    # %%\n",
    "    rcc_peak_info_file = os.path.join(base_dir, 'rcc_result', 'peak_info.csv')\n",
    "    rcc_peak_info_df = pd.read_csv(rcc_peak_info_file, index_col=0)\n",
    "\n",
    "    rcc_peak_info_df = rcc_peak_info_df[rcc_peak_info_df['freq'] >= ref_freq].copy()\n",
    "\n",
    "    print(f'Number of peaks in the reference cohort after {ref_freq} filter: ', rcc_peak_info_df.shape[0])\n",
    "\n",
    "        \n",
    "    rcc_matched_targets_file = os.path.join(base_dir,'rcc_result', 'matched_targets HILIC POSITIVE ION MODE.csv')\n",
    "    rcc_matched_targets_df = pd.read_csv(rcc_matched_targets_file, index_col=0)\n",
    "    rcc_matched_targets_df.loc[rcc_peak_info_df.index]\n",
    "\n",
    "    potential_feats = rcc_matched_targets_df[rcc_matched_targets_df['potential_target']].index.to_list()\n",
    "    print('Number of features that potentially match to a target metabolite: ', len(potential_feats))\n",
    "\n",
    "    double_match_ids = rcc_matched_targets_df[rcc_matched_targets_df['potential_target_count'] > 1]\n",
    "    num_double_match = double_match_ids.shape[0]\n",
    "    print('Number of features that potentially match to more than one target metabolite: ', double_match_ids.shape[0])\n",
    "    print(rcc_matched_targets_df.loc[double_match_ids.index, 'potential_target_id'])\n",
    "\n",
    "    # here are the double matches in RCC, two are the same metabolite (but different adducts?)\n",
    "    # FT3202                           tryptophanTryptophan_μM\n",
    "    # FT3237                           kynurenineKynurenine_μM\n",
    "    # FT8451    C18:1 LPC plasmalogen_AC18:1 LPC plasmalogen_B\n",
    "\n",
    "\n",
    "    potential_feat_names = rcc_matched_targets_df.loc[potential_feats]['potential_target_id'].unique()\n",
    "    # print('Number of potential feature names: ', len(potential_feat_names))\n",
    "    print(potential_feat_names)\n",
    "\n",
    "    print('Number of target metabolite captured: ', len(potential_feat_names))\n",
    "\n",
    "    # for now don't remove the double counts, since they are NOT actually double counts\n",
    "    num_rcc_targets_found =  len(potential_feat_names)\n",
    "    rcc_target_feats = potential_feats\n",
    "\n",
    "    # add to the matt ft dictionary\n",
    "    matt_ft_dict['rcc_targets'] = rcc_target_feats\n",
    "\n",
    "    return matt_ft_dict, rcc_peak_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_10: 10\n",
      "top_25: 25\n",
      "168_os_pfs: 168\n",
      "net_matched: 86\n",
      "Number of peaks in the reference cohort after 0.6 filter:  4016\n",
      "Number of features that potentially match to a target metabolite:  188\n",
      "Number of features that potentially match to more than one target metabolite:  3\n",
      "feats\n",
      "FT3202                           tryptophanTryptophan_μM\n",
      "FT3237                           kynurenineKynurenine_μM\n",
      "FT8451    C18:1 LPC plasmalogen_AC18:1 LPC plasmalogen_B\n",
      "Name: potential_target_id, dtype: object\n",
      "['trimethylamine-N-oxide' 'alanine' 'GABA' 'serine' 'hypotaurine'\n",
      " 'cytosine' 'creatinine' 'betaine' 'threonine' 'niacinamide' 'taurine'\n",
      " 'ornithine' 'N-acetylalanine' 'N-carbamoyl-beta-alanine'\n",
      " 'N-methylproline' 'leucine' 'hydroxyproline' 'N-acetylputrescine'\n",
      " '1-methylnicotinamide' 'trigonelline' 'anthranilic acid' 'urocanic acid'\n",
      " 'imidazole propionate' 'ectoine' 'proline-betaine' 'glutamate'\n",
      " '4-acetamidobutanoate' 'butyrobetaine' 'glutamine' 'lysine' 'methionine'\n",
      " 'N1-methyl-2-pyridone-5-carboxamide' '4-hydroxy-3-methylacetophenone'\n",
      " 'guanine' 'allantoin' 'carnitine' '2-aminooctanoate' 'urate'\n",
      " '7-methylguanine' '3-methylxanthine' 'phenylalanine' 'N-acetylleucine'\n",
      " '3-methylhistidine' 'serotonin' 'citrulline' 'N6,N6-dimethyllysine'\n",
      " 'N-acetylornithine' 'cotinine' '5-hydroxytryptophol' 'arginine'\n",
      " 'tyrosine' 'hippurate' '4-pyridoxate' 'N6-acetyllysine'\n",
      " 'N6,N6,N6-trimethyllysine' 'N1-acetylspermidine' 'NMMA' 'kynurenic acid'\n",
      " 'N-acetylmethionine' 'hydroxycotinine' 'caffeine' '4-hydroxyhippurate'\n",
      " '1,7-dimethyluric acid' 'DMGV' 'SDMA' 'C2 carnitine'\n",
      " 'tryptophanTryptophan_μM' 'kynurenineKynurenine_μM' 'pantothenol'\n",
      " 'cinnamoylglycine' 'N-alpha-acetylarginine' 'C3 carnitine'\n",
      " 'acetyl-galactosamine' '5-hydroxytryptophan' 'pantothenate'\n",
      " 'myristoleate' 'C4 carnitine' 'C5:1 carnitine' 'C5 carnitine'\n",
      " 'C4-OH carnitine' 'N-acetyltryptophan' 'pseudouridine' 'ribothymidine'\n",
      " 'alpha-glycerophosphocholine' 'C3-DC-CH3 carnitine' 'C6 carnitine'\n",
      " 'thiamine' 'inosine' 'atenolol' 'metoprolol' 'phenylacetylglutamine'\n",
      " 'C5-DC carnitine' '1-methyladenosine' 'diacetylspermine'\n",
      " 'N4-acetylcytidine' 'C8 carnitine' 'piperine' '1-methylguanosine'\n",
      " 'C9 carnitine' 'sphinganine' 'threo-sphingosine' 'warfarin'\n",
      " '3-(N-acetyl-L-cystein-S-yl) acetaminophen' 'C10 carnitine'\n",
      " 'linoleoyl ethanolamide' 'acetaminophen glucuronide' 'C12 carnitine'\n",
      " 'C12:1 carnitine' 'oleoyl glycine' 'cortisol' 'cortisone'\n",
      " 'C14:1 carnitine' 'C14 carnitine' 'C16 carnitine' 'C18:2 carnitine'\n",
      " 'C18 carnitine' 'C18:1 carnitine' 'valsartan' 'C20:4 carnitine'\n",
      " 'glycodeoxycholate/glycochenodeoxycholate' 'C16:0 LPE' 'C14:0 LPC'\n",
      " 'glycocholate' 'C18:2 LPE' 'C18:1 LPE' 'C18:0 LPE'\n",
      " 'C16:1 LPC plasmalogen' 'C16:1 LPC' 'C16:0 LPC' 'C20:4 LPE'\n",
      " 'C18:1 LPC plasmalogen_B'\n",
      " 'C18:1 LPC plasmalogen_AC18:1 LPC plasmalogen_B' 'C20:0 LPE' 'C18:3 LPC'\n",
      " 'C18:0 LPC' 'C18:1 LPC' 'C22:6 LPE' 'C16:0 ceramide (d18:1)'\n",
      " 'C26 carnitine' 'C20:4 LPC' 'C20:5 LPC' 'C22:6 LPC' 'C22:5 LPC'\n",
      " 'biliverdin' 'bilirubin' 'urobilinogen' 'C34:1 DAG' 'C34:2 DAG'\n",
      " 'C34:1 DAG*' 'C24:1 ceramide (d18:1)' 'C14:0 SM' 'C34:3 PE plasmalogen'\n",
      " 'C34:2 PE plasmalogen' 'C16:0 SM' 'C16:1 SM' 'C30:0 PC' 'C34:2 PE'\n",
      " 'C36:5 PE plasmalogen' 'C34:0 PE' 'C18:1 SM' 'C36:3 PE plasmalogen'\n",
      " 'C32:2 PC' 'C18:0 SM' 'C34:3 PC plasmalogen' 'C36:4 PE' 'C36:2 PE'\n",
      " 'C38:7 PE plasmalogen' 'C38:5 PE plasmalogen' 'C38:6 PE plasmalogen'\n",
      " 'C34:4 PC' 'C20:0 SM' 'C34:3 PC' 'C38:6 PE' 'C38:4 PE'\n",
      " 'C36:5 PC plasmalogen' 'C36:2 PS plasmalogen' 'thyroxine'\n",
      " 'C40:7 PE plasmalogen' 'C36:2 PC' 'C38:6 PC plasmalogen' 'C40:6 PE'\n",
      " 'C38:7 PC plasmalogen']\n",
      "Number of target metabolite captured:  182\n"
     ]
    }
   ],
   "source": [
    "matt_ft_dict, rcc_peak_info_df = get_key_ft_dct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_str = 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcc_metadata_file = os.path.join(dropbox_dir, 'development_CohortCombination','clean_rcc_metadata_encoded.csv')\n",
    "rcc_metadata_file = os.path.join(dropbox_dir, 'development_CohortCombination','alignment_RCC_2024_Feb_27/rcc_result','clean_metadata_encoded.csv')\n",
    "rcc_metadata = pd.read_csv(rcc_metadata_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set\n",
       "Pretrain      903\n",
       "Finetune      449\n",
       "Test          149\n",
       "Validation    143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcc_metadata['Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_corrected = pd.read_csv(os.path.join(data_dir, 'subset_data_corrected.csv'), index_col=0)\n",
    "# nan_mask = pd.read_csv(os.path.join(data_dir, 'subset_nan_mask.csv'), index_col=0)\n",
    "\n",
    "\n",
    "# metadata_df = pd.read_csv(os.path.join(data_dir, 'metadata_df.csv'), index_col=0)\n",
    "X_data = pd.read_csv(os.path.join(subset_dir, 'X.csv'), index_col=0)\n",
    "nan_mask = pd.read_csv(os.path.join(subset_dir, 'nans.csv'), index_col=0)\n",
    "y_data = pd.read_csv(os.path.join(subset_dir, 'y.csv'), index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous age group was wrong\n",
    "# y_data.drop('Age_Group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Matt Set', 'Set']\n"
     ]
    }
   ],
   "source": [
    "y_data.drop(['Set','Matt Set'], axis=1, inplace=True)\n",
    "missing_y_cols = [c for c in rcc_metadata.columns if c not in y_data.columns]\n",
    "print(missing_y_cols)\n",
    "\n",
    "y_data = pd.concat([y_data, rcc_metadata[missing_y_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data['Set'].fillna(\"Pretrain\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set\n",
       "Pretrain      16944\n",
       "Finetune        449\n",
       "Test            149\n",
       "Validation      143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data['Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.to_csv(os.path.join(subset_dir, 'y.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  17685\n",
      "number of features:  2736\n"
     ]
    }
   ],
   "source": [
    "print('number of samples: ', X_data.shape[0])\n",
    "print('number of features: ', X_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set\n",
       "Pretrain      16944\n",
       "Finetune        449\n",
       "Test            149\n",
       "Validation      143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data['Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of top_10 captured: 1.0\n",
      "Percentage of top_25 captured: 0.92\n",
      "Percentage of 168_os_pfs captured: 0.7976190476190477\n",
      "Percentage of net_matched captured: 0.8488372093023255\n",
      "Percentage of rcc_targets captured: 0.8457446808510638\n"
     ]
    }
   ],
   "source": [
    "# check how many of the key features are missing\n",
    "kept_fts = X_data.columns.to_list()\n",
    "\n",
    "for key_name, key_feat in matt_ft_dict.items():\n",
    "    captured_fts = [ft for ft in key_feat if ft in kept_fts]\n",
    "    perct_captured = len(captured_fts)/len(key_feat)\n",
    "    print(f'Percentage of {key_name} captured: {perct_captured}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_files = y_data[y_data['Set']=='Pretrain'].index.to_list()\n",
    "finetune_files = y_data[y_data['Set']=='Finetune'].index.to_list()\n",
    "holdout_test_files = y_data[y_data['Set']=='Test'].index.to_list()\n",
    "holdout_val_files = y_data[y_data['Set']=='Validation'].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Split the Pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf_params = {\n",
    "        'n_splits': 5,\n",
    "        'n_repeats': 2,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "stratify_col = 'Cohort ID Expanded'\n",
    "\n",
    "save_dir = os.path.join(subset_dir, f'{stratify_col} pretrain_folds')\n",
    "os.makedirs(save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = RepeatedStratifiedKFold(**rskf_params)\n",
    "\n",
    "rskf_list = []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(rskf.split(pretrain_files, y_data.loc[pretrain_files,stratify_col])):\n",
    "    test_bool = np.zeros(len(pretrain_files), dtype=bool)\n",
    "    test_bool[test_idx] = True\n",
    "    rskf_list.append(test_bool)\n",
    "\n",
    "rskf_df = pd.DataFrame(index=pretrain_files)\n",
    "for i, test_idx in enumerate(rskf_list):\n",
    "    rskf_df[f'fold_{i}'] = test_idx\n",
    "\n",
    "rskf_df.to_csv(os.path.join(save_dir, 'splits.csv'))\n",
    "\n",
    "rskf_info = {\n",
    "    'rskf_params': rskf_params,\n",
    "    'stratify_col': stratify_col,\n",
    "    'desc_str': desc_str,\n",
    "    'size': rskf_df.shape[0],\n",
    "    'Train Bool' : False,\n",
    "    'Test Bool' : True,\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, 'splits_info.json'), 'w') as f:\n",
    "    json.dump(rskf_info, f, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Split the Finetune data, using single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf_params = {\n",
    "        'n_splits': 5,\n",
    "        'n_repeats': 10,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "\n",
    "metadata_subset = y_data.loc[finetune_files].copy()\n",
    "yes_remove_nans = True\n",
    "\n",
    "# stratify_col = 'Nivo Benefit BINARY'\n",
    "stratify_col = 'MSKCC BINARY'\n",
    "\n",
    "save_dir = os.path.join(subset_dir, f'{stratify_col} finetune_folds')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_subset[stratify_col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = RepeatedStratifiedKFold(**rskf_params)\n",
    "\n",
    "rskf_list = []\n",
    "\n",
    "# check for nans\n",
    "if metadata_subset[stratify_col].isna().any():\n",
    "    if yes_remove_nans:\n",
    "        metadata_subset = metadata_subset[~metadata_subset[stratify_col].isna()]\n",
    "    else:\n",
    "        print('There are nans in the stratify column')\n",
    "        fill_val = metadata_subset[stratify_col].max() + 1\n",
    "        metadata_subset[stratify_col] = metadata_subset[stratify_col].fillna(fill_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(rskf.split(metadata_subset.index, metadata_subset[stratify_col])):\n",
    "\n",
    "    test_bool = np.zeros(metadata_subset.shape[0], dtype=bool)\n",
    "    test_bool[test_idx] = True\n",
    "    rskf_list.append(test_bool)\n",
    "\n",
    "rskf_df = pd.DataFrame(index=metadata_subset.index)\n",
    "for i, test_idx in enumerate(rskf_list):\n",
    "    rskf_df[f'fold_{i}'] = test_idx\n",
    "\n",
    "rskf_df.to_csv(os.path.join(save_dir, 'splits.csv'))\n",
    "\n",
    "rskf_info = {\n",
    "    'rskf_params': rskf_params,\n",
    "    'stratify_col': stratify_col,\n",
    "    'desc_str': desc_str,\n",
    "    'size': rskf_df.shape[0],\n",
    "    'Train Bool' : False,\n",
    "    'Test Bool' : True,\n",
    "    'remove nans': yes_remove_nans\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, 'splits_info.json'), 'w') as f:\n",
    "    json.dump(rskf_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Split the Finetune data, using multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf_params = {\n",
    "        'n_splits': 5,\n",
    "        'n_repeats': 10,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "metadata_subset = y_data.loc[finetune_files].copy()\n",
    "\n",
    "# stratify_cols =['MSKCC','Treatment','Benefit']\n",
    "# stratify_cols =['MSKCC','Benefit']\n",
    "stratify_cols =['Treatment','Benefit']\n",
    "\n",
    "stratify_col = 'multiple_categories'\n",
    "metadata_subset[stratify_col] = metadata_subset[stratify_cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "save_dir = os.path.join(subset_dir, f'{stratify_cols} finetune_folds')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiple_categories\n",
       "EVEROLIMUS_ICB    97\n",
       "NIVOLUMAB_ICB     86\n",
       "NIVOLUMAB_NCB     80\n",
       "NIVOLUMAB_CB      76\n",
       "EVEROLIMUS_NCB    55\n",
       "EVEROLIMUS_CB     55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_subset['multiple_categories'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = RepeatedStratifiedKFold(**rskf_params)\n",
    "\n",
    "rskf_list = []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(rskf.split(metadata_subset.index, metadata_subset[stratify_col])):\n",
    "\n",
    "    test_bool = np.zeros(len(metadata_subset), dtype=bool)\n",
    "    test_bool[test_idx] = True\n",
    "    rskf_list.append(test_bool)\n",
    "\n",
    "rskf_df = pd.DataFrame(index=metadata_subset.index)\n",
    "for i, test_idx in enumerate(rskf_list):\n",
    "    rskf_df[f'fold_{i}'] = test_idx\n",
    "\n",
    "rskf_df.to_csv(os.path.join(save_dir, 'splits.csv'))\n",
    "\n",
    "rskf_info = {\n",
    "    'rskf_params': rskf_params,\n",
    "    'stratify_col': stratify_col,\n",
    "    'desc_str': desc_str,\n",
    "    'size': rskf_df.shape[0],\n",
    "    'Train Bool' : False,\n",
    "    'Test Bool' : True,\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, 'splits_info.json'), 'w') as f:\n",
    "    json.dump(rskf_info, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classical Models on the desired tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/alignment_RCC_2024_Feb_27/alignment_id_36/grid_search_index_1/subset_robust_Freq, Cohort Log Size Weighted_0.2_rem_['549', '551', '547']_recompute\n"
     ]
    }
   ],
   "source": [
    "print(subset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nivo Benefit BINARY finetune_folds', 'MSKCC BINARY finetune_folds', \"['MSKCC', 'Benefit'] finetune_folds\", 'Cohort ID Expanded pretrain_folds', \"['Treatment', 'Benefit'] finetune_folds\", \"['MSKCC', 'Treatment', 'Benefit'] finetune_folds\", 'x_finetune_folds']\n"
     ]
    }
   ],
   "source": [
    "# what are the available splits?\n",
    "\n",
    "subset_dir_files = os.listdir(subset_dir)\n",
    "split_dirs = [f for f in subset_dir_files if '_folds' in f]\n",
    "print(split_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.sklearn_models import run_train_sklearn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc3_baseline = y_data.loc[finetune_files].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSKCC BINARY\n",
      "{}\n",
      "/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/alignment_RCC_2024_Feb_27/alignment_id_36/grid_search_index_1/subset_robust_Freq, Cohort Log Size Weighted_0.2_rem_['549', '551', '547']_recompute/MSKCC BINARY finetune_folds/MSKCC BINARY\n"
     ]
    }
   ],
   "source": [
    "# desc_str =['MSKCC','Treatment','Benefit']\n",
    "# desc_str = 'Nivo Benefit BINARY'\n",
    "desc_str = 'MSKCC BINARY'\n",
    "\n",
    "save_dir = os.path.join(subset_dir, f'{desc_str} finetune_folds')\n",
    "\n",
    "# finetune_label_col = 'Benefit'\n",
    "# finetune_label_mapper  = {'CB': 1, 'NCB': 0, 'ICB': np.nan}\n",
    "# finetune_filter = rcc3_baseline[rcc3_baseline['Treatment'].isin(['NIVOLUMAB'])].index.to_list()\n",
    "\n",
    "\n",
    "# finetune_label_col = 'MSKCC'\n",
    "# finetune_label_mapper  = {'FAVORABLE': 1, 'POOR': 0, 'INTERMEDIATE': np.nan}\n",
    "# finetune_filter = rcc3_baseline.index.to_list()\n",
    "\n",
    "# finetune_label_col = 'Nivo Benefit BINARY'\n",
    "# finetune_label_mapper = {}\n",
    "# finetune_filter = []\n",
    "\n",
    "finetune_label_col = 'MSKCC BINARY'\n",
    "finetune_label_mapper = {}\n",
    "finetune_filter = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(finetune_label_col)\n",
    "print(finetune_label_mapper)\n",
    "\n",
    "\n",
    "task_dir = os.path.join(save_dir, finetune_label_col)\n",
    "\n",
    "splits_df = pd.read_csv(os.path.join(save_dir, 'splits.csv'), index_col=0)\n",
    "if len(finetune_filter) > 0:\n",
    "    print('number of samples after the finetune filter:' , len(finetune_filter))\n",
    "    splits_df = splits_df.loc[finetune_filter].copy()\n",
    "\n",
    "X = X_data.loc[splits_df.index]\n",
    "y = y_data.loc[splits_df.index]\n",
    "nans = nan_mask.loc[splits_df.index].astype(bool)\n",
    "\n",
    "n_folds = splits_df.shape[1]\n",
    "yes_dropna = True\n",
    "\n",
    "task_info = {\n",
    "    'label_col': finetune_label_col,\n",
    "    'label_mapper': finetune_label_mapper,\n",
    "    'filter': finetune_filter,\n",
    "    'desc_str': desc_str,\n",
    "    'size': splits_df.shape[0],\n",
    "    'folds': splits_df.shape[1],\n",
    "    'dropna': yes_dropna,\n",
    "}\n",
    "\n",
    "if finetune_label_mapper:\n",
    "    task_info['size by label'] =  y[finetune_label_col].map(finetune_label_mapper).value_counts().to_dict(),\n",
    "else:\n",
    "    task_info['size by label'] =  y[finetune_label_col].value_counts().to_dict()\n",
    "\n",
    "os.makedirs(task_dir, exist_ok=True)\n",
    "with open(os.path.join(task_dir, 'task_info.json'), 'w') as f:\n",
    "    json.dump(task_info, f, indent=4)\n",
    "\n",
    "print(task_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping nan values in the y column\n",
      "number of samples after dropping nan values in the y column:  240\n",
      "svc_default_summary.csv\n",
      "logistic_regression_default_summary.csv\n",
      "random_forest_default_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = os.path.join(task_dir, 'classical_models')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "if finetune_label_mapper:\n",
    "    y_values = y[finetune_label_col].map(finetune_label_mapper)\n",
    "else:\n",
    "    y_values = y[finetune_label_col]\n",
    "    \n",
    "if yes_dropna:\n",
    "    print('dropping nan values in the y column')\n",
    "    y_values = y_values.dropna()\n",
    "    X = X.loc[y_values.index]\n",
    "    splits_df = splits_df.loc[y_values.index]\n",
    "\n",
    "    print('number of samples after dropping nan values in the y column: ', y_values.shape[0])    \n",
    "\n",
    "for model_kind in ['logistic_regression', 'random_forest', 'svc']:\n",
    "    gather_output = []\n",
    "    model_name = model_kind + '_default'\n",
    "    output_summary_file = os.path.join(output_dir, f'{model_name}_summary.csv')\n",
    "    for subset_id in range(n_folds):\n",
    "\n",
    "\n",
    "\n",
    "        train_idx = splits_df.index[splits_df[f'fold_{subset_id}'] == False]\n",
    "        test_idx = splits_df.index[splits_df[f'fold_{subset_id}'] == True]\n",
    "        finetune_dataset = SimpleDataset(X.loc[train_idx], y_values.loc[train_idx])\n",
    "        \n",
    "        class_weights = 1 / torch.bincount(finetune_dataset.y.long())\n",
    "\n",
    "        test_dataset = SimpleDataset(X.loc[test_idx], y_values.loc[test_idx])\n",
    "\n",
    "        data_dict = {'train': finetune_dataset, 'test': test_dataset}\n",
    "\n",
    "        output_data = run_train_sklearn_model(data_dict,output_dir,\n",
    "                                model_kind = model_kind,\n",
    "                                model_name=f'{model_name}_{subset_id}',\n",
    "                                # model_name=model_name,\n",
    "                                param_grid={})\n",
    "        \n",
    "\n",
    "        gather_output.append(output_data)\n",
    "\n",
    "    ## Summarize the important results\n",
    "    # best_epoch_list = [output['best_epoch'] for output in gather_output]\n",
    "    val_auroc_list = [output['end_state_auroc']['test'] for output in gather_output]\n",
    "    train_auroc_list = [output['end_state_auroc']['train'] for output in gather_output]\n",
    "    # model_name = gather_output[0]['model_name']\n",
    "    auc_summary = pd.DataFrame({\n",
    "                    'train_auroc': train_auroc_list,\n",
    "                    'test_auroc': val_auroc_list,})\n",
    "\n",
    "\n",
    "    result_summary_avg = auc_summary.mean()\n",
    "    result_summary_std = auc_summary.std()\n",
    "    result_summary_avg.index = [f'AVG {col}' for col in result_summary_avg.index]\n",
    "    result_summary_std.index = [f'STD {col}' for col in result_summary_std.index]\n",
    "    result_summary = pd.concat([result_summary_avg, result_summary_std])\n",
    "    result_summary.to_csv(output_summary_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_files = os.listdir(output_dir)\n",
    "output_summary_files = [f for f in output_files if f.endswith('summary.csv')]\n",
    "other_files = [f for f in output_files if f not in output_summary_files]\n",
    "\n",
    "all_res = []\n",
    "\n",
    "for f in output_summary_files:\n",
    "    print(f)\n",
    "    model_name = f.split('_summary.csv')[0]\n",
    "    res = pd.read_csv(os.path.join(output_dir, f), index_col=0)\n",
    "    res.columns = [model_name]\n",
    "    all_res.append(res)\n",
    "    \n",
    "# delete the other files to save room\n",
    "for f in other_files:\n",
    "    os.remove(os.path.join(output_dir, f))    \n",
    "\n",
    "res_summary = pd.concat(all_res, axis=1)    \n",
    "res_summary = res_summary.round(4)\n",
    "res_summary.to_csv(os.path.join(task_dir, 'classical_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svc_default</th>\n",
       "      <th>logistic_regression_default</th>\n",
       "      <th>random_forest_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AVG train_auroc</th>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG test_auroc</th>\n",
       "      <td>0.902</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD train_auroc</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD test_auroc</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 svc_default  logistic_regression_default  \\\n",
       "AVG train_auroc        0.999                        1.000   \n",
       "AVG test_auroc         0.902                        0.901   \n",
       "STD train_auroc        0.000                        0.000   \n",
       "STD test_auroc         0.036                        0.036   \n",
       "\n",
       "                 random_forest_default  \n",
       "AVG train_auroc                  1.000  \n",
       "AVG test_auroc                   0.880  \n",
       "STD train_auroc                  0.000  \n",
       "STD test_auroc                   0.039  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_summary.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzlearn_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
