{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shutil\n",
    "import torch\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for MSKCC Prediction Optimization Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/hilic_pos_2024_feb_05_read_norm_poolmap/subset all_studies with align score 0.25 from Merge_Jan25_align_80_40_fillna_avg/num_cohorts_thresh_0.5/std_1_Multi'\n",
    "save_dir = '/Users/jonaheaton/Desktop/mskcc_study_feb13'\n",
    "\n",
    "\n",
    "X_finetune_file = 'X_finetune.csv'\n",
    "y_finetune_file = 'y_finetune.csv'\n",
    "X_pretrain_file = 'X_pretrain.csv'\n",
    "\n",
    "n_repeats = 6\n",
    "cv_splits = 5\n",
    "cross_val_seed = 42\n",
    "y_stratify_col = 'MSKCC'\n",
    "\n",
    "\n",
    "# save the pretraining data to the save_dir\n",
    "# X_pretrain = pd.read_csv(os.path.join(input_dir, X_pretrain_file), index_col=0)\n",
    "# X_pretrain.to_csv(os.path.join(save_dir, 'X_pretrain.csv'))\n",
    "shutil.copy(os.path.join(input_dir, X_pretrain_file), os.path.join(save_dir, 'X_pretrain.csv'))\n",
    "\n",
    "X_finetune = pd.read_csv(os.path.join(input_dir, X_finetune_file), index_col=0)\n",
    "y_finetune = pd.read_csv(os.path.join(input_dir, y_finetune_file), index_col=0)\n",
    "y_stratify = y_finetune[y_stratify_col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(n_repeats):\n",
    "    skf = StratifiedKFold(n_splits=cv_splits, random_state=cross_val_seed+iter, shuffle=True)\n",
    "    skf.get_n_splits(X_finetune, y_stratify)\n",
    "\n",
    "    # create a subdirectory for each of the cross-validation splits\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X_finetune, y_stratify)):\n",
    "\n",
    "        subset_id = iter * cv_splits + i\n",
    "        X_train, X_val = X_finetune.iloc[train_index], X_finetune.iloc[test_index]\n",
    "        y_train, y_val = y_finetune.iloc[train_index], y_finetune.iloc[test_index]\n",
    "        X_train.to_csv(os.path.join(save_dir, f'X_train_{subset_id}.csv'))\n",
    "        X_val.to_csv(os.path.join(save_dir, f'X_test_{subset_id}.csv'))\n",
    "        y_train.to_csv(os.path.join(save_dir, f'y_train_{subset_id}.csv'))\n",
    "        y_val.to_csv(os.path.join(save_dir, f'y_test_{subset_id}.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/hilic_pos_2024_feb_05_read_norm_poolmap/subset all_studies with align score 0.25 from Merge_Jan25_align_80_40_fillna_avg/num_cohorts_thresh_0.5/std_1_Multi'\n",
    "save_dir = '/Users/jonaheaton/Desktop/benefit_study_feb20'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "X_finetune_file = 'X_finetune.csv'\n",
    "y_finetune_file = 'y_finetune.csv'\n",
    "X_pretrain_file = 'X_pretrain.csv'\n",
    "\n",
    "n_repeats = 6\n",
    "cv_splits = 5\n",
    "cross_val_seed = 42\n",
    "y_stratify_col = 'Benefit'\n",
    "\n",
    "\n",
    "# save the pretraining data to the save_dir\n",
    "# X_pretrain = pd.read_csv(os.path.join(input_dir, X_pretrain_file), index_col=0)\n",
    "# X_pretrain.to_csv(os.path.join(save_dir, 'X_pretrain.csv'))\n",
    "# shutil.copy(os.path.join(input_dir, X_pretrain_file), os.path.join(save_dir, 'X_pretrain.csv'))\n",
    "\n",
    "X_finetune = pd.read_csv(os.path.join(input_dir, X_finetune_file), index_col=0)\n",
    "y_finetune = pd.read_csv(os.path.join(input_dir, y_finetune_file), index_col=0)\n",
    "y_stratify = y_finetune[y_stratify_col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(n_repeats):\n",
    "    skf = StratifiedKFold(n_splits=cv_splits, random_state=cross_val_seed+iter, shuffle=True)\n",
    "    skf.get_n_splits(X_finetune, y_stratify)\n",
    "\n",
    "    # create a subdirectory for each of the cross-validation splits\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X_finetune, y_stratify)):\n",
    "\n",
    "        subset_id = iter * cv_splits + i\n",
    "        X_train, X_val = X_finetune.iloc[train_index], X_finetune.iloc[test_index]\n",
    "        y_train, y_val = y_finetune.iloc[train_index], y_finetune.iloc[test_index]\n",
    "        X_train.to_csv(os.path.join(save_dir, f'X_train_{subset_id}.csv'))\n",
    "        X_val.to_csv(os.path.join(save_dir, f'X_test_{subset_id}.csv'))\n",
    "        y_train.to_csv(os.path.join(save_dir, f'y_train_{subset_id}.csv'))\n",
    "        y_val.to_csv(os.path.join(save_dir, f'y_test_{subset_id}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Pre-training Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    }
   ],
   "source": [
    "input_dir = '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/hilic_pos_2024_feb_05_read_norm_poolmap/subset all_studies with align score 0.25 from Merge_Jan25_align_80_40_fillna_avg/num_cohorts_thresh_0.5'\n",
    "sub_dir ='std_1_Multi'\n",
    "save_dir = '/Users/jonaheaton/Desktop/reconstruction_study_feb16'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "X_pretrain_file = 'X_pretrain.csv'\n",
    "y_pretrain_file = 'y_pretrain.csv'\n",
    "metadata_file = 'combined_metadata.csv'\n",
    "\n",
    "n_repeats = 1\n",
    "cv_splits = 4\n",
    "cross_val_seed = 42\n",
    "\n",
    "\n",
    "X_pretrain = pd.read_csv(os.path.join(input_dir, sub_dir, X_pretrain_file), index_col=0)\n",
    "y_pretrain = pd.read_csv(os.path.join(input_dir, sub_dir, y_pretrain_file), index_col=0)\n",
    "metadata = pd.read_csv(os.path.join(input_dir, metadata_file), index_col=0)\n",
    "\n",
    "gend_labels = y_pretrain\n",
    "gend_labels_no_nan = gend_labels.dropna()\n",
    "study_ids = metadata.loc[X_pretrain.index,'Study']\n",
    "cohort_label = metadata.loc[X_pretrain.index,'label6']\n",
    "\n",
    "# convert with label encoder\n",
    "gend_label_encoder = LabelEncoder()\n",
    "gend_label_encoder.fit(gend_labels_no_nan)\n",
    "cohort_label_encoder = LabelEncoder()\n",
    "\n",
    "gend_encoded = pd.Series(gend_label_encoder.transform(gend_labels_no_nan), index=gend_labels_no_nan.index, name='gender')\n",
    "cohort_encoded = pd.Series(cohort_label_encoder.fit_transform(cohort_label), index=cohort_label.index, name='cohort')\n",
    "\n",
    "# create a new dataframe with the encoded labels\n",
    "y_pretrain_encoded = pd.concat([gend_encoded, cohort_encoded], axis=1)\n",
    "\n",
    "X = X_pretrain\n",
    "y = y_pretrain_encoded.loc[X.index]\n",
    "\n",
    "# save the encoding mapping to a json file\n",
    "gend_mapping = dict(zip(gend_label_encoder.classes_, gend_label_encoder.transform(gend_label_encoder.classes_)))\n",
    "cohort_mapping = dict(zip(cohort_label_encoder.classes_, cohort_label_encoder.transform(cohort_label_encoder.classes_)))\n",
    "\n",
    "mapping = {'gend': gend_mapping, 'cohort': cohort_mapping}\n",
    "with open(os.path.join(save_dir, 'label_mapping.json'), 'w') as f:\n",
    "    # convert int64 to int\n",
    "    mapping = {k: {k2: int(v2) for k2, v2 in v.items()} for k, v in mapping.items()}\n",
    "    json.dump(mapping, f, indent=4)\n",
    "\n",
    "\n",
    "for iter in range(n_repeats):\n",
    "    skf = StratifiedKFold(n_splits=cv_splits, random_state=cross_val_seed+iter, shuffle=True)\n",
    "    skf.get_n_splits(X, y['cohort'])\n",
    "\n",
    "    # create a subdirectory for each of the cross-validation splits\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y['cohort'])):\n",
    "\n",
    "        subset_id = iter * cv_splits + i\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train.to_csv(os.path.join(save_dir, f'X_train_{subset_id}.csv'))\n",
    "        X_val.to_csv(os.path.join(save_dir, f'X_test_{subset_id}.csv'))\n",
    "        y_train.to_csv(os.path.join(save_dir, f'y_train_{subset_id}.csv'))\n",
    "        y_val.to_csv(os.path.join(save_dir, f'y_test_{subset_id}.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['cohort'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'runtime_hour', 'study_week', 'Region', 'Sex', 'Race',\n",
       "       'OS (months)', 'Death Observed', 'batch_id', 'ref', 'run_order',\n",
       "       'Treatment', 'phase', 'MSKCC', 'survival class', 'Study', 'Ref',\n",
       "       'label', 'Subject ID', 'Sample ID', 'Factors.plasma', 'Date Analyzed',\n",
       "       'Batch', 'CHEAR_Project_ID', 'Sample.Matrix', 'Raw_files',\n",
       "       'Factors.Fiber', 'Factors.Timepoint', 'Factors.Sex',\n",
       "       'Factors.Diagnosis', 'Factors.Treatment', 'SubjectID', 'Factors.Age',\n",
       "       'Factors.Race', 'Factors.Study_Diet', 'Type', 'BMI', 'Ethnicity',\n",
       "       'Time', 'Unnamed: 11', 'Factors.FACTORS', 'Date', 'CHEAR_Project ID',\n",
       "       'Factors.Sample type', 'file numb', 'Factors.Plasma', 'column',\n",
       "       'Age.At.Blood.Draw', 'label6', 'label3', 'Result_ID', 'Study_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import ClassifierDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = save_dir\n",
    "finetune_label_col = 'MSKCC'\n",
    "finetune_label_encoder = {'FAVORABLE': 1, 'POOR': 0, 'INTERMEDIATE': np.nan}\n",
    "finetune_dataset = ClassifierDataset(data_dir, \n",
    "                            subset='train_{}'.format(subset_id),\n",
    "                            label_col=finetune_label_col,\n",
    "                            label_encoder=finetune_label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = finetune_dataset.y\n",
    "# convert tensor to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = 1 / torch.bincount(y_vals.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0090, 0.0049])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "24\n",
      "18\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_sz = len(finetune_dataset)\n",
    "pretrain_batch_size = 23\n",
    "print(train_sz // pretrain_batch_size)\n",
    "new_batch_sz = (train_sz // (train_sz // pretrain_batch_size))\n",
    "print(new_batch_sz)\n",
    "\n",
    "# what is the remainder?\n",
    "print(train_sz % pretrain_batch_size)\n",
    "print(train_sz % new_batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_batch_sz(len_dataset, batch_sz):\n",
    "    curr_remainder = len_dataset % batch_sz\n",
    "    new_batch_sz = int(len_dataset/np.ceil(len_dataset / batch_sz))\n",
    "    new_remainder = len_dataset % new_batch_sz\n",
    "    print(f'curr_remainder: {curr_remainder}, new_remainder: {new_remainder}')\n",
    "    return new_batch_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_remainder: 18, new_remainder: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clean_batch_sz(train_sz, pretrain_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.782608695652174"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sz / pretrain_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(train_sz / pretrain_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzlearn_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
