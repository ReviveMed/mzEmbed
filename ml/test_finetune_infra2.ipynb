{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neptune\n",
    "import torch\n",
    "from neptune.utils import stringify_unsupported\n",
    "from misc import save_json, load_json, get_clean_batch_sz\n",
    "from utils_neptune import get_latest_dataset, check_neptune_existance\n",
    "from models import get_encoder, get_head, MultiHead, create_model_wrapper, create_pytorch_model_from_info, CompoundModel\n",
    "from train4 import train_compound_model, create_dataloaders_old, CompoundDataset\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "NEPTUNE_API_TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxMGM5ZDhiMy1kOTlhLTRlMTAtOGFlYy1hOTQzMDE1YjZlNjcifQ=='\n",
    "\n",
    "PROJECT_ID = 'revivemed/Survival-RCC'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_encoder(dropout_rate=None,use_rand_init=False,load_dir=None,verbose=False):\n",
    "    if load_dir is None:\n",
    "        load_dir = os.path.expanduser('~/PRETRAINED_MODELS/2925')\n",
    "        os.makedirs(load_dir,exist_ok=True)\n",
    "\n",
    "    encoder_kwargs_path = os.path.join(load_dir,'encoder_kwargs.json')\n",
    "    encoder_state_path =  os.path.join(load_dir,'encoder_state.pt')\n",
    "\n",
    "    if (not os.path.exists(encoder_kwargs_path)) or (not os.path.exists(encoder_state_path)):\n",
    "\n",
    "        neptune_model = neptune.init_model(project='revivemed/Survival-RCC',\n",
    "            api_token= NEPTUNE_API_TOKEN,\n",
    "            with_id='SUR-MOD',\n",
    "            mode=\"read-only\")\n",
    "\n",
    "        if not os.path.exists(encoder_state_path):\n",
    "            neptune_model['model/encoder_state'].download(encoder_state_path)\n",
    "        if not os.path.exists(encoder_kwargs_path):\n",
    "            encoder_kwargs = neptune_model['original_kwargs/encoder_kwargs'].fetch()\n",
    "            if 'input_size' not in encoder_kwargs:\n",
    "                encoder_kwargs['input_size'] = 2736\n",
    "            if 'kind' not in encoder_kwargs:\n",
    "                encoder_kwargs['kind'] = neptune_model['original_kwargs/encoder_kind'].fetch()\n",
    "            if 'hidden_size' not in encoder_kwargs:\n",
    "                if 'hidden_size_mult' in encoder_kwargs:\n",
    "                    latent_size = encoder_kwargs['latent_size']\n",
    "                    encoder_kwargs['hidden_size'] = int(encoder_kwargs['hidden_size_mult']*latent_size)\n",
    "                else:\n",
    "                    raise ValueError()\n",
    "                # remove the hidden_size_mult key\n",
    "                encoder_kwargs.pop('hidden_size_mult')\n",
    "            save_json(encoder_kwargs,encoder_kwargs_path)\n",
    "\n",
    "        neptune_model.stop()\n",
    "\n",
    "    encoder_kwargs = load_json(encoder_kwargs_path)\n",
    "    if (dropout_rate is not None):\n",
    "        if verbose: print('Setting dropout rate to',dropout_rate)\n",
    "        encoder_kwargs['dropout_rate'] = dropout_rate\n",
    "\n",
    "    encoder = get_encoder(**encoder_kwargs)\n",
    "\n",
    "    if not use_rand_init:\n",
    "        encoder.load_state_dict(torch.load(encoder_state_path))\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model_heads(head_kwargs_dict,backup_input_size=None,verbose=False):\n",
    "    if head_kwargs_dict is None:\n",
    "        return MultiHead([]) #get_head(kind='Dummy')\n",
    "    \n",
    "    if isinstance(head_kwargs_dict,dict):\n",
    "        head_kwargs_list = [head_kwargs_dict[k] for k in head_kwargs_dict.keys()]\n",
    "    elif isinstance(head_kwargs_dict,list):\n",
    "        head_kwargs_list = head_kwargs_dict\n",
    "    else:\n",
    "        raise ValueError(f'Invalid head_kwargs_dict type: {type(head_kwargs_dict)}')\n",
    "\n",
    "    head_list = []\n",
    "    if len(head_kwargs_list) == 0:\n",
    "       return MultiHead([])\n",
    "\n",
    "    for h_kwargs in head_kwargs_list:\n",
    "            \n",
    "        if 'input_size' not in h_kwargs:\n",
    "            if backup_input_size is None:\n",
    "                raise ValueError('backup_input_size is None')\n",
    "            if verbose: print('Setting input_size to',backup_input_size)\n",
    "            h_kwargs['input_size'] = backup_input_size\n",
    "\n",
    "        head = get_head(**h_kwargs)\n",
    "        head_list.append(head)\n",
    "\n",
    "\n",
    "    head = MultiHead(head_list)\n",
    "    return head\n",
    "\n",
    "\n",
    "\n",
    "def build_model_components(head_kwargs_dict,adv_kwargs_dict=None,dropout_rate=None,use_rand_init=False):\n",
    "\n",
    "    encoder = get_pretrained_encoder(dropout_rate=dropout_rate,use_rand_init=use_rand_init)\n",
    "\n",
    "    #TODO we need better handling of head input size to account for the size of the other_vars\n",
    "    head = get_model_heads(head_kwargs_dict,backup_input_size=encoder.latent_size+1)\n",
    "    if head.kind == 'MultiHead':\n",
    "        head.name = 'HEAD'\n",
    "\n",
    "    adv = get_model_heads(adv_kwargs_dict,backup_input_size=encoder.latent_size)\n",
    "    if adv.kind == 'MultiHead':\n",
    "        adv.name = 'ADVERSARY'\n",
    "\n",
    "    return encoder, head, adv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_wrapper(X,y,model_components_dict={},run_dict={},**train_kwargs):\n",
    "\n",
    "    assert isinstance(model_components_dict,dict), 'model_components_dict should be a dictionary'\n",
    "\n",
    "    if isinstance(run_dict, neptune.metadata_containers.run.Run) or isinstance(run_dict, neptune.handler.Handler):\n",
    "        print('Record the model fitting to Neptune')\n",
    "        use_neptune= True\n",
    "    else:\n",
    "        use_neptune = False\n",
    "        \n",
    "    ### Model Component Defaults\n",
    "    y_head_cols = model_components_dict.get('y_head_cols',None)\n",
    "    y_adv_cols = model_components_dict.get('y_adv_cols',None)\n",
    "    head_kwargs_dict = model_components_dict.get('head_kwargs_dict',None)\n",
    "    adv_kwargs_dict = model_components_dict.get('adv_kwargs_dict',None)\n",
    "\n",
    "    if y_head_cols is None:\n",
    "        # select all of the numeric columns\n",
    "        y_head_cols = list(y.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "    if y_adv_cols is None:\n",
    "        y_adv_cols = []\n",
    "\n",
    "    ### Train Defaults\n",
    "    dropout_rate = train_kwargs.get('dropout_rate', None)\n",
    "    use_rand_init = train_kwargs.get('use_rand_init', False)\n",
    "    batch_size = train_kwargs.get('batch_size', 64)\n",
    "    holdout_frac = train_kwargs.get('holdout_frac', 0)\n",
    "    early_stopping_patience = train_kwargs.get('early_stopping_patience', 0)\n",
    "    scheduler_kind = train_kwargs.get('scheduler_kind', None)\n",
    "    train_name = train_kwargs.get('train_name', 'train')\n",
    "\n",
    "    ### Prepare the Data Loader\n",
    "    X_size = X.shape[1]\n",
    "    if (holdout_frac > 0) and (early_stopping_patience < 1) and (scheduler_kind is None):\n",
    "        # raise ValueError('holdout_frac > 0 and early_stopping_patience < 1 is not recommended')\n",
    "        print('holdout_frac > 0 and early_stopping_patience < 1 is not recommended, set hold out frac to 0')\n",
    "        print('UNLESS you are using a scheduler, in which case the holdout_frac is used for the scheduler')\n",
    "        holdout_frac = 0\n",
    "        batch_size = get_clean_batch_sz(X_size, batch_size)\n",
    "    else:\n",
    "        batch_size = get_clean_batch_sz(X_size*(1-holdout_frac), batch_size)\n",
    "\n",
    "    train_dataset = CompoundDataset(X,y[y_head_cols], y[y_adv_cols])\n",
    "    # stratify on the adversarial column (stratify=2)\n",
    "    # this is probably not the most memory effecient method, would be better to do stratification before creating the dataset\n",
    "    # train_loader_dct = create_dataloaders(train_dataset, batch_size, holdout_frac, set_name=train_name, stratify=2)\n",
    "    train_loader_dct = create_dataloaders_old(train_dataset, batch_size, holdout_frac, set_name=train_name)\n",
    "\n",
    "\n",
    "    ### Build the Model Components\n",
    "    encoder, head, adv = build_model_components(head_kwargs_dict=head_kwargs_dict,\n",
    "                                                adv_kwargs_dict=adv_kwargs_dict,\n",
    "                                                dropout_rate=dropout_rate,\n",
    "                                                use_rand_init=use_rand_init)\n",
    "\n",
    "    if train_dataset is not None:\n",
    "        #TODO load the class weights from the model info json\n",
    "        head.update_class_weights(train_dataset.y_head)\n",
    "        adv.update_class_weights(train_dataset.y_adv)\n",
    "\n",
    "    if len(adv.heads)==0:\n",
    "        train_kwargs['adversary_weight'] = 0\n",
    "\n",
    "    ### Train the Model\n",
    "    encoder, head, adv = train_compound_model(train_loader_dct, \n",
    "                                            encoder, head, adv, \n",
    "                                            run=run_dict, **train_kwargs)\n",
    "    if encoder is None:\n",
    "        raise ValueError('Encoder is None after training, training failed')\n",
    "\n",
    "    \n",
    "    return run_dict, encoder, head, adv\n",
    "\n",
    "\n",
    "\n",
    "def save_model_wrapper(encoder, head, adv, save_dir=None,run_dict={},prefix='training_run'):\n",
    "\n",
    "        if isinstance(run_dict, neptune.metadata_containers.run.Run) or isinstance(run_dict, neptune.handler.Handler):\n",
    "            print('Save models to Neptune')\n",
    "            use_neptune= True\n",
    "        else:\n",
    "            use_neptune = False\n",
    "\n",
    "        delete_after_upload = False\n",
    "        if save_dir is None:\n",
    "            save_dir = os.path.expanduser('~/TEMP_MODELS')\n",
    "            if use_neptune:\n",
    "                delete_after_upload = True\n",
    "                if os.path.exists(save_dir):\n",
    "                    # delete the directory\n",
    "                    shutil.rmtree(save_dir)\n",
    "            os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "        upload_models_to_gcp = False    \n",
    "\n",
    "        encoder.save_state_to_path(save_dir,save_name='encoder_state.pt')\n",
    "        encoder.save_info(save_dir,save_name='encoder_info.json')\n",
    "        head.save_state_to_path(save_dir,save_name='head_state.pt')\n",
    "        head.save_info(save_dir,save_name='head_info.json')\n",
    "        adv.save_state_to_path(save_dir,save_name='adversary_state.pt')\n",
    "        adv.save_info(save_dir,save_name='adversary_info.json')\n",
    "\n",
    "        # torch.save(head.state_dict(), f'{save_dir}/{setup_id}_head_state_dict.pth')\n",
    "        # torch.save(adv.state_dict(), f'{save_dir}/{setup_id}_adv_state_dict.pth')\n",
    "        if use_neptune:\n",
    "            run_dict[f'{prefix}/models/encoder_state'].upload(f'{save_dir}/encoder_state.pt')\n",
    "            run_dict[f'{prefix}/models/encoder_info'].upload(f'{save_dir}/encoder_info.json')\n",
    "            run_dict[f'{prefix}/models/head_state'].upload(f'{save_dir}/head_state.pt')\n",
    "            run_dict[f'{prefix}/models/head_info'].upload(f'{save_dir}/head_info.json')\n",
    "            run_dict[f'{prefix}/models/adv_state'].upload(f'{save_dir}/adv_state.pt')\n",
    "            run_dict[f'{prefix}/models/adv_info'].upload(f'{save_dir}/adv_info.json')\n",
    "            run_dict.wait()\n",
    "\n",
    "        if upload_models_to_gcp:\n",
    "            raise NotImplementedError('upload_models_to_gcp not implemented')\n",
    "\n",
    "        if use_neptune and delete_after_upload:\n",
    "            run_dict.wait()\n",
    "            shutil.rmtree(save_dir)\n",
    "\n",
    "        return run_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_wrapper(encoder, head, adv, X_data_eval, y_data_eval,y_cols,y_head=None):\n",
    "\n",
    "    if y_head is None:\n",
    "        chosen_head = head\n",
    "    else:\n",
    "        multihead_name_list = head.heads_names\n",
    "        if y_head not in multihead_name_list:\n",
    "            raise ValueError(f'Invalid head name: {y_head}')\n",
    "        chosen_head_idx = multihead_name_list.index(y_head)\n",
    "        chosen_head = head.heads[chosen_head_idx]\n",
    "\n",
    "        if isinstance(chosen_head.y_idx,list):\n",
    "            if len(y_cols) != len(chosen_head.y_idx):\n",
    "                raise ValueError(f'Invalid y_cols length: {len(y_cols)} vs {len(chosen_head.y_idx)}')\n",
    "            if len(y_cols) == 1:\n",
    "                chosen_head.y_idx = [0]\n",
    "            else:\n",
    "                chosen_head.y_idx = list(range(len(y_cols)))\n",
    "        else:\n",
    "            if len(y_cols) != 1:\n",
    "                raise ValueError(f'Invalid y_cols length: {len(y_cols)} vs {len(chosen_head.y_idx)}')\n",
    "            chosen_head.y_idx = 0\n",
    "\n",
    "        # if len(y_cols) != len(chosen_head.y_idx):\n",
    "        #     raise ValueError(f'Invalid y_cols length: {len(y_cols)} vs {len(chosen_head.y_idx)}')\n",
    "        # if len(y_cols) == 1:\n",
    "        #     chosen_head.y_idx = 0\n",
    "        # else:\n",
    "        #     chosen_head.y_idx = list(range(len(y_cols)))\n",
    "        \n",
    "    model = CompoundModel(encoder, chosen_head)\n",
    "    skmodel = create_pytorch_model_from_info(full_model=model)\n",
    "\n",
    "    return skmodel.score(X_data_eval.to_numpy(),y_data_eval[y_cols].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_wrapper(data_dir,params,output_dir=None,train_name='train', prefix='training_run',\n",
    "              eval_name_list=['val'],eval_params_list=None,run_dict=None):\n",
    "    \n",
    "    # if run_dict is None:\n",
    "    #     run_dict = {}\n",
    "\n",
    "    if isinstance(run_dict, neptune.metadata_containers.run.Run) or isinstance(run_dict, neptune.handler.Handler):\n",
    "        print('Using Neptune')\n",
    "        use_neptune= True\n",
    "        #TODO: check if the models are already trained on neptune\n",
    "        download_models_from_neptune = check_neptune_existance(run_dict,'models/encoder_info')\n",
    "\n",
    "\n",
    "    if use_neptune:\n",
    "        run_dict[f'{prefix}/dataset'].track_files(data_dir)\n",
    "        run_dict[f'{prefix}/model_name'] = 'Model2925'\n",
    "        run_dict[f'{prefix}/params'] = stringify_unsupported(params)\n",
    "\n",
    "    if eval_params_list is None:\n",
    "        eval_params_list = [{}]\n",
    "\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.expanduser('~/TEMP_MODELS')\n",
    "\n",
    "    X_filename = 'X_finetune'\n",
    "    y_filename = 'y_finetune'\n",
    "    saved_model_dir = os.path.join(output_dir,prefix,'models')        \n",
    "    os.makedirs(saved_model_dir,exist_ok=True)\n",
    "    task_components_dict = params['task_kwargs']\n",
    "    train_kwargs = params['train_kwargs']\n",
    "\n",
    "    if (not os.path.exists(f'{saved_model_dir}/encoder_info.json')) and (use_neptune) and (download_models_from_neptune): \n",
    "        run_dict[f'{prefix}/models/encoder_state'].download(f'{saved_model_dir}/encoder_state.pt')\n",
    "        run_dict[f'{prefix}/models/encoder_info'].download(f'{saved_model_dir}/encoder_info.json')\n",
    "        run_dict[f'{prefix}/models/head_state'].download(f'{saved_model_dir}/head_state.pt')\n",
    "        run_dict[f'{prefix}/models/head_info'].download(f'{saved_model_dir}/head_info.json')\n",
    "        run_dict[f'{prefix}/models/adv_state'].download(f'{saved_model_dir}/adv_state.pt')\n",
    "        run_dict[f'{prefix}/models/adv_info'].download(f'{saved_model_dir}/adv_info.json')\n",
    "    \n",
    "    if os.path.exists(f'{saved_model_dir}/encoder_info.json'):\n",
    "        encoder = create_model_wrapper(f'{saved_model_dir}/encoder_info.json',f'{saved_model_dir}/encoder_state.pt')\n",
    "        head = create_model_wrapper(f'{saved_model_dir}/head_info.json',f'{saved_model_dir}/head_state.pt',is_encoder=False)\n",
    "        if os.path.exists(f'{saved_model_dir}/adv_info.json'):\n",
    "            adv = create_model_wrapper(f'{saved_model_dir}/adv_info.json',f'{saved_model_dir}/adv_state.pt',is_encoder=False)\n",
    "        else:\n",
    "            adv = MultiHead([])\n",
    "\n",
    "    else:\n",
    "        X_data_train = pd.read_csv(f'{data_dir}/{X_filename}_{train_name}.csv', index_col=0)\n",
    "        y_data_train = pd.read_csv(f'{data_dir}/{y_filename}_{train_name}.csv', index_col=0)\n",
    "\n",
    "        _, encoder, head, adv = fit_model_wrapper(X=X_data_train,\n",
    "                                                            y=y_data_train,\n",
    "                                                            model_components_dict=task_components_dict,\n",
    "                                                            run_dict=run_dict[prefix],**train_kwargs)\n",
    "\n",
    "        save_model_wrapper(encoder, head, adv, save_dir=saved_model_dir,run_dict=run_dict,prefix=prefix)\n",
    "\n",
    "\n",
    "    metrics = defaultdict(dict)\n",
    "    for eval_name in eval_name_list:\n",
    "        X_data_eval = pd.read_csv(f'{data_dir}/{X_filename}_{eval_name}.csv', index_col=0)\n",
    "        y_data_eval = pd.read_csv(f'{data_dir}/{y_filename}_{eval_name}.csv', index_col=0)\n",
    "\n",
    "        for eval_params in eval_params_list:\n",
    "            y_col_name = eval_params.get('y_col_name',None)\n",
    "            y_cols = eval_params.get('y_cols',None)\n",
    "            y_head = eval_params.get('y_head',None)\n",
    "            if y_cols is None:\n",
    "                y_cols = params['task_kwargs']['y_head_cols']\n",
    "\n",
    "            if y_col_name is None:\n",
    "                metrics[f'{eval_name}' ].update(evaluate_model_wrapper(encoder, head, adv, X_data_eval, y_data_eval,y_cols=y_cols,y_head=y_head))\n",
    "            else:\n",
    "                metrics[f'{eval_name}__{y_col_name}'].update(evaluate_model_wrapper(encoder, head, adv, X_data_eval, y_data_eval,y_cols=y_cols,y_head=y_head))\n",
    "                \n",
    "    if use_neptune:\n",
    "        run_dict[f'{prefix}/metrics'] = metrics\n",
    "        run_dict.wait()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_structure_kwargs = {\n",
    "    'encoder_weight': 0,\n",
    "    'adversary_weight': 0,\n",
    "    'head_hidden_layers': 0,\n",
    "    'auxillary_heads' : {'name': 'dummy', 'weight': 0},\n",
    "    'adversary_heads' : {}\n",
    "}\n",
    "\n",
    "finetune_train_kwargs = {\n",
    "    'batch_size': 64,\n",
    "    'holdout_frac': 0,\n",
    "    'early_stopping_patience': 0,\n",
    "    'scheduler_kind': None,\n",
    "    'train_name': 'train',\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'noise_factor': 0.1,\n",
    "    'weight_decay': 0,\n",
    "    'l2_reg_weight': 0,\n",
    "    'l1_reg_weight': 0,\n",
    "    'dropout_rate': 0.1,\n",
    "    'adversarial_start_epoch': 0,\n",
    "    'clip_grads_with_norm': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sweep_kwargs = {\n",
    "    'holdout_frac': 0,\n",
    "    'head_hidden_layers': 0,\n",
    "    'encoder_kwargs__dropout_rate': 0.2,\n",
    "    'train_kwargs__num_epochs': 30,\n",
    "    'train_kwargs__early_stopping_patience': 0,\n",
    "    'train_kwargs__learning_rate': 0.0005,\n",
    "    'train_kwargs__l2_reg_weight': 0.0005,\n",
    "    'train_kwargs__l1_reg_weight': 0.005,\n",
    "    'train_kwargs__noise_factor': 0.1,\n",
    "    'train_kwargs__weight_decay': 0,\n",
    "    'train_kwargs__adversary_weight': 1,\n",
    "    'train_kwargs__adversarial_start_epoch': 10,\n",
    "    'train_kwargs__encoder_weight': 0,\n",
    "    'train_kwargs__clip_grads_with_norm': False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_head_kwargs_by_desc(desc_str,num_hidden_layers=0,weight=1,y_cols=None):\n",
    "    if (desc_str is None) or (desc_str == ''):\n",
    "        return None, [], []\n",
    "    \n",
    "    if y_cols is None:\n",
    "        y_cols = []\n",
    "\n",
    "    if 'weight-' in desc_str:\n",
    "        match = re.search(r'weight-(\\d+)', desc_str)\n",
    "        if match:\n",
    "            weight = int(match.group(1))\n",
    "            desc_str = desc_str.replace(match.group(0),'')\n",
    "\n",
    "    if 'mskcc' in desc_str.lower():\n",
    "        # if 'mskcc-ord' in desc_str.lower:\n",
    "            # raise NotImplementedError('not implemented yet')\n",
    "        y_head_cols = ['MSKCC BINARY']\n",
    "        head_name = 'MSKCC'\n",
    "        head_kind = 'Binary'\n",
    "        num_classes = 2\n",
    "        y_idx = 0\n",
    "        plot_latent_space_cols = ['MSKCC']\n",
    "\n",
    "    elif 'imdc' in desc_str.lower():\n",
    "        y_head_cols = ['IMDC BINARY']\n",
    "        head_name = 'IMDC'\n",
    "        head_kind = 'Binary'\n",
    "        num_classes = 2\n",
    "        y_idx = 0\n",
    "        plot_latent_space_cols = ['IMDC']\n",
    "\n",
    "    elif 'nivo-benefit' in desc_str.lower():\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    elif 'benefit' in desc_str.lower():\n",
    "        y_head_cols = ['Benefit BINARY']\n",
    "        head_name = 'Benefit'\n",
    "        head_kind = 'Binary'\n",
    "        num_classes = 2\n",
    "        y_idx = 0\n",
    "        plot_latent_space_cols = ['Benefit']\n",
    "\n",
    "    elif 'both-os' in desc_str.lower():\n",
    "        y_head_cols = ['OS','OS_Event']\n",
    "        head_name = 'OS'\n",
    "        head_kind = 'Cox'\n",
    "        num_classes = 1\n",
    "        y_idx = [0,1]\n",
    "        plot_latent_space_cols = ['OS']   \n",
    "\n",
    "    elif 'both-pfs' in desc_str.lower():\n",
    "        y_head_cols = ['PFS','PFS_Event']\n",
    "        head_name = 'PFS'\n",
    "        head_kind = 'Cox'\n",
    "        num_classes = 1\n",
    "        y_idx = [0,1]\n",
    "        plot_latent_space_cols = ['PFS']        \n",
    "\n",
    "    elif 'nivo-os' in desc_str.lower():\n",
    "        y_head_cols = ['NIVO OS','OS_Event']\n",
    "        head_name = 'NIVO OS'\n",
    "        head_kind = 'Cox'\n",
    "        num_classes = 1\n",
    "        y_idx = [0,1]\n",
    "        plot_latent_space_cols = ['NIVO OS']   \n",
    "\n",
    "    elif 'nivo-pfs' in desc_str.lower():\n",
    "        y_head_cols = ['NIVO PFS','PFS_Event']\n",
    "        head_name = 'NIVO PFS'\n",
    "        head_kind = 'Cox'\n",
    "        num_classes = 1\n",
    "        y_idx = [0,1]\n",
    "        plot_latent_space_cols = ['NIVO PFS']         \n",
    "\n",
    "    elif 'ever-os' in desc_str.lower():\n",
    "        y_head_cols = ['EVER OS','OS_Event']\n",
    "        head_name = 'EVER OS'\n",
    "        head_kind = 'Cox'\n",
    "        num_classes = 1\n",
    "        y_idx = [0,1]\n",
    "        plot_latent_space_cols = ['EVER OS']   \n",
    "\n",
    "    elif 'ever-pfs' in desc_str.lower():\n",
    "        y_head_cols = ['EVER PFS','PFS_Event']\n",
    "        head_name = 'EVER PFS'\n",
    "        head_kind = 'Cox'\n",
    "        num_classes = 1\n",
    "        y_idx = [0,1]\n",
    "        plot_latent_space_cols = ['EVER OS']            \n",
    "    else:\n",
    "        raise ValueError('Unknown desc_str:',desc_str)\n",
    "\n",
    "    for col in y_head_cols:\n",
    "        if col not in y_cols:\n",
    "            y_cols.append(col)\n",
    "\n",
    "    if len(y_head_cols) == 1:\n",
    "        y_idx = y_cols.index(y_head_cols[0])\n",
    "    else:\n",
    "        y_idx = [y_cols.index(col) for col in y_head_cols]\n",
    "\n",
    "\n",
    "\n",
    "    head_kwargs = {\n",
    "            'kind': head_kind,\n",
    "            'name': head_name,\n",
    "            'weight': weight,\n",
    "            'y_idx': y_idx,\n",
    "            'hidden_size': 4,\n",
    "            'num_hidden_layers': num_hidden_layers,\n",
    "            'dropout_rate': 0,\n",
    "            'activation': 'leakyrelu',\n",
    "            'use_batch_norm': False,\n",
    "            'num_classes': num_classes,\n",
    "            }\n",
    "\n",
    "    return head_kwargs, y_head_cols, plot_latent_space_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_components_dict_from_str(desc_str,sweep_kwargs=None):\n",
    "\n",
    "    if sweep_kwargs is None:\n",
    "        sweep_kwargs = default_sweep_kwargs\n",
    "\n",
    "    model_components_dict = {}\n",
    "\n",
    "    clean_desc_str = desc_str\n",
    "    if 'optuna_' in desc_str:\n",
    "        clean_desc_str = clean_desc_str.replace('optuna_','')\n",
    "    if 'Optimized_' in clean_desc_str:\n",
    "        clean_desc_str = clean_desc_str.replace('Optimized_','')\n",
    "    if '__' in clean_desc_str:\n",
    "        clean_desc_str = clean_desc_str.split('__')[0]\n",
    "\n",
    "    if 'ADV' in clean_desc_str:\n",
    "        adv_desc_str = clean_desc_str.split('ADV')[1]\n",
    "        head_desc_str = clean_desc_str.split('ADV')[0]\n",
    "    else:\n",
    "        adv_desc_str = ''\n",
    "        head_desc_str = clean_desc_str\n",
    "\n",
    "    y_head_cols = []\n",
    "    head_kwargs_list = []\n",
    "    plot_latent_space_cols = []\n",
    "    \n",
    "    if 'AND' in head_desc_str:\n",
    "        head_desc_str_list = head_desc_str.split('AND')\n",
    "    else:\n",
    "        head_desc_str_list = [head_desc_str]\n",
    "\n",
    "    head_hidden_layers = sweep_kwargs.get('head_hidden_layers',0)\n",
    "\n",
    "    for h_desc in head_desc_str_list:\n",
    "        head_weight = sweep_kwargs.get(f'{h_desc}__weight',1)\n",
    "        head_kwargs, head_cols, plot_latent_space_head_cols = get_head_kwargs_by_desc(h_desc,\n",
    "                                                                                    num_hidden_layers=head_hidden_layers,\n",
    "                                                                                    weight=head_weight,y_cols=y_head_cols)\n",
    "        if head_kwargs is None:\n",
    "            continue\n",
    "        head_kwargs_list.append(head_kwargs)\n",
    "        for col in head_cols:\n",
    "            if col not in y_head_cols:\n",
    "                y_head_cols.append(col)\n",
    "\n",
    "        for col in plot_latent_space_head_cols:\n",
    "            if col not in plot_latent_space_cols:\n",
    "                plot_latent_space_cols.append(col)\n",
    "\n",
    "\n",
    "\n",
    "    y_adv_cols = []\n",
    "    adv_kwargs_list = []\n",
    "\n",
    "    if 'AND' in adv_desc_str:\n",
    "        adv_desc_str_list = adv_desc_str.split('AND')\n",
    "    else:\n",
    "        adv_desc_str_list = [adv_desc_str]\n",
    "\n",
    "    for a_desc in adv_desc_str_list:\n",
    "        adv_weight = sweep_kwargs.get(f'{a_desc}__weight',1)\n",
    "        adv_kwargs, adv_cols, plot_latent_space_adv_cols = get_head_kwargs_by_desc(a_desc,\n",
    "                                                                                   num_hidden_layers=head_hidden_layers,\n",
    "                                                                                    weight=adv_weight,y_cols=y_adv_cols)\n",
    "        if adv_kwargs is None:\n",
    "            continue\n",
    "        adv_kwargs_list.append(adv_kwargs)\n",
    "        for col in adv_cols:\n",
    "            if col not in y_adv_cols:\n",
    "                y_adv_cols.append(col)\n",
    "        for col in plot_latent_space_adv_cols:\n",
    "            if col not in plot_latent_space_cols:\n",
    "                plot_latent_space_cols.append(col)\n",
    "\n",
    "    model_components_dict = {\n",
    "        'y_head_cols': y_head_cols,\n",
    "        'y_adv_cols': y_adv_cols,\n",
    "        'head_kwargs_dict': head_kwargs_list,\n",
    "        'adv_kwargs_dict': adv_kwargs_list,\n",
    "        'plot_latent_space_cols': plot_latent_space_cols\n",
    "    }\n",
    "\n",
    "    return model_components_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(desc_str,sweep_kwargs=None):\n",
    "\n",
    "    model_components_dict = parse_model_components_dict_from_str(desc_str,sweep_kwargs)\n",
    "\n",
    "    train_kwargs_list = [\n",
    "        'batch_size',\n",
    "        'holdout_frac',\n",
    "        'early_stopping_patience',\n",
    "        'scheduler_kind',\n",
    "        'num_epochs',\n",
    "        'learning_rate',\n",
    "        'noise_factor',\n",
    "        'weight_decay',\n",
    "        'l2_reg_weight',\n",
    "        'l1_reg_weight',\n",
    "        'dropout_rate',\n",
    "        'adversarial_start_epoch',\n",
    "        'clip_grads_with_norm',\n",
    "        'encoder_weight',\n",
    "        'adversary_weight',\n",
    "        'train_name'\n",
    "    ]\n",
    "\n",
    "    train_kwargs_dict = {k:v for k,v in sweep_kwargs.items() if k in train_kwargs_list}\n",
    "    params = {\n",
    "        'task_kwargs': model_components_dict,\n",
    "        'train_kwargs': train_kwargs_dict\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/revivemed/Survival-RCC/e/SUR-2481\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(project=PROJECT_ID,\n",
    "                                api_token=NEPTUNE_API_TOKEN,\n",
    "                                with_id='SUR-2481')\n",
    "                                # mode=neptune_mode,\n",
    "                                # capture_stdout=yes_logging,\n",
    "                                # capture_stderr=yes_logging,\n",
    "                                # capture_hardware_metrics=yes_logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_finetune_optimization/April_30_Finetune_Data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dict = run['training_run']\n",
    "params = get_params('IMDC AND MSKCC',sweep_kwargs=default_sweep_kwargs)\n",
    "\n",
    "eval_params_list = [\n",
    "    {},\n",
    "    {'y_col_name':'IMDC',\n",
    "    'y_head':'MSKCC',\n",
    "    'y_cols': ['IMDC BINARY']},\n",
    "]\n",
    "\n",
    "run_model_wrapper(data_dir,params,output_dir=None,train_name='train',prefix='training_run1', \n",
    "              eval_name_list=['val'],eval_params_list=eval_params_list,run_dict=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Neptune\n",
      "Warning: training is not a valid argument for VAE\n",
      "Warning: goal is not a valid argument for VAE\n",
      "Warning: kind is not a valid argument for VAE\n",
      "Warning: file_id is not a valid argument for VAE\n",
      "Warning: kl_weight is not a valid argument for VAE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'val': {'Cox_EVER OS': {'Concordance Index': 0.7134986225895317},\n",
       "              'Cox_NIVO OS': {'Concordance Index': 0.6332842415316642}},\n",
       "             'val__NIVO OS': {'Concordance Index': 0.6991899852724595}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_dict = run['training_run']\n",
    "params = get_params('EVER-OS AND NIVO-OS',sweep_kwargs=default_sweep_kwargs)\n",
    "\n",
    "eval_params_list = [\n",
    "    {},\n",
    "    {'y_col_name':'NIVO OS',\n",
    "    'y_head':'EVER OS',\n",
    "    'y_cols': ['NIVO OS','OS_Event']},\n",
    "]\n",
    "\n",
    "run_model_wrapper(data_dir,params,output_dir=None,train_name='train',prefix='training_run2', \n",
    "              eval_name_list=['val'],eval_params_list=eval_params_list,run_dict=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_kwargs': {'y_head_cols': ['EVER OS', 'OS_Event', 'NIVO OS'],\n",
       "  'y_adv_cols': [],\n",
       "  'head_kwargs_dict': [{'kind': 'Cox',\n",
       "    'name': 'EVER OS',\n",
       "    'weight': 1,\n",
       "    'y_idx': [0, 1],\n",
       "    'hidden_size': 4,\n",
       "    'num_hidden_layers': 0,\n",
       "    'dropout_rate': 0,\n",
       "    'activation': 'leakyrelu',\n",
       "    'use_batch_norm': False,\n",
       "    'num_classes': 1},\n",
       "   {'kind': 'Cox',\n",
       "    'name': 'NIVO OS',\n",
       "    'weight': 1,\n",
       "    'y_idx': [2, 1],\n",
       "    'hidden_size': 4,\n",
       "    'num_hidden_layers': 0,\n",
       "    'dropout_rate': 0,\n",
       "    'activation': 'leakyrelu',\n",
       "    'use_batch_norm': False,\n",
       "    'num_classes': 1}],\n",
       "  'adv_kwargs_dict': [],\n",
       "  'plot_latent_space_cols': ['EVER OS', 'NIVO OS']},\n",
       " 'train_kwargs': {'holdout_frac': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params('IMDC AND MSKCC',sweep_kwargs=default_sweep_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, head, adv = build_model_components(head_kwargs_dict=params['task_kwargs']['head_kwargs_dict'],\n",
    "                                            adv_kwargs_dict=params['task_kwargs']['adv_kwargs_dict'],\n",
    "                                            dropout_rate=0,\n",
    "                                            use_rand_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = CompoundModel(encoder,head.heads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head.heads_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/jonaheaton/Downloads/head_info.json'\n",
    "model_info = load_json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/revivemed/Survival-RCC/e/SUR-2481/metadata\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mz_embed_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
