diff --git a/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/last_ack_version b/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/last_ack_version
deleted file mode 100644
index 199b0cb..0000000
--- a/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/last_ack_version
+++ /dev/null
@@ -1 +0,0 @@
-40958
\ No newline at end of file
diff --git a/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/last_put_version b/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/last_put_version
deleted file mode 100644
index 976723a..0000000
--- a/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/last_put_version
+++ /dev/null
@@ -1 +0,0 @@
-41041
\ No newline at end of file
diff --git a/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/metadata.json b/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/metadata.json
deleted file mode 100644
index 059b133..0000000
--- a/ml/.neptune/async/run__9cfa462b-ea38-48d1-8466-7fbe7b06d2a1__3130338__hbrpoig8/metadata.json
+++ /dev/null
@@ -1,10 +0,0 @@
-{
-  "mode": "async",
-  "containerId": "9cfa462b-ea38-48d1-8466-7fbe7b06d2a1",
-  "containerType": "run",
-  "structureVersion": 3,
-  "os": "Linux-5.10.0-32-cloud-amd64-x86_64-with-glibc2.31",
-  "pythonVersion": "3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]",
-  "neptuneClientVersion": "1.10.4",
-  "createdAt": "2024-09-10T16:55:30.477481+00:00"
-}
\ No newline at end of file
diff --git a/ml/models_VAE.py b/ml/models_VAE.py
index 1b904f7..6ed08ef 100644
--- a/ml/models_VAE.py
+++ b/ml/models_VAE.py
@@ -90,13 +90,29 @@ def _nan_cleaner(y_output, y_true, ignore_nan=True, label_type='class'):
 
 
 #########################################################
+
 class Dense_Layers(nn.Module):
-    def __init__(self, input_size, output_size, num_hidden_layers=1, dropout_rate=0.2, activation='leakyrelu', use_batch_norm=False, act_on_output_layer=False):
+    def __init__(self, input_size, output_size, num_hidden_layers=1, dropout_rate=0.2, 
+                 activation='leakyrelu', use_batch_norm=False, act_on_output_layer=False, mode='encode'):
+        """
+        A generic fully connected layer class that can act as either encoder or decoder.
+        
+        Parameters:
+            input_size (int): Input size of the first layer.
+            output_size (int): Final output size (e.g., latent size for encoder, input size for decoder).
+            num_hidden_layers (int): Number of hidden layers to apply.
+            dropout_rate (float): Dropout rate applied between layers.
+            activation (str): Activation function to use. Supported: 'leakyrelu', 'relu', 'tanh', etc.
+            use_batch_norm (bool): Whether to apply batch normalization.
+            act_on_output_layer (bool): Whether to apply activation on the final output layer.
+            mode (str): 'encode' for encoder (compression), 'decode' for decoder (expansion).
+        """
         super(Dense_Layers, self).__init__()
 
-        # Create a list to store the size of each layer
-        hidden_sizes = self._compute_hidden_sizes(input_size, output_size, num_hidden_layers)
+        # Compute the sizes of the hidden layers based on mode (encode/decode)
+        hidden_sizes = self._compute_hidden_sizes(input_size, output_size, num_hidden_layers, mode)
 
+        # Choose the activation function
         if activation == 'leakyrelu' or activation == 'leaky_relu':
             activation_func = nn.LeakyReLU()
         elif activation == 'relu':
@@ -134,12 +150,30 @@ class Dense_Layers(nn.Module):
     def forward(self, x):
         return self.network(x)
 
-    def _compute_hidden_sizes(self, input_size, latent_size, num_hidden_layers):
-        # Calculate the ratio for geometric progression
-        r = (latent_size / input_size) ** (1 / num_hidden_layers)
-
-        # Create the hidden sizes list using geometric progression
-        hidden_sizes = [int(input_size * (r ** (i + 1))) for i in range(num_hidden_layers)]
+    def _compute_hidden_sizes(self, input_size, output_size, num_hidden_layers, mode='encode'):
+        """
+        Computes the hidden sizes based on either encoding or decoding.
+        
+        Parameters:
+            input_size (int): Input size of the first layer.
+            output_size (int): Final output size (latent size for encoding, input size for decoding).
+            num_hidden_layers (int): Number of hidden layers.
+            mode (str): Whether to compute for encoding (compression) or decoding (expansion).
+        
+        Returns:
+            List of hidden layer sizes.
+        """
+        if mode == 'encode':
+            # For encoding: progressively reduce the size
+            r = (output_size / input_size) ** (1 / num_hidden_layers)
+            hidden_sizes = [int(input_size * (r ** (i + 1))) for i in range(num_hidden_layers)]
+        elif mode == 'decode':
+            # For decoding: progressively increase the size
+            r = (input_size / output_size) ** (1 / num_hidden_layers)
+            hidden_sizes = [int(output_size * (r ** (i + 1))) for i in range(num_hidden_layers - 1, -1, -1)]
+        else:
+            raise ValueError("mode should be 'encode' or 'decode'")
+        
         return hidden_sizes
 
 
@@ -190,22 +224,26 @@ class VAE(nn.Module):
         self.kl_weight = 1.0
         self.latent_weight = 0.5
 
+
+        # Encoder: compress input_size -> 2*latent_size (mu + log_var)
         self.encoder = Dense_Layers(input_size=input_size, 
-                                    output_size=2*latent_size, 
+                                    output_size=2 * latent_size, 
                                     num_hidden_layers=num_hidden_layers,
-                                    dropout_rate = dropout_rate,
-                                    activation = activation,
+                                    dropout_rate=dropout_rate,
+                                    activation=activation,
                                     use_batch_norm=use_batch_norm, 
-                                    act_on_output_layer=act_on_latent_layer)
-        
-        self.decoder = Dense_Layers(input_size=latent_size, 
-                                    output_size = input_size,
-                                    num_hidden_layers = num_hidden_layers, 
-                                    dropout_rate = dropout_rate,
-                                    activation = activation,
-                                    use_batch_norm = use_batch_norm,
-                                    act_on_output_layer=act_on_latent_layer)
+                                    act_on_output_layer=act_on_latent_layer,
+                                    mode='encode')  # Use mode='encode' for compression
 
+        # Decoder: expand latent_size -> input_size
+        self.decoder = Dense_Layers(input_size=2 * latent_size, 
+                                    output_size=input_size,
+                                    num_hidden_layers=num_hidden_layers,
+                                    dropout_rate=dropout_rate,
+                                    activation=activation,
+                                    use_batch_norm=use_batch_norm,
+                                    act_on_output_layer=act_on_latent_layer,
+                                    mode='decode')  # Use mode='decode' for expansion
 
         # self.encoder = Dense_Layers(input_size, hidden_size, 2*latent_size, 
         #                             num_hidden_layers, dropout_rate, activation,
diff --git a/ml/run_pretraining_LP_min_test.py b/ml/run_pretraining_LP_min_test.py
index 14c03cc..24610ab 100644
--- a/ml/run_pretraining_LP_min_test.py
+++ b/ml/run_pretraining_LP_min_test.py
@@ -98,7 +98,7 @@ def get_study_kwargs(head_kwargs_dict, adv_kwargs_dict,
 
                              batch_size=96, batch_size_min=32, batch_size_max=128, batch_size_step=32,
                              noise_factor=None, noise_factor_min=0, noise_factor_max=0.25, noise_factor_step=0.05,
-                             num_epochs=100, num_epochs_min=50, num_epochs_max=400, num_epochs_step=25,
+                             num_epochs=10, num_epochs_min=50, num_epochs_max=400, num_epochs_step=25,
                              num_epochs_log=False,
                              learning_rate=None, learning_rate_min=0.00001, learning_rate_max=0.005,
                              learning_rate_step=None, learning_rate_log=True,
