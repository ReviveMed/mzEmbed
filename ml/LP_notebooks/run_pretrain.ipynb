{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from prep_run import create_selected_data, make_kwargs_set, get_task_head_kwargs, round_kwargs_to_sig, flatten_dict, \\\n",
    "    unflatten_dict\n",
    "from utils_neptune import get_latest_dataset, get_run_id_list\n",
    "from setup3 import setup_neptune_run\n",
    "\n",
    "from prep_study2 import objective_func4, reuse_run, get_study_objective_keys, get_study_objective_directions, \\\n",
    "    add_runs_to_study\n",
    "from prep_run import get_selection_df, convert_model_kwargs_list_to_dict, convert_distributions_to_suggestion\n",
    "# import torch_cpu_loader\n",
    "\n",
    "## \n",
    "# %% Load the latest data\n",
    "\n",
    "NEPTUNE_API_TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwZDlmZGM4ZC05OGM2LTQ2YzctYmRhNi0zMjIwODMzMWM1ODYifQ=='\n",
    "WEBAPP_DB_LOC = 'mysql://root:zm6148mz@34.134.200.45/mzlearn_webapp_DB'\n",
    "\n",
    "project_id = 'revivemed/RCC'\n",
    "\n",
    "USE_WEBAPP_DB = True\n",
    "SAVE_TRIALS = True\n",
    "ADD_EXISTING_RUNS_TO_STUDY = False\n",
    "limit_add = -1  # limit the number of runs added to the study\n",
    "\n",
    "encoder_kind = 'VAE'\n",
    "\n",
    "STUDY_DICT = {\n",
    "    # oputuna study parameters\n",
    "    # study name is the optuna optimization study name\n",
    "    'study_name': 'pretrain latent penalty w=0.001, KL annealing',\n",
    "    'encoder_kind': encoder_kind,\n",
    "    'objectives': {\n",
    "        'reconstruction_loss': {\n",
    "            'weight': 3,\n",
    "            'name': 'Reconstruction Loss',\n",
    "            'direction': 'minimize',\n",
    "            'transform': '',\n",
    "            'default_value': 9999\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_study_kwargs(head_kwargs_dict, adv_kwargs_dict,\n",
    "                     latent_size_min=100, latent_size_max=254,\n",
    "                     num_hidden_layers_min=2, num_hidden_layers_max=4):\n",
    "    kwargs = make_kwargs_set(encoder_kind=encoder_kind,\n",
    "                             head_kwargs_dict=head_kwargs_dict,\n",
    "                             adv_kwargs_dict=adv_kwargs_dict,\n",
    "\n",
    "                             # vae model paremeters\n",
    "                             # if you want to skip tuning, change the None the desired value\n",
    "                             # -1 means the parameter was not used\n",
    "                             # fixed value is fixed to the value\n",
    "                             latent_size=None, latent_size_min=latent_size_min, latent_size_max=latent_size_max,\n",
    "                             latent_size_step=50,\n",
    "                             hidden_size=-1, hidden_size_min=16, hidden_size_max=64, hidden_size_step=16,\n",
    "                             hidden_size_mult=1.5, hidden_size_mult_min=1.25, hidden_size_mult_max=2,\n",
    "                             hidden_size_mult_step=0.25,\n",
    "                             num_hidden_layers=None, num_hidden_layers_min=num_hidden_layers_min,\n",
    "                             num_hidden_layers_max=num_hidden_layers_max,\n",
    "                             num_hidden_layers_step=1,\n",
    "\n",
    "                             # metabol-transformer model parameters\n",
    "                             num_attention_heads=-1, num_attention_heads_min=1, num_attention_heads_max=5,\n",
    "                             num_attention_heads_step=1,\n",
    "                             num_decoder_layers=-1, num_decoder_layers_min=1, num_decoder_layers_max=5,\n",
    "                             num_decoder_layers_step=1,\n",
    "                             embed_dim=-1, embed_dim_min=4, embed_dim_max=64, embed_dim_step=4,\n",
    "                             decoder_embed_dim=-1, decoder_embed_dim_min=4, decoder_embed_dim_max=64,\n",
    "                             decoder_embed_dim_step=4,\n",
    "                             default_hidden_fraction=-1, default_hidden_fraction_min=0, default_hidden_fraction_max=0.5,\n",
    "                             default_hidden_fraction_step=0.1,\n",
    "\n",
    "                             dropout_rate=0, dropout_rate_min=0, dropout_rate_max=0.5, dropout_rate_step=0.1,\n",
    "                             encoder_weight=1.0, encoder_weight_min=0, encoder_weight_max=2, encoder_weight_step=0.5,\n",
    "                             head_weight=1.0, head_weight_min=0, head_weight_max=2, head_weight_step=0.5,\n",
    "                             adv_weight=1.0, adv_weight_min=0, adv_weight_max=2, adv_weight_step=0.5,\n",
    "\n",
    "                             task_head_weight=None, task_head_weight_min=0, task_head_weight_max=15,\n",
    "                             task_head_weight_step=0.05,\n",
    "                             task_adv_weight=-1, task_adv_weight_min=0, task_adv_weight_max=10,\n",
    "                             task_adv_weight_step=0.1,\n",
    "                             task_hidden_size=4, task_hidden_size_min=4, task_hidden_size_max=64,\n",
    "                             task_hidden_size_step=4,\n",
    "                             task_num_hidden_layers=1, task_num_hidden_layers_min=1, task_num_hidden_layers_max=3,\n",
    "                             task_num_hidden_layers_step=1,\n",
    "\n",
    "                             weight_decay=None, weight_decay_min=1e-5, weight_decay_max=1e-2, weight_decay_step=0.00001,\n",
    "                             weight_decay_log=True,\n",
    "                             l1_reg_weight=0, l1_reg_weight_min=0, l1_reg_weight_max=0.01, l1_reg_weight_step=0.0001,\n",
    "                             l1_reg_weight_log=False,\n",
    "                             l2_reg_weight=0, l2_reg_weight_min=0, l2_reg_weight_max=0.01, l2_reg_weight_step=0.0001,\n",
    "                             l2_reg_weight_log=False,\n",
    "\n",
    "                             batch_size=96, batch_size_min=32, batch_size_max=128, batch_size_step=32,\n",
    "                             noise_factor=None, noise_factor_min=0, noise_factor_max=0.25, noise_factor_step=0.05,\n",
    "                             num_epochs=None, num_epochs_min=50, num_epochs_max=400, num_epochs_step=25,\n",
    "                             num_epochs_log=False,\n",
    "                             learning_rate=None, learning_rate_min=0.00001, learning_rate_max=0.005,\n",
    "                             learning_rate_step=None, learning_rate_log=True,\n",
    "                             early_stopping_patience=0, early_stopping_patience_min=0, early_stopping_patience_max=50,\n",
    "                             early_stopping_patience_step=5,\n",
    "                             adversarial_start_epoch=0, adversarial_start_epoch_min=-1, adversarial_start_epoch_max=10,\n",
    "                             adversarial_start_epoch_step=2,\n",
    "                             )\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_api_token = NEPTUNE_API_TOKEN\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "input_data_dir = f'{homedir}/INPUT_DATA'\n",
    "output_dir = f'{homedir}/PROCESSED_DATA'\n",
    "setup_id = 'pretrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STUDY_INFO_DICT = STUDY_DICT\n",
    "\n",
    "def compute_objective(run_id):\n",
    "    return objective_func4(run_id,\n",
    "                            study_info_dict=STUDY_INFO_DICT,\n",
    "                            project_id=project_id,\n",
    "                            neptune_api_token=NEPTUNE_API_TOKEN,\n",
    "                            setup_id=setup_id,\n",
    "                            eval_file_id=eval_file_id)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    try:\n",
    "        kwargs = get_study_kwargs(head_kwargs_dict, adv_kwargs_dict,\n",
    "                                    latent_size_min=latent_size_min, latent_size_max=latent_size_max,\n",
    "                                    num_hidden_layers_min=num_hidden_layers_min,\n",
    "                                    num_hidden_layers_max=num_hidden_layers_max)\n",
    "\n",
    "        kwargs = convert_model_kwargs_list_to_dict(kwargs)\n",
    "        kwargs = flatten_dict(kwargs)  # flatten the dict for optuna compatibility\n",
    "        kwargs = convert_distributions_to_suggestion(kwargs,\n",
    "                                                        trial)  # convert the distributions to optuna suggestions\n",
    "        kwargs = round_kwargs_to_sig(kwargs, sig_figs=2)\n",
    "        kwargs = unflatten_dict(kwargs)  # unflatten the dict for the setup function\n",
    "\n",
    "        kwargs['study_info_dict'] = STUDY_INFO_DICT\n",
    "\n",
    "        run_id = setup_neptune_run(input_data_dir,\n",
    "                                    setup_id=setup_id,\n",
    "                                    project_id=project_id,\n",
    "\n",
    "                                    neptune_mode='async',\n",
    "                                    yes_logging=True,\n",
    "                                    neptune_api_token=neptune_api_token,\n",
    "                                    tags=[tag],\n",
    "                                    y_head_cols=y_head_cols,\n",
    "                                    y_adv_cols=y_adv_cols,\n",
    "                                    num_repeats=1,\n",
    "\n",
    "                                    run_training=True,\n",
    "                                    X_fit_file=X_fit_file,\n",
    "                                    y_fit_file=y_fit_file,\n",
    "                                    train_name=fit_file_id,\n",
    "\n",
    "                                    run_evaluation=True,\n",
    "                                    X_eval_file=X_eval_file,\n",
    "                                    y_eval_file=y_eval_file,\n",
    "                                    eval_name=eval_file_id,\n",
    "\n",
    "                                    save_latent_space=True,\n",
    "                                    plot_latent_space_cols=plot_latent_space_cols,\n",
    "                                    plot_latent_space='sns',\n",
    "\n",
    "                                    # with_run_id=with_run_id,\n",
    "                                    # load_model_from_run_id=None,\n",
    "                                    # load_model_loc = None,\n",
    "                                    # load_encoder_loc= 'pretrain',\n",
    "\n",
    "                                    **kwargs)\n",
    "\n",
    "        trial.set_user_attr('run_id', run_id)\n",
    "        trial.set_user_attr('setup_id', setup_id)\n",
    "\n",
    "        return compute_objective(run_id)\n",
    "\n",
    "    # except Exception as e:\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        # return float('nan')\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "if USE_WEBAPP_DB:\n",
    "    print('using webapp database')\n",
    "    storage_name = WEBAPP_DB_LOC\n",
    "\n",
    "if 'study_name' in STUDY_INFO_DICT:\n",
    "    if 'encoder_kind' in STUDY_INFO_DICT:\n",
    "        study_name = STUDY_INFO_DICT['study_name'] + f' {STUDY_INFO_DICT[\"encoder_kind\"]}'\n",
    "    else:\n",
    "        study_name = STUDY_INFO_DICT['study_name']\n",
    "else:\n",
    "    study_name = f'{encoder_kind} Study'\n",
    "\n",
    "study = optuna.create_study(directions=get_study_objective_directions(STUDY_INFO_DICT),\n",
    "                            study_name=study_name,\n",
    "                            storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=num_trials)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
