{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import shutil\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/hilic_pos_2024_feb_05_read_norm_poolmap/subset all_studies with align score 0.25 from Merge_Jan25_align_80_40_fillna_avg/num_cohorts_thresh_0.5/std_1_Multi'\n",
    "save_dir = '/Users/jonaheaton/Desktop/mskcc_study_feb13'\n",
    "\n",
    "\n",
    "X_finetune_file = 'X_finetune.csv'\n",
    "y_finetune_file = 'y_finetune.csv'\n",
    "X_pretrain_file = 'X_pretrain.csv'\n",
    "\n",
    "n_repeats = 6\n",
    "cv_splits = 5\n",
    "cross_val_seed = 42\n",
    "y_stratify_col = 'MSKCC'\n",
    "\n",
    "\n",
    "# save the pretraining data to the save_dir\n",
    "# X_pretrain = pd.read_csv(os.path.join(input_dir, X_pretrain_file), index_col=0)\n",
    "# X_pretrain.to_csv(os.path.join(save_dir, 'X_pretrain.csv'))\n",
    "shutil.copy(os.path.join(input_dir, X_pretrain_file), os.path.join(save_dir, 'X_pretrain.csv'))\n",
    "\n",
    "X_finetune = pd.read_csv(os.path.join(input_dir, X_finetune_file), index_col=0)\n",
    "y_finetune = pd.read_csv(os.path.join(input_dir, y_finetune_file), index_col=0)\n",
    "y_stratify = y_finetune[y_stratify_col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(n_repeats):\n",
    "    skf = StratifiedKFold(n_splits=cv_splits, random_state=cross_val_seed+iter, shuffle=True)\n",
    "    skf.get_n_splits(X_finetune, y_stratify)\n",
    "\n",
    "    # create a subdirectory for each of the cross-validation splits\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X_finetune, y_stratify)):\n",
    "\n",
    "        subset_id = iter * cv_splits + i\n",
    "        X_train, X_val = X_finetune.iloc[train_index], X_finetune.iloc[test_index]\n",
    "        y_train, y_val = y_finetune.iloc[train_index], y_finetune.iloc[test_index]\n",
    "        X_train.to_csv(os.path.join(save_dir, f'X_train_{subset_id}.csv'))\n",
    "        X_val.to_csv(os.path.join(save_dir, f'X_test_{subset_id}.csv'))\n",
    "        y_train.to_csv(os.path.join(save_dir, f'y_train_{subset_id}.csv'))\n",
    "        y_val.to_csv(os.path.join(save_dir, f'y_test_{subset_id}.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import ClassifierDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = save_dir\n",
    "finetune_label_col = 'MSKCC'\n",
    "finetune_label_encoder = {'FAVORABLE': 1, 'POOR': 0, 'INTERMEDIATE': np.nan}\n",
    "finetune_dataset = ClassifierDataset(data_dir, \n",
    "                            subset='train_{}'.format(subset_id),\n",
    "                            label_col=finetune_label_col,\n",
    "                            label_encoder=finetune_label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = finetune_dataset.y\n",
    "# convert tensor to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = 1 / torch.bincount(y_vals.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0090, 0.0049])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "24\n",
      "18\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_sz = len(finetune_dataset)\n",
    "pretrain_batch_size = 23\n",
    "print(train_sz // pretrain_batch_size)\n",
    "new_batch_sz = (train_sz // (train_sz // pretrain_batch_size))\n",
    "print(new_batch_sz)\n",
    "\n",
    "# what is the remainder?\n",
    "print(train_sz % pretrain_batch_size)\n",
    "print(train_sz % new_batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_batch_sz(len_dataset, batch_sz):\n",
    "    curr_remainder = len_dataset % batch_sz\n",
    "    new_batch_sz = int(len_dataset/np.ceil(len_dataset / batch_sz))\n",
    "    new_remainder = len_dataset % new_batch_sz\n",
    "    print(f'curr_remainder: {curr_remainder}, new_remainder: {new_remainder}')\n",
    "    return new_batch_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_remainder: 18, new_remainder: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clean_batch_sz(train_sz, pretrain_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.782608695652174"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sz / pretrain_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(train_sz / pretrain_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzlearn_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
