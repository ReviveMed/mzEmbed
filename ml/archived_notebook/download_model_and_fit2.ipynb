{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import neptune\n",
    "from models import create_compound_model_from_info, create_pytorch_model_from_info, MultiHead\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from utils_neptune import check_if_path_in_struc, get_sub_struc_from_path\n",
    "from collections import defaultdict\n",
    "\n",
    "NEPTUNE_API_TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxMGM5ZDhiMy1kOTlhLTRlMTAtOGFlYy1hOTQzMDE1YjZlNjcifQ=='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 'RCC-2925'\n",
    "\n",
    "task_id = 'Optimized_NIVO-OS ADV EVER-OS__133_finetune'\n",
    "local_dir = os.path.expanduser('~/Desktop/saved_models')\n",
    "model_dir = f'{local_dir}/{run_id}/{task_id}'\n",
    "components_dir = f'{local_dir}/{run_id}/{task_id}/components'\n",
    "os.makedirs(components_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/revivemed/RCC/e/RCC-2925\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(project='revivemed/RCC',\n",
    "    api_token= NEPTUNE_API_TOKEN,\n",
    "    with_id=run_id,\n",
    "    mode=\"read-only\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_kwargs = run[task_id+'/original_kwargs'].fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/revivemed/RCC/e/RCC-2925/metadata\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/revivemed/RCC/e/RCC-2925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8f485fb7864824bae613813bbc5151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching file...: 0 [00:00, ?/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/revivemed/RCC/e/RCC-2925/metadata\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = neptune.init_run(project='revivemed/RCC',\n",
    "    api_token= NEPTUNE_API_TOKEN,\n",
    "    with_id=run_id,\n",
    "    mode=\"read-only\")   \n",
    "run_struc= run.get_structure()\n",
    "\n",
    "substruc = get_sub_struc_from_path(run_struc,f'{task_id}/models')\n",
    "for key in substruc.keys():\n",
    "    if 'info' in key:\n",
    "        run[f'{task_id}/models/{key}'].download(f'{components_dir}/{key}.json')\n",
    "    elif 'state' in key:\n",
    "        run[f'{task_id}/models/{key}'].download(f'{components_dir}/{key}.pt')\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: training is not a valid argument for VAE\n",
      "Warning: goal is not a valid argument for VAE\n",
      "Warning: kind is not a valid argument for VAE\n",
      "Warning: file_id is not a valid argument for VAE\n",
      "Warning: kl_weight is not a valid argument for VAE\n",
      "Warning: kind is not a valid argument for Dense_Layers\n",
      "Warning: name is not a valid argument for Dense_Layers\n",
      "Warning: weight is not a valid argument for Dense_Layers\n",
      "Warning: y_idx is not a valid argument for Dense_Layers\n",
      "Warning: num_classes is not a valid argument for Dense_Layers\n"
     ]
    }
   ],
   "source": [
    "# load the relavent encoder info and head info\n",
    "head_key = 'Cox_NIVO'\n",
    "model_files = os.listdir(components_dir)\n",
    "\n",
    "encoder_info = None\n",
    "head_info = None\n",
    "encoder_state = None\n",
    "head_state = None\n",
    "\n",
    "for f in model_files:\n",
    "    if ('encoder' in f):\n",
    "        if 'info' in f:\n",
    "            encoder_info = json.load(open(f'{components_dir}/{f}'))\n",
    "        elif 'state' in f:\n",
    "            encoder_state = torch.load(f'{components_dir}/{f}')\n",
    "    if (head_key in f):\n",
    "        if 'info' in f:\n",
    "            head_info = json.load(open(f'{components_dir}/{f}'))\n",
    "            # head_name = f.replace('_info.json', '')\n",
    "        elif 'state' in f:\n",
    "            head_state = torch.load(f'{components_dir}/{f}')\n",
    "            \n",
    "            \n",
    "if encoder_info is not None and head_info is not None:            \n",
    "    model = create_compound_model_from_info(encoder_info=encoder_info, \n",
    "                                            head_info= head_info,\n",
    "                                            encoder_state_dict=encoder_state,\n",
    "                                            head_state_dict=head_state)\n",
    "    \n",
    "    model.save_info(model_dir, f'VAE {head_key} info.json')\n",
    "    model.save_state_to_path(model_dir, f'VAE {head_key} state.pt')\n",
    "\n",
    "    skmodel = create_pytorch_model_from_info(full_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/app/finetune_data'\n",
    "data_dir= '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_finetune_optimization/April_30_Finetune_Data'\n",
    "X_data = pd.read_csv(f'{data_dir}/X_finetune_test.csv',index_col=0)\n",
    "y_data = pd.read_csv(f'{data_dir}/y_finetune_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " 'goal': 'survival',\n",
       " 'kind': 'Cox',\n",
       " 'name': 'NIVO OS',\n",
       " 'y_idx': [0, 1],\n",
       " 'weight': 1,\n",
       " 'input_size': 109,\n",
       " 'output_size': 1,\n",
       " 'architecture': {'kind': 'Cox',\n",
       "  'name': 'NIVO OS',\n",
       "  'weight': 1,\n",
       "  'y_idx': [0, 1],\n",
       "  'hidden_size': 4,\n",
       "  'num_hidden_layers': 1,\n",
       "  'dropout_rate': 0,\n",
       "  'activation': 'leakyrelu',\n",
       "  'use_batch_norm': False,\n",
       "  'num_classes': 1,\n",
       "  'input_size': 109},\n",
       " 'score_func_dict': {'Concordance Index': '<function Cox_Head.__init__.<locals>.<lambda> at 0x7f0f07157670>'},\n",
       " 'file_id': 'Cox_NIVO OS',\n",
       " 'loss_reduction': 'mean'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompoundModel(\n",
       "  (encoder): VAE(\n",
       "    (encoder): Dense_Layers(\n",
       "      (network): Sequential(\n",
       "        (0): Linear(in_features=2736, out_features=162, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.4, inplace=False)\n",
       "        (hidden_layer): Sequential(\n",
       "          (0): Linear(in_features=162, out_features=162, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (output_layer): Sequential(\n",
       "          (0): Linear(in_features=162, out_features=216, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): Dense_Layers(\n",
       "      (network): Sequential(\n",
       "        (0): Linear(in_features=108, out_features=162, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.4, inplace=False)\n",
       "        (hidden_layer): Sequential(\n",
       "          (0): Linear(in_features=162, out_features=162, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (output_layer): Sequential(\n",
       "          (0): Linear(in_features=162, out_features=2736, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Cox_Head(\n",
       "    (network): Dense_Layers(\n",
       "      (network): Sequential(\n",
       "        (0): Linear(in_features=109, out_features=4, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (output_layer): Sequential(\n",
       "          (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (loss_func): CoxPHLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skmodel.get_params()['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = skmodel.predict(X_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Concordance Index': 0.6098095902701625}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skmodel.score(X_data.to_numpy(),y_data[['OS','OS_Event']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Concordance Index': 0.6371973587674248}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skmodel.score(X_data.to_numpy(),y_data[['NIVO OS','OS_Event']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Concordance Index': 0.5876379690949227}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skmodel.score(X_data.to_numpy(),y_data[['EVER OS','OS_Event']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_survival_report(task_id,pretrained=True):\n",
    "    if ('finetune' in task_id) or ('randinit' in task_id):\n",
    "        finetune_id = task_id\n",
    "    else:\n",
    "        if pretrained:\n",
    "            finetune_id = task_id + '_finetune'\n",
    "        else:\n",
    "            finetune_id = task_id + '_randinit'\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_survival_report(desc_str,head_key,pretrained=True,data_dir=None,local_dir=None):\n",
    "    if data_dir is None:\n",
    "        data_dir = '/app/finetune_data'\n",
    "    if local_dir is None:\n",
    "        local_dir = os.path.expanduser('~/saved_models')\n",
    "    if ('OS' not in head_key):\n",
    "        raise ValueError('head_key must be an OS')\n",
    "    \n",
    "    if ('finetune' in desc_str) or ('randinit' in desc_str):\n",
    "        task_id = desc_str\n",
    "    else:\n",
    "        if pretrained:\n",
    "            task_id = desc_str + '_finetune'\n",
    "        else:\n",
    "            task_id = desc_str + '_randinit'\n",
    "\n",
    "    model_dir = f'{local_dir}/{run_id}/{task_id}'\n",
    "    components_dir = f'{local_dir}/{run_id}/{task_id}/components'\n",
    "    os.makedirs(components_dir, exist_ok=True)\n",
    "    model_files = os.listdir(components_dir)\n",
    "    \n",
    "    if (len(model_files) < 4):\n",
    "\n",
    "        run = neptune.init_run(project='revivemed/RCC',\n",
    "            api_token= NEPTUNE_API_TOKEN,\n",
    "            with_id=run_id,\n",
    "            mode=\"read-only\")   \n",
    "        run_struc= run.get_structure()\n",
    "\n",
    "        original_kwargs=run[task_id+'/original_kwargs'].fetch()\n",
    "        json.dump(original_kwargs,open(f'{components_dir}/original_kwargs.json','w'),indent=4)\n",
    "        substruc = get_sub_struc_from_path(run_struc,f'{task_id}/models')\n",
    "        for key in substruc.keys():\n",
    "            if 'info' in key:\n",
    "                run[f'{task_id}/models/{key}'].download(f'{components_dir}/{key}.json')\n",
    "            elif 'state' in key:\n",
    "                run[f'{task_id}/models/{key}'].download(f'{components_dir}/{key}.pt')\n",
    "\n",
    "        run.stop()\n",
    "        model_files = os.listdir(components_dir)\n",
    "\n",
    "\n",
    "    encoder_info = None\n",
    "    head_info = None\n",
    "    encoder_state = None\n",
    "    head_state = None\n",
    "\n",
    "    for f in model_files:\n",
    "        if ('encoder' in f):\n",
    "            if 'info' in f:\n",
    "                encoder_info = json.load(open(f'{components_dir}/{f}'))\n",
    "            elif 'state' in f:\n",
    "                encoder_state = torch.load(f'{components_dir}/{f}')\n",
    "        if (head_key in f):\n",
    "            if 'info' in f:\n",
    "                head_info = json.load(open(f'{components_dir}/{f}'))\n",
    "                # head_name = f.replace('_info.json', '')\n",
    "            elif 'state' in f:\n",
    "                head_state = torch.load(f'{components_dir}/{f}')\n",
    "        if 'oringal_kwargs' in f:\n",
    "            original_kwargs = json.load(open(f'{components_dir}/{f}'))\n",
    "                \n",
    "    if encoder_info is not None and head_info is not None:            \n",
    "        model = create_compound_model_from_info(encoder_info=encoder_info, \n",
    "                                                head_info= head_info,\n",
    "                                                encoder_state_dict=encoder_state,\n",
    "                                                head_state_dict=head_state)\n",
    "        \n",
    "        params = {}\n",
    "        params['encoder dropout_rate'] = encoder_info['dropout_rate']\n",
    "        params['head name'] = head_info['name']\n",
    "        params['head weight'] = head_info['weight']*original_kwargs['train_kwargs']['head_weight']\n",
    "        params['head layers']= head_info['architecture']['num_hidden_layers']\n",
    "        params['num auxillary heads'] = len(original_kwargs['head_kwargs_dict']) -1\n",
    "        \n",
    "        params['num adversarial heads'] = len(original_kwargs['adversarial_head_kwargs_dict'])\n",
    "        params['adversary weight'] = original_kwargs['train_kwargs']['adversary_weight']\n",
    "        params['adversarial_start_epoch'] = original_kwargs['train_kwargs']['adversarial_start_epoch']\n",
    "        if params['adversary weight'] == 0 or params['num adversarial heads'] == 0:\n",
    "            params['adversary weight'] = 0\n",
    "            params['num adversarial heads'] = 0\n",
    "            params['adversarial_start_epoch'] = 0\n",
    "        \n",
    "        params['encoder weight'] = original_kwargs['train_kwargs']['encoder_weight']\n",
    "        params['learning rate'] = original_kwargs['train_kwargs']['learning_rate']\n",
    "        params['l1_reg_weight'] = original_kwargs['train_kwargs']['l1_reg_weight']\n",
    "        params['l2_reg_weight'] = original_kwargs['train_kwargs']['l2_reg_weight']\n",
    "        params['noise_factor'] = original_kwargs['train_kwargs']['noise_factor']\n",
    "        params['num_epochs'] = original_kwargs['train_kwargs']['num_epochs']\n",
    "        params['weight_decay'] = original_kwargs['train_kwargs']['weight_decay']\n",
    "\n",
    "        \n",
    "        model.save_info(model_dir, f'VAE {head_key} info.json')\n",
    "        model.save_state_to_path(model_dir, f'VAE {head_key} state.pt')\n",
    "\n",
    "        skmodel = create_pytorch_model_from_info(full_model=model)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "    X_data = pd.read_csv(f'{data_dir}/X_finetune_test.csv',index_col=0)\n",
    "    y_data = pd.read_csv(f'{data_dir}/y_finetune_test.csv',index_col=0)\n",
    "\n",
    "\n",
    "    score_dict = {}\n",
    "    score_dict['both-OS'] = skmodel.score(X_data.to_numpy(),y_data[['OS','OS_Event']].to_numpy())['Concordance Index']\n",
    "    score_dict['NIVO-OS'] = skmodel.score(X_data.to_numpy(),y_data[['NIVO OS','OS_Event']].to_numpy())['Concordance Index']\n",
    "    score_dict['EVER-OS'] = skmodel.score(X_data.to_numpy(),y_data[['EVER OS','OS_Event']].to_numpy())['Concordance Index']\n",
    "    \n",
    "    res_dict = {\n",
    "        'test c-index' : score_dict,\n",
    "        'params' : params\n",
    "    }\n",
    "\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jonaheaton/Desktop/saved_models'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_finetune_optimization/April_30_Finetune_Data'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m desc_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimized_NIVO-OS ADV EVER-OS_finetune\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m report_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m({})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "desc_str = 'Optimized_NIVO-OS ADV EVER-OS_finetune'\n",
    "report_dict = defaultdict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_str = 'Optimized_NIVO-OS ADV EVER-OS_finetune'\n",
    "report_dict = defaultdict({})\n",
    "\n",
    "\n",
    "for head_key in ['NIVO OS', 'EVER OS', 'OS']:\n",
    "    report_df[head_key] = generate_survival_report(desc_str,head_key,pretrained=True,data_dir=data_dir,local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run= neptune.init_run(\n",
    "        project='revivemed/RCC',\n",
    "        api_token= NEPTUNE_API_TOKEN,\n",
    "        with_id='RCC-2925',\n",
    "        mode='read-only',\n",
    ")\n",
    "\n",
    "run_struc = run.get_structure()\n",
    "\n",
    "num_trials_key = 'original_kwargs/optimized_study_info/number of total trials'\n",
    "chosen_trials_key = 'original_kwargs/optimized_study_info/best trial'\n",
    "res_dict = defaultdict(dict)\n",
    "run_struc = run.get_structure()\n",
    "for task_key in run_struc.keys():\n",
    "    if 'Optimized_' in task_key:\n",
    "        \n",
    "        if check_if_path_in_struc(run_struc[task_key],num_trials_key):\n",
    "            num_trials = run[task_key][num_trials_key].fetch()\n",
    "            chosen_trial = run[task_key][chosen_trials_key].fetch()\n",
    "            res_dict[task_key]['num_trials'] = num_trials\n",
    "            res_dict[task_key]['chosen_trial'] = chosen_trial\n",
    "\n",
    "        if 'avg' in run_struc[task_key].keys():\n",
    "            eval_res_dict = run[task_key]['avg'].fetch()\n",
    "            res_dict[task_key].update(eval_res_dict)\n",
    "\n",
    "\n",
    "pd.DataFrame(res_dict).T.to_csv('~/Desktop/neptune_results 3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
