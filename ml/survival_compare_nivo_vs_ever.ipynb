{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import neptune\n",
    "from models import create_compound_model_from_info, create_pytorch_model_from_info, MultiHead\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from utils_neptune import check_if_path_in_struc, get_sub_struc_from_path\n",
    "from collections import defaultdict\n",
    "\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "\n",
    "\n",
    "NEPTUNE_API_TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxMGM5ZDhiMy1kOTlhLTRlMTAtOGFlYy1hOTQzMDE1YjZlNjcifQ=='\n",
    "# data_dir= '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_finetune_optimization/April_30_Finetune_Data'\n",
    "# local_dir = os.path.expanduser('~/Desktop/saved_models')\n",
    "\n",
    "data_dir= '/app/finetune_data'\n",
    "output_dir = '/app/mz_embed_engine/output'\n",
    "local_dir = os.path.expanduser('~/saved_models')\n",
    "run_id = 'RCC-2925'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Survival Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: training is not a valid argument for VAE\n",
      "Warning: goal is not a valid argument for VAE\n",
      "Warning: kind is not a valid argument for VAE\n",
      "Warning: file_id is not a valid argument for VAE\n",
      "Warning: kl_weight is not a valid argument for VAE\n",
      "Warning: kind is not a valid argument for Dense_Layers\n",
      "Warning: name is not a valid argument for Dense_Layers\n",
      "Warning: weight is not a valid argument for Dense_Layers\n",
      "Warning: y_idx is not a valid argument for Dense_Layers\n",
      "Warning: num_classes is not a valid argument for Dense_Layers\n"
     ]
    }
   ],
   "source": [
    "task_id = 'Optimized_NIVO-OS_finetune'\n",
    "head_key = 'NIVO OS'\n",
    "\n",
    "model_dir = f'{local_dir}/{run_id}/{task_id}'\n",
    "components_dir = f'{local_dir}/{run_id}/{task_id}/components'\n",
    "os.makedirs(components_dir, exist_ok=True)\n",
    "model_files = os.listdir(components_dir)\n",
    "\n",
    "if (len(model_files) < 4):\n",
    "\n",
    "    run = neptune.init_run(project='revivemed/RCC',\n",
    "        api_token= NEPTUNE_API_TOKEN,\n",
    "        with_id=run_id,\n",
    "        mode=\"read-only\")   \n",
    "    run_struc= run.get_structure()\n",
    "\n",
    "    original_kwargs=run[task_id+'/original_kwargs'].fetch()\n",
    "    json.dump(original_kwargs,open(f'{components_dir}/original_kwargs.json','w'),indent=4)\n",
    "    substruc = get_sub_struc_from_path(run_struc,f'{task_id}/models')\n",
    "    for key in substruc.keys():\n",
    "        if 'info' in key:\n",
    "            run[f'{task_id}/models/{key}'].download(f'{components_dir}/{key}.json')\n",
    "        elif 'state' in key:\n",
    "            run[f'{task_id}/models/{key}'].download(f'{components_dir}/{key}.pt')\n",
    "\n",
    "    run.stop()\n",
    "    model_files = os.listdir(components_dir)\n",
    "\n",
    "\n",
    "encoder_info = None\n",
    "head_info = None\n",
    "encoder_state = None\n",
    "head_state = None\n",
    "original_kwargs = None\n",
    "\n",
    "for f in model_files:\n",
    "    if ('encoder' in f):\n",
    "        if 'info' in f:\n",
    "            encoder_info = json.load(open(f'{components_dir}/{f}'))\n",
    "        elif 'state' in f:\n",
    "            encoder_state = torch.load(f'{components_dir}/{f}')\n",
    "    if (head_key in f):\n",
    "        if 'info' in f:\n",
    "            head_info = json.load(open(f'{components_dir}/{f}'))\n",
    "            # head_name = f.replace('_info.json', '')\n",
    "        elif 'state' in f:\n",
    "            head_state = torch.load(f'{components_dir}/{f}')\n",
    "    if 'original_kwargs' in f:\n",
    "        original_kwargs = json.load(open(f'{components_dir}/{f}'))\n",
    "            \n",
    "if (encoder_info is not None) and (head_info is not None):            \n",
    "    model = create_compound_model_from_info(encoder_info=encoder_info, \n",
    "                                            head_info= head_info,\n",
    "                                            encoder_state_dict=encoder_state,\n",
    "                                            head_state_dict=head_state)\n",
    "    \n",
    "    params = {}\n",
    "    params['encoder dropout_rate'] = encoder_info['dropout_rate']\n",
    "    params['head name'] = head_info['name']\n",
    "    params['head layers']= head_info['architecture']['num_hidden_layers']\n",
    "    if (original_kwargs is not None) and ('train_kwargs' in original_kwargs):\n",
    "        params['head weight'] = head_info['weight']*original_kwargs['train_kwargs']['head_weight']\n",
    "        params['num auxillary heads'] = len(original_kwargs['head_kwargs_dict']) -1\n",
    "        \n",
    "        if 'adversarial_head_kwargs_dict' in original_kwargs:\n",
    "            params['num adversarial heads'] = len(original_kwargs['adversarial_head_kwargs_dict'])\n",
    "        else:\n",
    "            params['num adversarial heads'] = 0\n",
    "        params['adversary weight'] = original_kwargs['train_kwargs']['adversary_weight']\n",
    "        params['adversarial_start_epoch'] = original_kwargs['train_kwargs']['adversarial_start_epoch']\n",
    "        if params['adversary weight'] == 0 or params['num adversarial heads'] == 0:\n",
    "            params['adversary weight'] = 0\n",
    "            params['num adversarial heads'] = 0\n",
    "            params['adversarial_start_epoch'] = 0\n",
    "        \n",
    "        params['encoder weight'] = original_kwargs['train_kwargs']['encoder_weight']\n",
    "        params['learning rate'] = original_kwargs['train_kwargs']['learning_rate']\n",
    "        params['l1_reg_weight'] = original_kwargs['train_kwargs']['l1_reg_weight']\n",
    "        params['l2_reg_weight'] = original_kwargs['train_kwargs']['l2_reg_weight']\n",
    "        params['noise_factor'] = original_kwargs['train_kwargs']['noise_factor']\n",
    "        params['num_epochs'] = original_kwargs['train_kwargs']['num_epochs']\n",
    "        params['weight_decay'] = original_kwargs['train_kwargs']['weight_decay']\n",
    "\n",
    "    model.save_info(model_dir, f'VAE {head_key} info.json')\n",
    "    model.save_state_to_path(model_dir, f'VAE {head_key} state.pt')\n",
    "\n",
    "    skmodel = create_pytorch_model_from_info(full_model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = pd.read_csv(f'{data_dir}/X_finetune_test.csv',index_col=0)\n",
    "y_test_data = pd.read_csv(f'{data_dir}/y_finetune_test.csv',index_col=0)\n",
    "X_trainval_data = pd.read_csv(f'{data_dir}/X_finetune_trainval.csv',index_col=0)\n",
    "y_trainval_data = pd.read_csv(f'{data_dir}/y_finetune_trainval.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the predicted hazards and predicted surivals\n",
    "\n",
    "y_test_data['predicted hazard'] = skmodel.predict(X_test_data.to_numpy()).flatten()\n",
    "y_trainval_data['predicted hazard'] = skmodel.predict(X_trainval_data.to_numpy()).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the two-level threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalized function to run grid search with custom objective\n",
    "def prep_surv_y(survival_column, event_column):\n",
    "    \"\"\"\n",
    "    Function for formatting survival data as input to sksurv models.\n",
    "    Args:\n",
    "        survival_column: list of survival times\n",
    "        event_column: list of event indicators (0 for censored, 1 for event)\n",
    "    \"\"\"\n",
    "    surv_y = np.array([(event_column.iloc[i], survival_column.iloc[i]) for i in range(len(survival_column))], dtype=[('status', bool), ('time', float)])\n",
    "    # surv_y = np.array([(event_column[i], survival_column[i]) for i in range(len(survival_column))], dtype=[('status', bool), ('time', float)])\n",
    "    return surv_y\n",
    "\n",
    "def create_data_dict(data_dir,set_name,os_col,event_col,data_dict=None):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    X_path = f'{data_dir}/X_finetune_{set_name}.csv'\n",
    "    y_path = f'{data_dir}/y_finetune_{set_name}.csv'\n",
    "    X = pd.read_csv(X_path, index_col=0)\n",
    "    y = pd.read_csv(y_path, index_col=0)\n",
    "    not_nan = ~y[os_col].isna()\n",
    "    data_dict[set_name] = {\"X\": X.loc[not_nan],\n",
    "                           \"y\": prep_surv_y(y.loc[not_nan,os_col], y.loc[not_nan,event_col]),\n",
    "                           'os_col':os_col,\n",
    "                           'event_col':event_col,\n",
    "                           'X_file':X_path,\n",
    "                           'y_file':y_path}\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def get_best_cph_threshold(model_trained, new_data,min_class_size=5):\n",
    "    \"\"\"\n",
    "    Identify the threshold that maximizes C-index between classes where p remains significant. Note: this does NOT account for clinical covariates\n",
    "    Args:\n",
    "        model_trained: trained CoxPH model\n",
    "        new_data: dict with new data (transformed and scaled) and survival data\n",
    "        feats: list of features to use\n",
    "    Returns:\n",
    "        dictionary with p-value, coefficient, concordance index, and number of samples in each class\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = pd.DataFrame(new_data[\"y\"]).set_axis(['Event', 'Survival'], axis=1)\n",
    "\n",
    "    # predict survival\n",
    "    # if type(model_trained) == xgb.core.Booster:\n",
    "    #     predicted_surv = model_trained.predict(new_data[\"xgb_x\"])\n",
    "    # else: \n",
    "    predicted_surv = model_trained.predict(new_data[\"X\"].to_numpy())\n",
    "\n",
    "    # identify threshold maximizing c-index between classes where p remains significant\n",
    "    thresh_summary = {}\n",
    "    for th in np.unique(predicted_surv):\n",
    "        # predict classes\n",
    "        pred_ev = (predicted_surv > th)\n",
    "        if (sum(pred_ev) < min_class_size) or (sum(~pred_ev) < min_class_size):\n",
    "            continue\n",
    "\n",
    "        mod_d = tmp.copy()\n",
    "        mod_d['pred'] = pred_ev\n",
    "\n",
    "        if any(mod_d['pred']) & any(~mod_d['pred']):  # only look if there's at least 1 of both classes\n",
    "        \n",
    "            try:\n",
    "                cph = CoxPHFitter()\n",
    "                cph.fit(mod_d, 'Survival', 'Event', formula='pred')\n",
    "                thresh_summary[th] = {'p': cph.summary.loc['pred']['p'], 'coef': cph.summary.loc['pred']['coef'], 'c_ind': cph.concordance_index_, \"n_lo\": sum(pred_ev==0), \"n_hi\": sum(pred_ev==1)}\n",
    "            except:\n",
    "                thresh_summary[th] = {'p': np.nan, 'coef': np.nan, 'c_ind': np.nan, \"n_lo\": sum(pred_ev==0), \"n_hi\": sum(pred_ev==1)}\n",
    "\n",
    "            # calculate median survival of each class\n",
    "            kmf = KaplanMeierFitter()\n",
    "            for i in [False, True]:\n",
    "                lab_map = {True: \"lo\", False: \"hi\"}\n",
    "                kmf.fit(mod_d.loc[mod_d['pred']==i, 'Survival'], mod_d.loc[mod_d['pred']==i, 'Event'], label=f\"class_{i}\")\n",
    "                thresh_summary[th][f\"io_{lab_map[i]}_mst\"] = kmf.median_survival_time_\n",
    "\n",
    "    thresh_df = pd.DataFrame(thresh_summary).T\n",
    "    thresh_df = thresh_df.loc[thresh_df['io_hi_mst'] > thresh_df['io_lo_mst']].dropna()  # drop any where hi class has lower median survival\n",
    "    if any(thresh_df['p']<0.05):\n",
    "        best_thresh = thresh_df.loc[thresh_df['p']<0.05].sort_values('c_ind', ascending=False).iloc[0].name\n",
    "    else:\n",
    "        best_thresh = 0\n",
    "    return best_thresh, thresh_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict =create_data_dict(data_dir,'trainval','NIVO OS','OS_Event')\n",
    "# data_dict =create_data_dict(data_dir,'test','OS','OS_Event',data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['trainval']['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.245. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.251. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.256. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.261. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.267. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.272. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.276. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.281. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.286. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.290. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.295. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.299. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.303. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.312. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.316. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.320. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.323. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.327. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.331. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.334. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.338. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.341. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.345. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.348. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.351. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.355. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.358. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.361. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.364. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.367. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.370. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.373. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.376. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.378. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.381. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.384. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.387. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.389. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.392. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.394. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.397. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.399. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.402. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.404. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.406. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.409. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.411. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.413. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.415. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.417. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.419. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.422. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.424. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.426. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.428. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.429. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.431. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.433. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.435. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.437. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.439. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.440. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.442. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.444. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1614: ConvergenceWarning: Newton-Raphson failed to converge sufficiently. Please see the following tips in the lifelines documentation: https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1679: RuntimeWarning: overflow encountered in exp\n",
      "  scores = weights * exp(dot(X, beta))\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1530: LinAlgWarning: Ill-conditioned matrix (rcond=0): result may not be accurate.\n",
      "  inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1679: RuntimeWarning: overflow encountered in exp\n",
      "  scores = weights * exp(dot(X, beta))\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1530: LinAlgWarning: Ill-conditioned matrix (rcond=0): result may not be accurate.\n",
      "  inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1679: RuntimeWarning: overflow encountered in exp\n",
      "  scores = weights * exp(dot(X, beta))\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1530: LinAlgWarning: Ill-conditioned matrix (rcond=0): result may not be accurate.\n",
      "  inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1679: RuntimeWarning: overflow encountered in exp\n",
      "  scores = weights * exp(dot(X, beta))\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1530: LinAlgWarning: Ill-conditioned matrix (rcond=0): result may not be accurate.\n",
      "  inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1679: RuntimeWarning: overflow encountered in exp\n",
      "  scores = weights * exp(dot(X, beta))\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1530: LinAlgWarning: Ill-conditioned matrix (rcond=0): result may not be accurate.\n",
      "  inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1679: RuntimeWarning: overflow encountered in exp\n",
      "  scores = weights * exp(dot(X, beta))\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1530: LinAlgWarning: Ill-conditioned matrix (rcond=0): result may not be accurate.\n",
      "  inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1679: RuntimeWarning: overflow encountered in exp\n",
      "  scores = weights * exp(dot(X, beta))\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1530: LinAlgWarning: Ill-conditioned matrix (rcond=0): result may not be accurate.\n",
      "  inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.334. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column pred have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'pred'].var())\n",
      ">>> print(df.loc[~events, 'pred'].var())\n",
      "\n",
      "A very low variance means that the column pred completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.316. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_thresh,thresh_df = get_best_cph_threshold(skmodel, data_dict['trainval'], min_class_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>coef</th>\n",
       "      <th>c_ind</th>\n",
       "      <th>n_lo</th>\n",
       "      <th>n_hi</th>\n",
       "      <th>io_hi_mst</th>\n",
       "      <th>io_lo_mst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-19.573648</th>\n",
       "      <td>0.9909</td>\n",
       "      <td>18.127896</td>\n",
       "      <td>0.550746</td>\n",
       "      <td>[20]</td>\n",
       "      <td>[293]</td>\n",
       "      <td>inf</td>\n",
       "      <td>23.162218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-19.401531</th>\n",
       "      <td>0.990677</td>\n",
       "      <td>18.141931</td>\n",
       "      <td>0.553318</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[292]</td>\n",
       "      <td>inf</td>\n",
       "      <td>23.162218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-19.390272</th>\n",
       "      <td>0.990393</td>\n",
       "      <td>18.134208</td>\n",
       "      <td>0.555843</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[291]</td>\n",
       "      <td>inf</td>\n",
       "      <td>22.86653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-19.226923</th>\n",
       "      <td>0.990181</td>\n",
       "      <td>18.148796</td>\n",
       "      <td>0.558414</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[290]</td>\n",
       "      <td>inf</td>\n",
       "      <td>22.86653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-19.123745</th>\n",
       "      <td>0.990079</td>\n",
       "      <td>18.181314</td>\n",
       "      <td>0.560939</td>\n",
       "      <td>[24]</td>\n",
       "      <td>[289]</td>\n",
       "      <td>inf</td>\n",
       "      <td>22.505133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.066435</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.692085</td>\n",
       "      <td>0.579643</td>\n",
       "      <td>[289]</td>\n",
       "      <td>[24]</td>\n",
       "      <td>27.893224</td>\n",
       "      <td>2.6612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.067636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.60301</td>\n",
       "      <td>0.576592</td>\n",
       "      <td>[290]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>27.893224</td>\n",
       "      <td>2.562628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.103300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.462088</td>\n",
       "      <td>0.573202</td>\n",
       "      <td>[291]</td>\n",
       "      <td>[22]</td>\n",
       "      <td>27.893224</td>\n",
       "      <td>2.6612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.118107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.41708</td>\n",
       "      <td>0.570174</td>\n",
       "      <td>[292]</td>\n",
       "      <td>[21]</td>\n",
       "      <td>27.827515</td>\n",
       "      <td>2.4641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.120882</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.357927</td>\n",
       "      <td>0.567065</td>\n",
       "      <td>[293]</td>\n",
       "      <td>[20]</td>\n",
       "      <td>27.827515</td>\n",
       "      <td>2.135524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   p       coef     c_ind   n_lo   n_hi  io_hi_mst  io_lo_mst\n",
       "-19.573648    0.9909  18.127896  0.550746   [20]  [293]        inf  23.162218\n",
       "-19.401531  0.990677  18.141931  0.553318   [21]  [292]        inf  23.162218\n",
       "-19.390272  0.990393  18.134208  0.555843   [22]  [291]        inf   22.86653\n",
       "-19.226923  0.990181  18.148796  0.558414   [23]  [290]        inf   22.86653\n",
       "-19.123745  0.990079  18.181314  0.560939   [24]  [289]        inf  22.505133\n",
       "...              ...        ...       ...    ...    ...        ...        ...\n",
       " 2.066435        0.0   4.692085  0.579643  [289]   [24]  27.893224     2.6612\n",
       " 2.067636        0.0    4.60301  0.576592  [290]   [23]  27.893224   2.562628\n",
       " 2.103300        0.0   4.462088  0.573202  [291]   [22]  27.893224     2.6612\n",
       " 2.118107        0.0    4.41708  0.570174  [292]   [21]  27.827515     2.4641\n",
       " 2.120882        0.0   4.357927  0.567065  [293]   [20]  27.827515   2.135524\n",
       "\n",
       "[265 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.2614393\n"
     ]
    }
   ],
   "source": [
    "print(best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "IM-    310\n",
       "IM+    282\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval_data['class'] = 'IM+'\n",
    "y_trainval_data.loc[y_trainval_data['predicted hazard'] >= best_thresh, 'class'] = 'IM-'\n",
    "y_trainval_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class selection within NIVO Discovery\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class\n",
       "IM+    159\n",
       "IM-    154\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class selection within the NIVO treated patients\n",
    "print('Class selection within NIVO Discovery')\n",
    "y_trainval_data[y_trainval_data['Treatment is NIVO']==1]['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cols = ['Sex', 'Age_Group', 'Region', 'IMDC', 'Prior_2', 'class']\n",
    "\n",
    "plot_order = [\n",
    "    ('Age_Group', ' >75'),\n",
    "    ('Age_Group', '65-75'),\n",
    "    ('Age_Group', '<65'),\n",
    "    ('Sex', 'F'),\n",
    "    ('Sex', 'M'),\n",
    "    ('Prior_2', True),\n",
    "    ('Prior_2', False),\n",
    "    ('Region', 'WESTERN EUROPE'),\n",
    "    ('Region', 'US/CANADA'),\n",
    "    ('Region', 'REST OF WORLD'),\n",
    "    ('IMDC', 'POOR'),\n",
    "    ('IMDC', 'INTERMEDIATE'),\n",
    "    ('IMDC', 'FAVORABLE'),\n",
    "    ('Total', 'NIVOLUMAB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_col = 'OS'\n",
    "event_col = 'OS_Event'\n",
    "cph_df_cleaned = y_test_data.copy()\n",
    "\n",
    "cph_df_cleaned['class'] = 'IM+'\n",
    "cph_df_cleaned.loc[cph_df_cleaned['predicted hazard'] >= best_thresh, 'class'] = 'IM-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "IM-    87\n",
       "IM+    62\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cph_df_cleaned['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS OS_Event\n",
      "After dropping na from Sex, n= 149\n",
      "After dropping na from Age_Group, n= 149\n",
      "After dropping na from Region, n= 149\n",
      "After dropping na from IMDC, n= 149\n",
      "After dropping na from Prior_2, n= 149\n",
      "After dropping na from class, n= 149\n",
      "After dropping na from Sex, n= 62\n",
      "After dropping na from Age_Group, n= 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1120: ConvergenceWarning: Column Treatment[T.NIVOLUMAB] have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['OS_Event'].astype(bool)\n",
      ">>> print(df.loc[events, 'Treatment[T.NIVOLUMAB]'].var())\n",
      ">>> print(df.loc[~events, 'Treatment[T.NIVOLUMAB]'].var())\n",
      "\n",
      "A very low variance means that the column Treatment[T.NIVOLUMAB] completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/utils/__init__.py:1163: ConvergenceWarning: Column Treatment[T.NIVOLUMAB] has high sample correlation with the duration column. This may harm convergence. This could be a form of 'complete separation'.     See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "/usr/local/lib/python3.9/site-packages/lifelines/fitters/coxph_fitter.py:1607: ConvergenceWarning: Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.548. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping na from Region, n= 62\n",
      "After dropping na from IMDC, n= 62\n",
      "After dropping na from Prior_2, n= 62\n",
      "After dropping na from class, n= 62\n",
      "After dropping na from Sex, n= 87\n",
      "After dropping na from Age_Group, n= 87\n",
      "After dropping na from Region, n= 87\n",
      "After dropping na from IMDC, n= 87\n",
      "After dropping na from Prior_2, n= 87\n",
      "After dropping na from class, n= 87\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_to_use = 'black'\n",
    "\n",
    "# survival_col = 'OS'\n",
    "print(survival_col, event_col)\n",
    "\n",
    "all_summary_dl = []\n",
    "for filter_option in [\"All\", \"IM+\", \"IM-\"]:\n",
    "    all_summary_d = {}\n",
    "    cph_df_filt = cph_df_cleaned.copy()\n",
    "    if filter_option != \"All\":\n",
    "        cph_df_filt = cph_df_filt.loc[cph_df_filt['class']==filter_option].copy()\n",
    "    for col in var_cols:\n",
    "        cph_df_sub = cph_df_filt.copy()\n",
    "        cph_df_sub = cph_df_sub.dropna(subset=[col])\n",
    "        print(f\"After dropping na from {col}, n=\", cph_df_sub.shape[0])\n",
    "        cph_df_sub[col] = pd.Categorical(cph_df_sub[col])\n",
    "        # relevel categories so that each one can be reference\n",
    "        for cat_val in cph_df_sub[col].cat.categories:\n",
    "            cph_df_sub_sub = cph_df_sub.loc[cph_df_sub[col]==cat_val]\n",
    "            cph = CoxPHFitter()\n",
    "            cph_formula = 'Treatment'#*'+col # paper analysis\n",
    "            all_formula = 'Treatment'\n",
    "            try:\n",
    "                x=cph.fit(cph_df_sub_sub, duration_col=survival_col, event_col=event_col, formula=cph_formula, show_progress=False)\n",
    "                summary_table = x.summary.to_dict(orient=\"index\")\n",
    "                # summary_table = pd.DataFrame(summary_table.loc[\"Treatment[T.NIVOLUMAB]\",:]).T\n",
    "                all_summary_d[(col, cat_val)] = summary_table[\"Treatment[T.NIVOLUMAB]\"]\n",
    "                all_summary_d[(col, cat_val)]['NIVO n'] = cph_df_sub_sub.loc[(cph_df_sub_sub['Treatment']=='NIVOLUMAB') & (cph_df_sub_sub[col]==cat_val)].shape[0]\n",
    "                all_summary_d[(col, cat_val)]['EVERO n'] = cph_df_sub_sub.loc[(cph_df_sub_sub['Treatment']=='EVEROLIMUS') & (cph_df_sub_sub[col]==cat_val)].shape[0]\n",
    "                all_summary_d[(col, cat_val)]['class'] = filter_option\n",
    "            except:\n",
    "                all_summary_d[(col, cat_val)] = {\"coef\": np.nan, \"exp(coef)\": np.nan, \"se(coef)\": np.nan, \"coef lower 95%\": np.nan, \"coef upper 95%\": np.nan, \"exp(coef) lower 95%\": np.nan, \"exp(coef) upper 95%\": np.nan, \"z\": np.nan, \"p\": np.nan, \"-log2(p)\": np.nan, \"class\": filter_option}\n",
    "                all_summary_d[(col, cat_val)]['NIVO n'] = cph_df_sub_sub.loc[(cph_df_sub_sub['Treatment']=='NIVOLUMAB') & (cph_df_sub_sub[col]==cat_val)].shape[0]\n",
    "                all_summary_d[(col, cat_val)]['EVERO n'] = cph_df_sub_sub.loc[(cph_df_sub_sub['Treatment']=='EVEROLIMUS') & (cph_df_sub_sub[col]==cat_val)].shape[0]\n",
    "            # all_summary += [summary_table]\n",
    "    cph_all = CoxPHFitter()\n",
    "    x_all = cph_all.fit(cph_df_filt, duration_col=survival_col, event_col=event_col, formula=all_formula, show_progress=False)\n",
    "    summary_table = x_all.summary.to_dict(orient=\"index\")\n",
    "    all_summary_d[('Total', 'NIVOLUMAB')] = summary_table[\"Treatment[T.NIVOLUMAB]\"]\n",
    "    all_summary_d[('Total', 'NIVOLUMAB')]['NIVO n'] = cph_df_filt.loc[(cph_df_filt['Treatment']=='NIVOLUMAB')].shape[0]\n",
    "    all_summary_d[('Total', 'NIVOLUMAB')]['EVERO n'] = cph_df_filt.loc[(cph_df_filt['Treatment']=='EVEROLIMUS')].shape[0]\n",
    "    all_summary_d[('Total', 'NIVOLUMAB')]['class'] = filter_option\n",
    "\n",
    "\n",
    "    plot_table = pd.DataFrame.from_dict(all_summary_d, orient=\"index\")\n",
    "    all_summary_dl += [plot_table]\n",
    "\n",
    "plt.figure(figsize=(5, 7))\n",
    "\n",
    "col_dict = {\"IM+\": \"green\", \"IM-\": \"red\", \"All\": \"black\"}\n",
    "offset_dict = {\"IM+\": -0.25, \"IM-\": 0.25, \"All\": 0}\n",
    "\n",
    "plot_table = pd.concat(all_summary_dl)\n",
    "plot_table = plot_table.loc[['class' not in i for i in plot_table.index]]\n",
    "n_ind = dict(zip(plot_order, np.arange(len(plot_table.index.unique()))))\n",
    "for index, row in plot_table.iterrows():\n",
    "    index_name = f'{index[0]} {index[1]}'\n",
    "    if 'class' not in index_name:\n",
    "        # if any confidence interval is Inf, do not plot it\n",
    "        if row['exp(coef) lower 95%'] > 0 and row['exp(coef) upper 95%'] != np.inf:\n",
    "            # if any row has NIVO n <5 or EVERO n <5, do not plot it\n",
    "            if row['NIVO n'] >= 4 and row['EVERO n'] >= 4:\n",
    "                # offset the points slightly to avoid overlap\n",
    "                plt.plot(row['exp(coef)'], [n_ind[index]+offset_dict[row['class']]], 'o', color=col_dict[row['class']], markersize=8)\n",
    "                plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [n_ind[index]+offset_dict[row['class']], n_ind[index]+offset_dict[row['class']]], marker='', markersize=8, linestyle='-', color=col_dict[row['class']])\n",
    "        else:\n",
    "            # plot a horizontal line at 1\n",
    "            plt.plot([1, 1], [n_ind[index]+offset_dict[row['class']], n_ind[index]+offset_dict[row['class']]], marker='', markersize=8, linestyle='-', color=col_dict[row['class']])\n",
    "\n",
    "\n",
    "# limit x axis to 0.25 - 4\n",
    "# plt.xlim(0.1, 3)\n",
    "\n",
    "# add vertical dashed line at 1\n",
    "plt.axvline(x=1, linestyle='--', color='black')\n",
    "# plt.yticks(range(len(summary_table)), summary_table.index)\n",
    "plt.xscale('log')  # Use log scale for better visualization\n",
    "plt.xlabel('Hazard Ratio (exp(coef))')\n",
    "# plt.xticks([0.01, 0.1, 0.25, 0.5, 1, 2.5, 5], [0.01, 0.1, 0.25, 0.5, 1, 2.5, 5])\n",
    "\n",
    "# # Adjust axis limits\n",
    "# xmin, xmax, ymin, ymax = plt.axis()\n",
    "# plt.axis([xmin, xmax, ymin - 0.1, ymax + 0.1])  # Adjust the offset as needed\n",
    "plt.yticks(list(n_ind.values()), [f'{index[0]} {index[1]}' for index in n_ind.keys()])\n",
    "plt.yticks(fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# # format plot_table to 3 decimal places\n",
    "ptf = plot_table.round(3)\n",
    "# reverse order of rows\n",
    "ptf = ptf.iloc[::-1]\n",
    "ptf['HR (95% CI)'] = ptf.apply(lambda x: f\"{x['exp(coef)']} ({x['exp(coef) lower 95%']}, {x['exp(coef) upper 95%']})\", axis=1)\n",
    "# ptf[['HR (95% CI)', 'p', 'NIVO n', 'EVERO n', 'class']]\n",
    "ptf_d = ptf[['HR (95% CI)', 'p', 'NIVO n', 'EVERO n', 'class']].pivot(columns='class').to_dict(orient=\"index\")\n",
    "# for k,v in ptf.items():\n",
    "#     # add text of v to plot\n",
    "#     plt.text(10, n_ind[k], f\"{v[('HR (95% CI)', 'All')]}\")\n",
    "\n",
    "# ptf[['HR (95% CI)', 'p', 'NIVO n', 'EVERO n', 'class']].pivot(columns='class').iloc[::-1][[('HR (95% CI)', 'All'), ('p', 'All'), ('NIVO n', 'All'), ('EVERO n', 'All'), ('HR (95% CI)', 'IM+'), ('p', 'IM+'), ('NIVO n', 'IM+'), ('EVERO n', 'IM+'), ('HR (95% CI)', 'IM-'), ('p', 'IM-'), ('NIVO n', 'IM-'), ('EVERO n', 'IM-')]].to_clipboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>HR (95% CI)</th>\n",
       "      <th>p</th>\n",
       "      <th>NIVO n</th>\n",
       "      <th>EVERO n</th>\n",
       "      <th>HR (95% CI)</th>\n",
       "      <th>p</th>\n",
       "      <th>NIVO n</th>\n",
       "      <th>EVERO n</th>\n",
       "      <th>HR (95% CI)</th>\n",
       "      <th>p</th>\n",
       "      <th>NIVO n</th>\n",
       "      <th>EVERO n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>All</th>\n",
       "      <th>All</th>\n",
       "      <th>All</th>\n",
       "      <th>All</th>\n",
       "      <th>IM+</th>\n",
       "      <th>IM+</th>\n",
       "      <th>IM+</th>\n",
       "      <th>IM+</th>\n",
       "      <th>IM-</th>\n",
       "      <th>IM-</th>\n",
       "      <th>IM-</th>\n",
       "      <th>IM-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <th>NIVOLUMAB</th>\n",
       "      <td>0.718 (0.494, 1.044)</td>\n",
       "      <td>0.083</td>\n",
       "      <td>79.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.326 (0.164, 0.647)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.075 (0.678, 1.704)</td>\n",
       "      <td>0.759</td>\n",
       "      <td>46.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Sex</th>\n",
       "      <th>M</th>\n",
       "      <td>0.797 (0.514, 1.238)</td>\n",
       "      <td>0.313</td>\n",
       "      <td>60.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.379 (0.181, 0.795)</td>\n",
       "      <td>0.010</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.132 (0.629, 2.038)</td>\n",
       "      <td>0.680</td>\n",
       "      <td>36.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.631 (0.302, 1.318)</td>\n",
       "      <td>0.220</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.248 (0.041, 1.513)</td>\n",
       "      <td>0.131</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.726 (0.749, 3.98)</td>\n",
       "      <td>0.200</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Region</th>\n",
       "      <th>WESTERN EUROPE</th>\n",
       "      <td>1.008 (0.542, 1.877)</td>\n",
       "      <td>0.980</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.464 (0.146, 1.471)</td>\n",
       "      <td>0.192</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.499 (0.704, 3.194)</td>\n",
       "      <td>0.294</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US/CANADA</th>\n",
       "      <td>0.608 (0.337, 1.096)</td>\n",
       "      <td>0.098</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.268 (0.102, 0.7)</td>\n",
       "      <td>0.007</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.013 (0.438, 2.342)</td>\n",
       "      <td>0.976</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REST OF WORLD</th>\n",
       "      <td>0.595 (0.27, 1.312)</td>\n",
       "      <td>0.198</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.865 (0.078, 9.577)</td>\n",
       "      <td>0.906</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.698 (0.299, 1.63)</td>\n",
       "      <td>0.406</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Prior_2</th>\n",
       "      <th>True</th>\n",
       "      <td>0.583 (0.268, 1.267)</td>\n",
       "      <td>0.173</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.358 (0.091, 1.409)</td>\n",
       "      <td>0.142</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.315 (0.104, 0.959)</td>\n",
       "      <td>0.042</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>0.767 (0.5, 1.176)</td>\n",
       "      <td>0.224</td>\n",
       "      <td>61.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.319 (0.144, 0.707)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.28 (0.759, 2.158)</td>\n",
       "      <td>0.355</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">IMDC</th>\n",
       "      <th>POOR</th>\n",
       "      <td>0.694 (0.327, 1.472)</td>\n",
       "      <td>0.341</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.316 (0.026, 3.787)</td>\n",
       "      <td>0.363</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.675 (0.303, 1.505)</td>\n",
       "      <td>0.337</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT REPORTED</th>\n",
       "      <td>3.348 (0.336, 33.36)</td>\n",
       "      <td>0.303</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.348 (0.336, 33.36)</td>\n",
       "      <td>0.303</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTERMEDIATE</th>\n",
       "      <td>0.622 (0.38, 1.019)</td>\n",
       "      <td>0.059</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.286 (0.126, 0.648)</td>\n",
       "      <td>0.003</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.117 (0.58, 2.151)</td>\n",
       "      <td>0.742</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAVORABLE</th>\n",
       "      <td>0.69 (0.242, 1.97)</td>\n",
       "      <td>0.488</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.523 (0.116, 2.363)</td>\n",
       "      <td>0.400</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.701 (0.155, 3.176)</td>\n",
       "      <td>0.645</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Age_Group</th>\n",
       "      <th>&lt;65</th>\n",
       "      <td>0.856 (0.528, 1.388)</td>\n",
       "      <td>0.529</td>\n",
       "      <td>48.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.406 (0.173, 0.953)</td>\n",
       "      <td>0.038</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.143 (0.606, 2.154)</td>\n",
       "      <td>0.679</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65-75</th>\n",
       "      <td>0.55 (0.28, 1.081)</td>\n",
       "      <td>0.083</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.278 (0.068, 1.137)</td>\n",
       "      <td>0.075</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.912 (0.421, 1.974)</td>\n",
       "      <td>0.815</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;75</th>\n",
       "      <td>0.493 (0.119, 2.045)</td>\n",
       "      <td>0.330</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0 (0.0, inf)</td>\n",
       "      <td>0.995</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.408 (0.544, 53.758)</td>\n",
       "      <td>0.150</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   HR (95% CI)      p NIVO n EVERO n  \\\n",
       "class                                      All    All    All     All   \n",
       "Total     NIVOLUMAB       0.718 (0.494, 1.044)  0.083   79.0    70.0   \n",
       "Sex       M               0.797 (0.514, 1.238)  0.313   60.0    48.0   \n",
       "          F               0.631 (0.302, 1.318)  0.220   19.0    22.0   \n",
       "Region    WESTERN EUROPE  1.008 (0.542, 1.877)  0.980   27.0    24.0   \n",
       "          US/CANADA       0.608 (0.337, 1.096)  0.098   32.0    26.0   \n",
       "          REST OF WORLD    0.595 (0.27, 1.312)  0.198   20.0    20.0   \n",
       "Prior_2   True            0.583 (0.268, 1.267)  0.173   18.0    16.0   \n",
       "          False             0.767 (0.5, 1.176)  0.224   61.0    54.0   \n",
       "IMDC      POOR            0.694 (0.327, 1.472)  0.341   17.0    17.0   \n",
       "          NOT REPORTED    3.348 (0.336, 33.36)  0.303    3.0     3.0   \n",
       "          INTERMEDIATE     0.622 (0.38, 1.019)  0.059   46.0    40.0   \n",
       "          FAVORABLE         0.69 (0.242, 1.97)  0.488   13.0    10.0   \n",
       "Age_Group <65             0.856 (0.528, 1.388)  0.529   48.0    43.0   \n",
       "          65-75             0.55 (0.28, 1.081)  0.083   25.0    21.0   \n",
       "           >75            0.493 (0.119, 2.045)  0.330    6.0     6.0   \n",
       "\n",
       "                                   HR (95% CI)      p NIVO n EVERO n  \\\n",
       "class                                      IM+    IM+    IM+     IM+   \n",
       "Total     NIVOLUMAB       0.326 (0.164, 0.647)  0.001   33.0    29.0   \n",
       "Sex       M               0.379 (0.181, 0.795)  0.010   24.0    24.0   \n",
       "          F               0.248 (0.041, 1.513)  0.131    9.0     5.0   \n",
       "Region    WESTERN EUROPE  0.464 (0.146, 1.471)  0.192   10.0     8.0   \n",
       "          US/CANADA         0.268 (0.102, 0.7)  0.007   15.0    17.0   \n",
       "          REST OF WORLD   0.865 (0.078, 9.577)  0.906    8.0     4.0   \n",
       "Prior_2   True            0.358 (0.091, 1.409)  0.142    7.0     9.0   \n",
       "          False           0.319 (0.144, 0.707)  0.005   26.0    20.0   \n",
       "IMDC      POOR            0.316 (0.026, 3.787)  0.363    3.0     4.0   \n",
       "          NOT REPORTED                     NaN    NaN    NaN     NaN   \n",
       "          INTERMEDIATE    0.286 (0.126, 0.648)  0.003   22.0    19.0   \n",
       "          FAVORABLE       0.523 (0.116, 2.363)  0.400    8.0     6.0   \n",
       "Age_Group <65             0.406 (0.173, 0.953)  0.038   19.0    22.0   \n",
       "          65-75           0.278 (0.068, 1.137)  0.075   11.0     4.0   \n",
       "           >75                  0.0 (0.0, inf)  0.995    3.0     3.0   \n",
       "\n",
       "                                    HR (95% CI)      p NIVO n EVERO n  \n",
       "class                                       IM-    IM-    IM-     IM-  \n",
       "Total     NIVOLUMAB        1.075 (0.678, 1.704)  0.759   46.0    41.0  \n",
       "Sex       M                1.132 (0.629, 2.038)  0.680   36.0    24.0  \n",
       "          F                 1.726 (0.749, 3.98)  0.200   10.0    17.0  \n",
       "Region    WESTERN EUROPE   1.499 (0.704, 3.194)  0.294   17.0    16.0  \n",
       "          US/CANADA        1.013 (0.438, 2.342)  0.976   17.0     9.0  \n",
       "          REST OF WORLD     0.698 (0.299, 1.63)  0.406   12.0    16.0  \n",
       "Prior_2   True             0.315 (0.104, 0.959)  0.042   11.0     7.0  \n",
       "          False             1.28 (0.759, 2.158)  0.355   35.0    34.0  \n",
       "IMDC      POOR             0.675 (0.303, 1.505)  0.337   14.0    13.0  \n",
       "          NOT REPORTED     3.348 (0.336, 33.36)  0.303    3.0     3.0  \n",
       "          INTERMEDIATE      1.117 (0.58, 2.151)  0.742   24.0    21.0  \n",
       "          FAVORABLE        0.701 (0.155, 3.176)  0.645    5.0     4.0  \n",
       "Age_Group <65              1.143 (0.606, 2.154)  0.679   29.0    21.0  \n",
       "          65-75            0.912 (0.421, 1.974)  0.815   14.0    17.0  \n",
       "           >75            5.408 (0.544, 53.758)  0.150    3.0     3.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptf[['HR (95% CI)', 'p', 'NIVO n', 'EVERO n', 'class']].pivot(columns='class').iloc[::-1][[('HR (95% CI)', 'All'), ('p', 'All'), ('NIVO n', 'All'), ('EVERO n', 'All'), ('HR (95% CI)', 'IM+'), ('p', 'IM+'), ('NIVO n', 'IM+'), ('EVERO n', 'IM+'), ('HR (95% CI)', 'IM-'), ('p', 'IM-'), ('NIVO n', 'IM-'), ('EVERO n', 'IM-')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp(coef)</th>\n",
       "      <th>se(coef)</th>\n",
       "      <th>coef lower 95%</th>\n",
       "      <th>coef upper 95%</th>\n",
       "      <th>exp(coef) lower 95%</th>\n",
       "      <th>exp(coef) upper 95%</th>\n",
       "      <th>cmp to</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "      <th>NIVO n</th>\n",
       "      <th>EVERO n</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Sex</th>\n",
       "      <th>F</th>\n",
       "      <td>-0.461199</td>\n",
       "      <td>6.305275e-01</td>\n",
       "      <td>0.376230</td>\n",
       "      <td>-1.198597</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.301617</td>\n",
       "      <td>1.318111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.225841</td>\n",
       "      <td>0.220259</td>\n",
       "      <td>2.182729</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>-0.226325</td>\n",
       "      <td>7.974590e-01</td>\n",
       "      <td>0.224287</td>\n",
       "      <td>-0.665920</td>\n",
       "      <td>0.213270</td>\n",
       "      <td>0.513801</td>\n",
       "      <td>1.237719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.009085</td>\n",
       "      <td>0.312934</td>\n",
       "      <td>1.676069</td>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Age_Group</th>\n",
       "      <th>&gt;75</th>\n",
       "      <td>-0.708180</td>\n",
       "      <td>4.925397e-01</td>\n",
       "      <td>0.726336</td>\n",
       "      <td>-2.131773</td>\n",
       "      <td>0.715412</td>\n",
       "      <td>0.118627</td>\n",
       "      <td>2.045029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.975004</td>\n",
       "      <td>0.329558</td>\n",
       "      <td>1.601394</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65-75</th>\n",
       "      <td>-0.598519</td>\n",
       "      <td>5.496248e-01</td>\n",
       "      <td>0.344936</td>\n",
       "      <td>-1.274581</td>\n",
       "      <td>0.077542</td>\n",
       "      <td>0.279548</td>\n",
       "      <td>1.080628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.735162</td>\n",
       "      <td>0.082712</td>\n",
       "      <td>3.595757</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;65</th>\n",
       "      <td>-0.155073</td>\n",
       "      <td>8.563525e-01</td>\n",
       "      <td>0.246524</td>\n",
       "      <td>-0.638251</td>\n",
       "      <td>0.328104</td>\n",
       "      <td>0.528216</td>\n",
       "      <td>1.388334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.629040</td>\n",
       "      <td>0.529323</td>\n",
       "      <td>0.917780</td>\n",
       "      <td>48</td>\n",
       "      <td>43</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Region</th>\n",
       "      <th>REST OF WORLD</th>\n",
       "      <td>-0.519062</td>\n",
       "      <td>5.950786e-01</td>\n",
       "      <td>0.403310</td>\n",
       "      <td>-1.309535</td>\n",
       "      <td>0.271412</td>\n",
       "      <td>0.269945</td>\n",
       "      <td>1.311815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.287004</td>\n",
       "      <td>0.198093</td>\n",
       "      <td>2.335751</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US/CANADA</th>\n",
       "      <td>-0.498364</td>\n",
       "      <td>6.075236e-01</td>\n",
       "      <td>0.300969</td>\n",
       "      <td>-1.088253</td>\n",
       "      <td>0.091525</td>\n",
       "      <td>0.336804</td>\n",
       "      <td>1.095844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.655865</td>\n",
       "      <td>0.097749</td>\n",
       "      <td>3.354771</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WESTERN EUROPE</th>\n",
       "      <td>0.008082</td>\n",
       "      <td>1.008115e+00</td>\n",
       "      <td>0.317067</td>\n",
       "      <td>-0.613357</td>\n",
       "      <td>0.629521</td>\n",
       "      <td>0.541530</td>\n",
       "      <td>1.876712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025491</td>\n",
       "      <td>0.979663</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">IMDC</th>\n",
       "      <th>FAVORABLE</th>\n",
       "      <td>-0.371015</td>\n",
       "      <td>6.900338e-01</td>\n",
       "      <td>0.535275</td>\n",
       "      <td>-1.420135</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.241681</td>\n",
       "      <td>1.970142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.693129</td>\n",
       "      <td>0.488229</td>\n",
       "      <td>1.034371</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTERMEDIATE</th>\n",
       "      <td>-0.474269</td>\n",
       "      <td>6.223401e-01</td>\n",
       "      <td>0.251339</td>\n",
       "      <td>-0.966884</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>0.380266</td>\n",
       "      <td>1.018517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.886966</td>\n",
       "      <td>0.059165</td>\n",
       "      <td>4.079115</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT REPORTED</th>\n",
       "      <td>1.208341</td>\n",
       "      <td>3.347926e+00</td>\n",
       "      <td>1.172988</td>\n",
       "      <td>-1.090672</td>\n",
       "      <td>3.507354</td>\n",
       "      <td>0.335990</td>\n",
       "      <td>33.359891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.030140</td>\n",
       "      <td>0.302944</td>\n",
       "      <td>1.722875</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POOR</th>\n",
       "      <td>-0.365353</td>\n",
       "      <td>6.939514e-01</td>\n",
       "      <td>0.383778</td>\n",
       "      <td>-1.117544</td>\n",
       "      <td>0.386838</td>\n",
       "      <td>0.327082</td>\n",
       "      <td>1.472317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.951992</td>\n",
       "      <td>0.341101</td>\n",
       "      <td>1.551728</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Prior_2</th>\n",
       "      <th>False</th>\n",
       "      <td>-0.265803</td>\n",
       "      <td>7.665904e-01</td>\n",
       "      <td>0.218539</td>\n",
       "      <td>-0.694132</td>\n",
       "      <td>0.162527</td>\n",
       "      <td>0.499508</td>\n",
       "      <td>1.176480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.216269</td>\n",
       "      <td>0.223883</td>\n",
       "      <td>2.159186</td>\n",
       "      <td>61</td>\n",
       "      <td>54</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>-0.539470</td>\n",
       "      <td>5.830574e-01</td>\n",
       "      <td>0.396189</td>\n",
       "      <td>-1.315986</td>\n",
       "      <td>0.237046</td>\n",
       "      <td>0.268210</td>\n",
       "      <td>1.267500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.361647</td>\n",
       "      <td>0.173309</td>\n",
       "      <td>2.528580</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <th>NIVOLUMAB</th>\n",
       "      <td>-0.331049</td>\n",
       "      <td>7.181697e-01</td>\n",
       "      <td>0.191060</td>\n",
       "      <td>-0.705521</td>\n",
       "      <td>0.043422</td>\n",
       "      <td>0.493851</td>\n",
       "      <td>1.044379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.732695</td>\n",
       "      <td>0.083150</td>\n",
       "      <td>3.588141</td>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>All</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Sex</th>\n",
       "      <th>F</th>\n",
       "      <td>-1.395102</td>\n",
       "      <td>2.478077e-01</td>\n",
       "      <td>0.923244</td>\n",
       "      <td>-3.204627</td>\n",
       "      <td>0.414423</td>\n",
       "      <td>0.040574</td>\n",
       "      <td>1.513496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.511087</td>\n",
       "      <td>0.130766</td>\n",
       "      <td>2.934939</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>-0.969967</td>\n",
       "      <td>3.790956e-01</td>\n",
       "      <td>0.377645</td>\n",
       "      <td>-1.710138</td>\n",
       "      <td>-0.229796</td>\n",
       "      <td>0.180841</td>\n",
       "      <td>0.794696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.568461</td>\n",
       "      <td>0.010215</td>\n",
       "      <td>6.613150</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Age_Group</th>\n",
       "      <th>&gt;75</th>\n",
       "      <td>-17.779836</td>\n",
       "      <td>1.898083e-08</td>\n",
       "      <td>3095.003158</td>\n",
       "      <td>-6083.874558</td>\n",
       "      <td>6048.314885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005745</td>\n",
       "      <td>0.995416</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65-75</th>\n",
       "      <td>-1.281783</td>\n",
       "      <td>2.775420e-01</td>\n",
       "      <td>0.719275</td>\n",
       "      <td>-2.691537</td>\n",
       "      <td>0.127971</td>\n",
       "      <td>0.067777</td>\n",
       "      <td>1.136520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.782048</td>\n",
       "      <td>0.074741</td>\n",
       "      <td>3.741948</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;65</th>\n",
       "      <td>-0.902147</td>\n",
       "      <td>4.056977e-01</td>\n",
       "      <td>0.435811</td>\n",
       "      <td>-1.756321</td>\n",
       "      <td>-0.047973</td>\n",
       "      <td>0.172679</td>\n",
       "      <td>0.953159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.070042</td>\n",
       "      <td>0.038448</td>\n",
       "      <td>4.700933</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Region</th>\n",
       "      <th>REST OF WORLD</th>\n",
       "      <td>-0.144812</td>\n",
       "      <td>8.651848e-01</td>\n",
       "      <td>1.226664</td>\n",
       "      <td>-2.549029</td>\n",
       "      <td>2.259405</td>\n",
       "      <td>0.078158</td>\n",
       "      <td>9.577387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.118054</td>\n",
       "      <td>0.906025</td>\n",
       "      <td>0.142377</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US/CANADA</th>\n",
       "      <td>-1.317818</td>\n",
       "      <td>2.677189e-01</td>\n",
       "      <td>0.490217</td>\n",
       "      <td>-2.278624</td>\n",
       "      <td>-0.357011</td>\n",
       "      <td>0.102425</td>\n",
       "      <td>0.699765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.688236</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>7.121185</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WESTERN EUROPE</th>\n",
       "      <td>-0.768194</td>\n",
       "      <td>4.638498e-01</td>\n",
       "      <td>0.588720</td>\n",
       "      <td>-1.922065</td>\n",
       "      <td>0.385676</td>\n",
       "      <td>0.146305</td>\n",
       "      <td>1.470608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.304855</td>\n",
       "      <td>0.191942</td>\n",
       "      <td>2.381255</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IMDC</th>\n",
       "      <th>FAVORABLE</th>\n",
       "      <td>-0.647965</td>\n",
       "      <td>5.231091e-01</td>\n",
       "      <td>0.769381</td>\n",
       "      <td>-2.155925</td>\n",
       "      <td>0.859994</td>\n",
       "      <td>0.115796</td>\n",
       "      <td>2.363147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.842190</td>\n",
       "      <td>0.399682</td>\n",
       "      <td>1.323077</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTERMEDIATE</th>\n",
       "      <td>-1.252965</td>\n",
       "      <td>2.856564e-01</td>\n",
       "      <td>0.417763</td>\n",
       "      <td>-2.071766</td>\n",
       "      <td>-0.434165</td>\n",
       "      <td>0.125963</td>\n",
       "      <td>0.647805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.999225</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>8.529262</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POOR</th>\n",
       "      <td>-1.151722</td>\n",
       "      <td>3.160920e-01</td>\n",
       "      <td>1.267018</td>\n",
       "      <td>-3.635031</td>\n",
       "      <td>1.331587</td>\n",
       "      <td>0.026383</td>\n",
       "      <td>3.787050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.909002</td>\n",
       "      <td>0.363349</td>\n",
       "      <td>1.460572</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Prior_2</th>\n",
       "      <th>False</th>\n",
       "      <td>-1.143714</td>\n",
       "      <td>3.186333e-01</td>\n",
       "      <td>0.406537</td>\n",
       "      <td>-1.940513</td>\n",
       "      <td>-0.346915</td>\n",
       "      <td>0.143630</td>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.813306</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>7.671973</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>-1.025977</td>\n",
       "      <td>3.584461e-01</td>\n",
       "      <td>0.698391</td>\n",
       "      <td>-2.394797</td>\n",
       "      <td>0.342843</td>\n",
       "      <td>0.091191</td>\n",
       "      <td>1.408948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.469059</td>\n",
       "      <td>0.141817</td>\n",
       "      <td>2.817900</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <th>NIVOLUMAB</th>\n",
       "      <td>-1.121415</td>\n",
       "      <td>3.258185e-01</td>\n",
       "      <td>0.350258</td>\n",
       "      <td>-1.807907</td>\n",
       "      <td>-0.434923</td>\n",
       "      <td>0.163997</td>\n",
       "      <td>0.647315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.201686</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>9.515556</td>\n",
       "      <td>33</td>\n",
       "      <td>29</td>\n",
       "      <td>IM+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Sex</th>\n",
       "      <th>F</th>\n",
       "      <td>0.545994</td>\n",
       "      <td>1.726323e+00</td>\n",
       "      <td>0.426125</td>\n",
       "      <td>-0.289196</td>\n",
       "      <td>1.381183</td>\n",
       "      <td>0.748865</td>\n",
       "      <td>3.979608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.281299</td>\n",
       "      <td>0.200089</td>\n",
       "      <td>2.321289</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.123907</td>\n",
       "      <td>1.131910e+00</td>\n",
       "      <td>0.300044</td>\n",
       "      <td>-0.464169</td>\n",
       "      <td>0.711983</td>\n",
       "      <td>0.628657</td>\n",
       "      <td>2.038029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412961</td>\n",
       "      <td>0.679635</td>\n",
       "      <td>0.557168</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Age_Group</th>\n",
       "      <th>&gt;75</th>\n",
       "      <td>1.687838</td>\n",
       "      <td>5.407777e+00</td>\n",
       "      <td>1.171784</td>\n",
       "      <td>-0.608817</td>\n",
       "      <td>3.984493</td>\n",
       "      <td>0.543994</td>\n",
       "      <td>53.758028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.440400</td>\n",
       "      <td>0.149754</td>\n",
       "      <td>2.739331</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65-75</th>\n",
       "      <td>-0.092421</td>\n",
       "      <td>9.117208e-01</td>\n",
       "      <td>0.394002</td>\n",
       "      <td>-0.864652</td>\n",
       "      <td>0.679809</td>\n",
       "      <td>0.421198</td>\n",
       "      <td>1.973501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>0.814542</td>\n",
       "      <td>0.295939</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;65</th>\n",
       "      <td>0.133750</td>\n",
       "      <td>1.143107e+00</td>\n",
       "      <td>0.323377</td>\n",
       "      <td>-0.500057</td>\n",
       "      <td>0.767556</td>\n",
       "      <td>0.606496</td>\n",
       "      <td>2.154495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413604</td>\n",
       "      <td>0.679164</td>\n",
       "      <td>0.558168</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Region</th>\n",
       "      <th>REST OF WORLD</th>\n",
       "      <td>-0.359432</td>\n",
       "      <td>6.980728e-01</td>\n",
       "      <td>0.432686</td>\n",
       "      <td>-1.207481</td>\n",
       "      <td>0.488617</td>\n",
       "      <td>0.298949</td>\n",
       "      <td>1.630060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.830699</td>\n",
       "      <td>0.406144</td>\n",
       "      <td>1.299938</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US/CANADA</th>\n",
       "      <td>0.012986</td>\n",
       "      <td>1.013070e+00</td>\n",
       "      <td>0.427608</td>\n",
       "      <td>-0.825111</td>\n",
       "      <td>0.851082</td>\n",
       "      <td>0.438186</td>\n",
       "      <td>2.342179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030368</td>\n",
       "      <td>0.975774</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WESTERN EUROPE</th>\n",
       "      <td>0.405010</td>\n",
       "      <td>1.499318e+00</td>\n",
       "      <td>0.385892</td>\n",
       "      <td>-0.351323</td>\n",
       "      <td>1.161344</td>\n",
       "      <td>0.703756</td>\n",
       "      <td>3.194224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.049544</td>\n",
       "      <td>0.293928</td>\n",
       "      <td>1.766467</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">IMDC</th>\n",
       "      <th>FAVORABLE</th>\n",
       "      <td>-0.355220</td>\n",
       "      <td>7.010194e-01</td>\n",
       "      <td>0.770837</td>\n",
       "      <td>-1.866033</td>\n",
       "      <td>1.155593</td>\n",
       "      <td>0.154736</td>\n",
       "      <td>3.175908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.460823</td>\n",
       "      <td>0.644925</td>\n",
       "      <td>0.632796</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTERMEDIATE</th>\n",
       "      <td>0.110224</td>\n",
       "      <td>1.116528e+00</td>\n",
       "      <td>0.334533</td>\n",
       "      <td>-0.545448</td>\n",
       "      <td>0.765895</td>\n",
       "      <td>0.579582</td>\n",
       "      <td>2.150919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329485</td>\n",
       "      <td>0.741789</td>\n",
       "      <td>0.430919</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT REPORTED</th>\n",
       "      <td>1.208341</td>\n",
       "      <td>3.347926e+00</td>\n",
       "      <td>1.172988</td>\n",
       "      <td>-1.090672</td>\n",
       "      <td>3.507354</td>\n",
       "      <td>0.335990</td>\n",
       "      <td>33.359891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.030140</td>\n",
       "      <td>0.302944</td>\n",
       "      <td>1.722875</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POOR</th>\n",
       "      <td>-0.392813</td>\n",
       "      <td>6.751550e-01</td>\n",
       "      <td>0.408905</td>\n",
       "      <td>-1.194252</td>\n",
       "      <td>0.408626</td>\n",
       "      <td>0.302930</td>\n",
       "      <td>1.504749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.960646</td>\n",
       "      <td>0.336730</td>\n",
       "      <td>1.570336</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Prior_2</th>\n",
       "      <th>False</th>\n",
       "      <td>0.246701</td>\n",
       "      <td>1.279796e+00</td>\n",
       "      <td>0.266537</td>\n",
       "      <td>-0.275702</td>\n",
       "      <td>0.769104</td>\n",
       "      <td>0.759039</td>\n",
       "      <td>2.157831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925578</td>\n",
       "      <td>0.354665</td>\n",
       "      <td>1.495469</td>\n",
       "      <td>35</td>\n",
       "      <td>34</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>-1.154052</td>\n",
       "      <td>3.153565e-01</td>\n",
       "      <td>0.567461</td>\n",
       "      <td>-2.266254</td>\n",
       "      <td>-0.041849</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.959014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.033712</td>\n",
       "      <td>0.041981</td>\n",
       "      <td>4.574133</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <th>NIVOLUMAB</th>\n",
       "      <td>0.072235</td>\n",
       "      <td>1.074908e+00</td>\n",
       "      <td>0.235185</td>\n",
       "      <td>-0.388719</td>\n",
       "      <td>0.533188</td>\n",
       "      <td>0.677925</td>\n",
       "      <td>1.704357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307140</td>\n",
       "      <td>0.758737</td>\n",
       "      <td>0.398329</td>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>IM-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               coef     exp(coef)     se(coef)  \\\n",
       "Sex       F               -0.461199  6.305275e-01     0.376230   \n",
       "          M               -0.226325  7.974590e-01     0.224287   \n",
       "Age_Group  >75            -0.708180  4.925397e-01     0.726336   \n",
       "          65-75           -0.598519  5.496248e-01     0.344936   \n",
       "          <65             -0.155073  8.563525e-01     0.246524   \n",
       "Region    REST OF WORLD   -0.519062  5.950786e-01     0.403310   \n",
       "          US/CANADA       -0.498364  6.075236e-01     0.300969   \n",
       "          WESTERN EUROPE   0.008082  1.008115e+00     0.317067   \n",
       "IMDC      FAVORABLE       -0.371015  6.900338e-01     0.535275   \n",
       "          INTERMEDIATE    -0.474269  6.223401e-01     0.251339   \n",
       "          NOT REPORTED     1.208341  3.347926e+00     1.172988   \n",
       "          POOR            -0.365353  6.939514e-01     0.383778   \n",
       "Prior_2   False           -0.265803  7.665904e-01     0.218539   \n",
       "          True            -0.539470  5.830574e-01     0.396189   \n",
       "Total     NIVOLUMAB       -0.331049  7.181697e-01     0.191060   \n",
       "Sex       F               -1.395102  2.478077e-01     0.923244   \n",
       "          M               -0.969967  3.790956e-01     0.377645   \n",
       "Age_Group  >75           -17.779836  1.898083e-08  3095.003158   \n",
       "          65-75           -1.281783  2.775420e-01     0.719275   \n",
       "          <65             -0.902147  4.056977e-01     0.435811   \n",
       "Region    REST OF WORLD   -0.144812  8.651848e-01     1.226664   \n",
       "          US/CANADA       -1.317818  2.677189e-01     0.490217   \n",
       "          WESTERN EUROPE  -0.768194  4.638498e-01     0.588720   \n",
       "IMDC      FAVORABLE       -0.647965  5.231091e-01     0.769381   \n",
       "          INTERMEDIATE    -1.252965  2.856564e-01     0.417763   \n",
       "          POOR            -1.151722  3.160920e-01     1.267018   \n",
       "Prior_2   False           -1.143714  3.186333e-01     0.406537   \n",
       "          True            -1.025977  3.584461e-01     0.698391   \n",
       "Total     NIVOLUMAB       -1.121415  3.258185e-01     0.350258   \n",
       "Sex       F                0.545994  1.726323e+00     0.426125   \n",
       "          M                0.123907  1.131910e+00     0.300044   \n",
       "Age_Group  >75             1.687838  5.407777e+00     1.171784   \n",
       "          65-75           -0.092421  9.117208e-01     0.394002   \n",
       "          <65              0.133750  1.143107e+00     0.323377   \n",
       "Region    REST OF WORLD   -0.359432  6.980728e-01     0.432686   \n",
       "          US/CANADA        0.012986  1.013070e+00     0.427608   \n",
       "          WESTERN EUROPE   0.405010  1.499318e+00     0.385892   \n",
       "IMDC      FAVORABLE       -0.355220  7.010194e-01     0.770837   \n",
       "          INTERMEDIATE     0.110224  1.116528e+00     0.334533   \n",
       "          NOT REPORTED     1.208341  3.347926e+00     1.172988   \n",
       "          POOR            -0.392813  6.751550e-01     0.408905   \n",
       "Prior_2   False            0.246701  1.279796e+00     0.266537   \n",
       "          True            -1.154052  3.153565e-01     0.567461   \n",
       "Total     NIVOLUMAB        0.072235  1.074908e+00     0.235185   \n",
       "\n",
       "                          coef lower 95%  coef upper 95%  exp(coef) lower 95%  \\\n",
       "Sex       F                    -1.198597        0.276200             0.301617   \n",
       "          M                    -0.665920        0.213270             0.513801   \n",
       "Age_Group  >75                 -2.131773        0.715412             0.118627   \n",
       "          65-75                -1.274581        0.077542             0.279548   \n",
       "          <65                  -0.638251        0.328104             0.528216   \n",
       "Region    REST OF WORLD        -1.309535        0.271412             0.269945   \n",
       "          US/CANADA            -1.088253        0.091525             0.336804   \n",
       "          WESTERN EUROPE       -0.613357        0.629521             0.541530   \n",
       "IMDC      FAVORABLE            -1.420135        0.678105             0.241681   \n",
       "          INTERMEDIATE         -0.966884        0.018347             0.380266   \n",
       "          NOT REPORTED         -1.090672        3.507354             0.335990   \n",
       "          POOR                 -1.117544        0.386838             0.327082   \n",
       "Prior_2   False                -0.694132        0.162527             0.499508   \n",
       "          True                 -1.315986        0.237046             0.268210   \n",
       "Total     NIVOLUMAB            -0.705521        0.043422             0.493851   \n",
       "Sex       F                    -3.204627        0.414423             0.040574   \n",
       "          M                    -1.710138       -0.229796             0.180841   \n",
       "Age_Group  >75              -6083.874558     6048.314885             0.000000   \n",
       "          65-75                -2.691537        0.127971             0.067777   \n",
       "          <65                  -1.756321       -0.047973             0.172679   \n",
       "Region    REST OF WORLD        -2.549029        2.259405             0.078158   \n",
       "          US/CANADA            -2.278624       -0.357011             0.102425   \n",
       "          WESTERN EUROPE       -1.922065        0.385676             0.146305   \n",
       "IMDC      FAVORABLE            -2.155925        0.859994             0.115796   \n",
       "          INTERMEDIATE         -2.071766       -0.434165             0.125963   \n",
       "          POOR                 -3.635031        1.331587             0.026383   \n",
       "Prior_2   False                -1.940513       -0.346915             0.143630   \n",
       "          True                 -2.394797        0.342843             0.091191   \n",
       "Total     NIVOLUMAB            -1.807907       -0.434923             0.163997   \n",
       "Sex       F                    -0.289196        1.381183             0.748865   \n",
       "          M                    -0.464169        0.711983             0.628657   \n",
       "Age_Group  >75                 -0.608817        3.984493             0.543994   \n",
       "          65-75                -0.864652        0.679809             0.421198   \n",
       "          <65                  -0.500057        0.767556             0.606496   \n",
       "Region    REST OF WORLD        -1.207481        0.488617             0.298949   \n",
       "          US/CANADA            -0.825111        0.851082             0.438186   \n",
       "          WESTERN EUROPE       -0.351323        1.161344             0.703756   \n",
       "IMDC      FAVORABLE            -1.866033        1.155593             0.154736   \n",
       "          INTERMEDIATE         -0.545448        0.765895             0.579582   \n",
       "          NOT REPORTED         -1.090672        3.507354             0.335990   \n",
       "          POOR                 -1.194252        0.408626             0.302930   \n",
       "Prior_2   False                -0.275702        0.769104             0.759039   \n",
       "          True                 -2.266254       -0.041849             0.103700   \n",
       "Total     NIVOLUMAB            -0.388719        0.533188             0.677925   \n",
       "\n",
       "                          exp(coef) upper 95%  cmp to         z         p  \\\n",
       "Sex       F                          1.318111     0.0 -1.225841  0.220259   \n",
       "          M                          1.237719     0.0 -1.009085  0.312934   \n",
       "Age_Group  >75                       2.045029     0.0 -0.975004  0.329558   \n",
       "          65-75                      1.080628     0.0 -1.735162  0.082712   \n",
       "          <65                        1.388334     0.0 -0.629040  0.529323   \n",
       "Region    REST OF WORLD              1.311815     0.0 -1.287004  0.198093   \n",
       "          US/CANADA                  1.095844     0.0 -1.655865  0.097749   \n",
       "          WESTERN EUROPE             1.876712     0.0  0.025491  0.979663   \n",
       "IMDC      FAVORABLE                  1.970142     0.0 -0.693129  0.488229   \n",
       "          INTERMEDIATE               1.018517     0.0 -1.886966  0.059165   \n",
       "          NOT REPORTED              33.359891     0.0  1.030140  0.302944   \n",
       "          POOR                       1.472317     0.0 -0.951992  0.341101   \n",
       "Prior_2   False                      1.176480     0.0 -1.216269  0.223883   \n",
       "          True                       1.267500     0.0 -1.361647  0.173309   \n",
       "Total     NIVOLUMAB                  1.044379     0.0 -1.732695  0.083150   \n",
       "Sex       F                          1.513496     0.0 -1.511087  0.130766   \n",
       "          M                          0.794696     0.0 -2.568461  0.010215   \n",
       "Age_Group  >75                            inf     0.0 -0.005745  0.995416   \n",
       "          65-75                      1.136520     0.0 -1.782048  0.074741   \n",
       "          <65                        0.953159     0.0 -2.070042  0.038448   \n",
       "Region    REST OF WORLD              9.577387     0.0 -0.118054  0.906025   \n",
       "          US/CANADA                  0.699765     0.0 -2.688236  0.007183   \n",
       "          WESTERN EUROPE             1.470608     0.0 -1.304855  0.191942   \n",
       "IMDC      FAVORABLE                  2.363147     0.0 -0.842190  0.399682   \n",
       "          INTERMEDIATE               0.647805     0.0 -2.999225  0.002707   \n",
       "          POOR                       3.787050     0.0 -0.909002  0.363349   \n",
       "Prior_2   False                      0.706865     0.0 -2.813306  0.004903   \n",
       "          True                       1.408948     0.0 -1.469059  0.141817   \n",
       "Total     NIVOLUMAB                  0.647315     0.0 -3.201686  0.001366   \n",
       "Sex       F                          3.979608     0.0  1.281299  0.200089   \n",
       "          M                          2.038029     0.0  0.412961  0.679635   \n",
       "Age_Group  >75                      53.758028     0.0  1.440400  0.149754   \n",
       "          65-75                      1.973501     0.0 -0.234571  0.814542   \n",
       "          <65                        2.154495     0.0  0.413604  0.679164   \n",
       "Region    REST OF WORLD              1.630060     0.0 -0.830699  0.406144   \n",
       "          US/CANADA                  2.342179     0.0  0.030368  0.975774   \n",
       "          WESTERN EUROPE             3.194224     0.0  1.049544  0.293928   \n",
       "IMDC      FAVORABLE                  3.175908     0.0 -0.460823  0.644925   \n",
       "          INTERMEDIATE               2.150919     0.0  0.329485  0.741789   \n",
       "          NOT REPORTED              33.359891     0.0  1.030140  0.302944   \n",
       "          POOR                       1.504749     0.0 -0.960646  0.336730   \n",
       "Prior_2   False                      2.157831     0.0  0.925578  0.354665   \n",
       "          True                       0.959014     0.0 -2.033712  0.041981   \n",
       "Total     NIVOLUMAB                  1.704357     0.0  0.307140  0.758737   \n",
       "\n",
       "                          -log2(p)  NIVO n  EVERO n class  \n",
       "Sex       F               2.182729      19       22   All  \n",
       "          M               1.676069      60       48   All  \n",
       "Age_Group  >75            1.601394       6        6   All  \n",
       "          65-75           3.595757      25       21   All  \n",
       "          <65             0.917780      48       43   All  \n",
       "Region    REST OF WORLD   2.335751      20       20   All  \n",
       "          US/CANADA       3.354771      32       26   All  \n",
       "          WESTERN EUROPE  0.029642      27       24   All  \n",
       "IMDC      FAVORABLE       1.034371      13       10   All  \n",
       "          INTERMEDIATE    4.079115      46       40   All  \n",
       "          NOT REPORTED    1.722875       3        3   All  \n",
       "          POOR            1.551728      17       17   All  \n",
       "Prior_2   False           2.159186      61       54   All  \n",
       "          True            2.528580      18       16   All  \n",
       "Total     NIVOLUMAB       3.588141      79       70   All  \n",
       "Sex       F               2.934939       9        5   IM+  \n",
       "          M               6.613150      24       24   IM+  \n",
       "Age_Group  >75            0.006628       3        3   IM+  \n",
       "          65-75           3.741948      11        4   IM+  \n",
       "          <65             4.700933      19       22   IM+  \n",
       "Region    REST OF WORLD   0.142377       8        4   IM+  \n",
       "          US/CANADA       7.121185      15       17   IM+  \n",
       "          WESTERN EUROPE  2.381255      10        8   IM+  \n",
       "IMDC      FAVORABLE       1.323077       8        6   IM+  \n",
       "          INTERMEDIATE    8.529262      22       19   IM+  \n",
       "          POOR            1.460572       3        4   IM+  \n",
       "Prior_2   False           7.671973      26       20   IM+  \n",
       "          True            2.817900       7        9   IM+  \n",
       "Total     NIVOLUMAB       9.515556      33       29   IM+  \n",
       "Sex       F               2.321289      10       17   IM-  \n",
       "          M               0.557168      36       24   IM-  \n",
       "Age_Group  >75            2.739331       3        3   IM-  \n",
       "          65-75           0.295939      14       17   IM-  \n",
       "          <65             0.558168      29       21   IM-  \n",
       "Region    REST OF WORLD   1.299938      12       16   IM-  \n",
       "          US/CANADA       0.035382      17        9   IM-  \n",
       "          WESTERN EUROPE  1.766467      17       16   IM-  \n",
       "IMDC      FAVORABLE       0.632796       5        4   IM-  \n",
       "          INTERMEDIATE    0.430919      24       21   IM-  \n",
       "          NOT REPORTED    1.722875       3        3   IM-  \n",
       "          POOR            1.570336      14       13   IM-  \n",
       "Prior_2   False           1.495469      35       34   IM-  \n",
       "          True            4.574133      11        7   IM-  \n",
       "Total     NIVOLUMAB       0.398329      46       41   IM-  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
