{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Sklearn models to train and predict RCC3 labels using Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn_models import run_train_sklearn_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to download data from dropbox and unzip it\n",
    "\n",
    "def download_data_dir(dropbox_url, save_dir='data'):\n",
    "    # Parse the file name from the URL\n",
    "    file_name = dropbox_url.split(\"/\")[-1]\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Send a GET request to the Dropbox URL\n",
    "    response = requests.get(dropbox_url, stream=True)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        zip_path = os.path.join(save_dir, file_name)\n",
    "        # Write the contents of the response to a file\n",
    "        with open(zip_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "    else:\n",
    "        print(f\"Failed to download data from {dropbox_url}. Status code: {response.status_code}\")\n",
    "\n",
    "    # delete the zip file\n",
    "    os.remove(zip_path)\n",
    "    return\n",
    "\n",
    "\n",
    "# pytorch Dataset class for the data\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (and explain) the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dir = 'example_data'\n",
    "example_data_url = 'https://www.dropbox.com/scl/fo/d1yqlmyafvu8tzujlg4nk/h?rlkey=zrxfapacmgb6yxsmonw4yt986&dl=1'\n",
    "# Download the data\n",
    "\n",
    "# download_data_dir(example_data_url, save_dir=subset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nivo Benefit BINARY finetune_folds', '.DS_Store', 'MSKCC BINARY finetune_folds', 'Cohort ID Expanded pretrain_folds', 'readme.txt', 'X.csv', 'y.csv', 'nans.csv']\n"
     ]
    }
   ],
   "source": [
    "#what is inside the example directory?\n",
    "print(os.listdir(subset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X data\n",
    "- the input peak matrix, columns are peaks, rows are samples\n",
    "- nans are already imputed using the mean of the peak within its corresponding study\n",
    "- in this example, the peaks are those determined by alignment\n",
    "- also in this example, we are including a lot of pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = pd.read_csv(os.path.join(subset_dir, 'X.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nan mask\n",
    "- nans is a binary matrix indicating which peaks were originally missing in which samples\n",
    "- columns are the peaks, rows are the samples\n",
    "- should have the exact same size and organization as X data\n",
    "- Since nans were already imputed, this is not used in this example\n",
    "- is useful if you want to change the imputation method, or calculate peak frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_mask = pd.read_csv(os.path.join(subset_dir, 'nans.csv'), index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Y data\n",
    "- the metadata associated with the samples\n",
    "- includes metadata from alignment with pre-training data AND metadata associated with the RCC studies\n",
    "- RCC related metadata has been cleaned and simplified. combines the information between genomics dataset, the the Nature communication dataset. A lot of superfolous information has been removed\n",
    "- the \"Matt Set\" column indicates whether a sample belongs in Matt's Train/Val/Split\n",
    "- the \"Set\" column corresponds to the \"Matt Set\" for the RCC3 *baseline* samples, all other samples are labeled as pretrai\n",
    "- columns have been created specifically for the Binary classification task \"Nivo Benefit BINARY\" and \"MSKCC BINARY\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.read_csv(os.path.join(subset_dir, 'y.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matt Set\n",
       "Train    746\n",
       "Other    425\n",
       "Test     244\n",
       "Val      229\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data['Matt Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set\n",
       "Pretrain      16944\n",
       "Finetune        449\n",
       "Test            149\n",
       "Validation      143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data['Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  17685\n",
      "number of features:  2736\n"
     ]
    }
   ],
   "source": [
    "print('number of samples: ', X_data.shape[0])\n",
    "print('number of features: ', X_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_files = y_data[y_data['Set']=='Pretrain'].index.to_list()\n",
    "finetune_files = y_data[y_data['Set']=='Finetune'].index.to_list()\n",
    "holdout_test_files = y_data[y_data['Set']=='Test'].index.to_list()\n",
    "holdout_val_files = y_data[y_data['Set']=='Validation'].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Stratified Split of the Training data, using single column\n",
    "this can be ignored since the data in the dropbox already has the splits defined\n",
    "Creates a repeated stratified split of the training data to use cross-validation, saved in the \"splits.csv\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf_params = {\n",
    "        'n_splits': 5,\n",
    "        'n_repeats': 10,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "\n",
    "metadata_subset = y_data.loc[finetune_files].copy()\n",
    "yes_remove_nans = True\n",
    "\n",
    "stratify_col = 'Nivo Benefit BINARY'\n",
    "# stratify_col = 'MSKCC BINARY'\n",
    "\n",
    "splits_dir = os.path.join(subset_dir, f'{stratify_col} finetune_folds')\n",
    "os.makedirs(splits_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = RepeatedStratifiedKFold(**rskf_params)\n",
    "\n",
    "rskf_list = []\n",
    "\n",
    "# check for nans\n",
    "if metadata_subset[stratify_col].isna().any():\n",
    "    if yes_remove_nans:\n",
    "        metadata_subset = metadata_subset[~metadata_subset[stratify_col].isna()]\n",
    "    else:\n",
    "        print('There are nans in the stratify column')\n",
    "        fill_val = metadata_subset[stratify_col].max() + 1\n",
    "        metadata_subset[stratify_col] = metadata_subset[stratify_col].fillna(fill_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(rskf.split(metadata_subset.index, metadata_subset[stratify_col])):\n",
    "\n",
    "    test_bool = np.zeros(metadata_subset.shape[0], dtype=bool)\n",
    "    test_bool[test_idx] = True\n",
    "    rskf_list.append(test_bool)\n",
    "\n",
    "rskf_df = pd.DataFrame(index=metadata_subset.index)\n",
    "for i, test_idx in enumerate(rskf_list):\n",
    "    rskf_df[f'fold_{i}'] = test_idx\n",
    "\n",
    "rskf_df.to_csv(os.path.join(splits_dir, 'splits.csv'))\n",
    "\n",
    "rskf_info = {\n",
    "    'rskf_params': rskf_params,\n",
    "    'stratify_col': stratify_col,\n",
    "    'size': rskf_df.shape[0],\n",
    "    'Train Bool' : False,\n",
    "    'Test Bool' : True,\n",
    "    'remove nans': yes_remove_nans\n",
    "}\n",
    "\n",
    "with open(os.path.join(splits_dir, 'splits_info.json'), 'w') as f:\n",
    "    json.dump(rskf_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classical Models on the desired tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nivo Benefit BINARY finetune_folds', 'MSKCC BINARY finetune_folds', \"['MSKCC', 'Benefit'] finetune_folds\", 'Cohort ID Expanded pretrain_folds', \"['Treatment', 'Benefit'] finetune_folds\", \"['MSKCC', 'Treatment', 'Benefit'] finetune_folds\", 'x_finetune_folds']\n"
     ]
    }
   ],
   "source": [
    "# what are the available splits?\n",
    "\n",
    "subset_dir_files = os.listdir(subset_dir)\n",
    "split_dirs = [f for f in subset_dir_files if '_folds' in f]\n",
    "print(split_dirs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the task directory and properly format the X and y for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc3_baseline = y_data.loc[finetune_files].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nivo Benefit BINARY\n",
      "{}\n",
      "example_data/Nivo Benefit BINARY finetune_folds/Nivo Benefit BINARY\n"
     ]
    }
   ],
   "source": [
    "desc_str = 'Nivo Benefit BINARY'\n",
    "yes_dropna = True #drops nan values from the label column\n",
    "\n",
    "# finetune_label_col = 'Benefit'\n",
    "# finetune_label_mapper  = {'CB': 1, 'NCB': 0, 'ICB': np.nan}\n",
    "# finetune_filter = rcc3_baseline[rcc3_baseline['Treatment'].isin(['NIVOLUMAB'])].index.to_list()\n",
    "\n",
    "\n",
    "# finetune_label_col = 'MSKCC'\n",
    "# finetune_label_mapper  = {'FAVORABLE': 1, 'POOR': 0, 'INTERMEDIATE': np.nan}\n",
    "# finetune_filter = rcc3_baseline.index.to_list()\n",
    "\n",
    "finetune_label_col = 'Nivo Benefit BINARY'\n",
    "finetune_label_mapper = {}\n",
    "finetune_filter = []\n",
    "\n",
    "splits_dir = os.path.join(subset_dir, f'{desc_str} finetune_folds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(finetune_label_col)\n",
    "print(finetune_label_mapper)\n",
    "\n",
    "\n",
    "task_dir = os.path.join(splits_dir, finetune_label_col)\n",
    "\n",
    "splits_df = pd.read_csv(os.path.join(splits_dir, 'splits.csv'), index_col=0)\n",
    "if len(finetune_filter) > 0:\n",
    "    print('number of samples after the finetune filter:' , len(finetune_filter))\n",
    "    splits_df = splits_df.loc[finetune_filter].copy()\n",
    "\n",
    "X = X_data.loc[splits_df.index]\n",
    "y = y_data.loc[splits_df.index]\n",
    "# nans = nan_mask.loc[splits_df.index].astype(bool)\n",
    "\n",
    "n_folds = splits_df.shape[1]\n",
    "\n",
    "task_info = {\n",
    "    'label_col': finetune_label_col,\n",
    "    'label_mapper': finetune_label_mapper,\n",
    "    'filter': finetune_filter,\n",
    "    'desc_str': desc_str,\n",
    "    'size': splits_df.shape[0],\n",
    "    'folds': splits_df.shape[1],\n",
    "    'dropna': yes_dropna,\n",
    "}\n",
    "\n",
    "if finetune_label_mapper:\n",
    "    task_info['size by label'] =  y[finetune_label_col].map(finetune_label_mapper).value_counts().to_dict(),\n",
    "else:\n",
    "    task_info['size by label'] =  y[finetune_label_col].value_counts().to_dict()\n",
    "\n",
    "os.makedirs(task_dir, exist_ok=True)\n",
    "with open(os.path.join(task_dir, 'task_info.json'), 'w') as f:\n",
    "    json.dump(task_info, f, indent=4)\n",
    "\n",
    "print(task_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cycle over the different models and run repeated cross-validation\n",
    "\n",
    "NOTE: We are computed a weighted AUROC by default, weighted by class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping nan values in the y column\n",
      "number of samples after dropping nan values in the y column:  156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/jonaheaton/opt/miniconda3/envs/mzlearn_3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 14 is smaller than n_iter=100. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_default_summary.csv\n",
      "logistic_regression_default_summary.csv\n",
      "logistic_regression_optimal_summary.csv\n",
      "random_forest_default_summary.csv\n"
     ]
    }
   ],
   "source": [
    "delete_other_files = False #delete the files generated from each fitting in the repeated Cross-validation\n",
    "# optimization_desc = 'default' #use sklearn's default hyperparameters\n",
    "optimization_desc = 'optimal' # use the default hyperparameter gridsearch specified in run_train_sklearn_model\n",
    "\n",
    "# which_models = ['logistic_regression', 'random_forest', 'svc']\n",
    "which_models = ['logistic_regression']\n",
    "\n",
    "\n",
    "output_dir = os.path.join(task_dir, 'classical_models')\n",
    "if finetune_label_mapper:\n",
    "    y_values = y[finetune_label_col].map(finetune_label_mapper)\n",
    "else:\n",
    "    y_values = y[finetune_label_col]\n",
    "    \n",
    "if yes_dropna:\n",
    "    print('dropping nan values in the y column')\n",
    "    y_values = y_values.dropna()\n",
    "    X = X.loc[y_values.index]\n",
    "    splits_df = splits_df.loc[y_values.index]\n",
    "\n",
    "    print('number of samples after dropping nan values in the y column: ', y_values.shape[0])    \n",
    "\n",
    "for model_kind in which_models:\n",
    "    gather_output = []\n",
    "    model_name = model_kind + '_' + optimization_desc\n",
    "    output_summary_file = os.path.join(output_dir, f'{model_name}_summary.csv')\n",
    "    \n",
    "    if optimization_desc == 'default':\n",
    "        param_grid = {}\n",
    "    elif optimization_desc == 'optimal':\n",
    "        param_grid = None # use the default hyperparameter gridsearch specified in run_train_sklearn_model\n",
    "\n",
    "    for subset_id in range(n_folds):\n",
    "\n",
    "\n",
    "\n",
    "        train_idx = splits_df.index[splits_df[f'fold_{subset_id}'] == False]\n",
    "        test_idx = splits_df.index[splits_df[f'fold_{subset_id}'] == True]\n",
    "        finetune_dataset = SimpleDataset(X.loc[train_idx], y_values.loc[train_idx])\n",
    "        \n",
    "        class_weights = 1 / torch.bincount(finetune_dataset.y.long())\n",
    "\n",
    "        test_dataset = SimpleDataset(X.loc[test_idx], y_values.loc[test_idx])\n",
    "\n",
    "        data_dict = {'train': finetune_dataset, 'CV': test_dataset}\n",
    "\n",
    "        #if you want to add the validation set to the final evaluation\n",
    "        # data_dict['val'] = SimpleDataset(X_data.loc[val_hold_idx], y_data.loc[val_hold_idx, finetune_label_col])\n",
    "\n",
    "        ##### ##### ##### ##### ##### ##### ##### ##### ##### #####\n",
    "        ##### Here is the code that actually runs the model optimization #####\n",
    "        output_data = run_train_sklearn_model(data_dict,output_dir,\n",
    "                                model_kind = model_kind,\n",
    "                                model_name=f'{model_name}_{subset_id}',\n",
    "                                param_grid=param_grid\n",
    "                                )\n",
    "        ##### ##### ##### ##### ##### ##### ##### ##### ##### #####\n",
    "        \n",
    "\n",
    "        gather_output.append(output_data)\n",
    "\n",
    "    ## Summarize the important results\n",
    "    # best_epoch_list = [output['best_epoch'] for output in gather_output]\n",
    "        \n",
    "    auc_summary_dct = {}\n",
    "    for phase in data_dict.keys():\n",
    "        auc_summary_dct[phase + ' AUROC'] = [output['end_state_auroc'][phase] for output in gather_output]\n",
    "\n",
    "    # cv_auroc_list = [output['end_state_auroc']['CV'] for output in gather_output]\n",
    "    # train_auroc_list = [output['end_state_auroc']['train'] for output in gather_output]\n",
    "    # model_name = gather_output[0]['model_name']\n",
    "    auc_summary = pd.DataFrame(auc_summary_dct)\n",
    "\n",
    "\n",
    "    result_summary_avg = auc_summary.mean()\n",
    "    result_summary_std = auc_summary.std()\n",
    "    result_summary_avg.index = [f'AVG {col}' for col in result_summary_avg.index]\n",
    "    result_summary_std.index = [f'STD {col}' for col in result_summary_std.index]\n",
    "    result_summary = pd.concat([result_summary_avg, result_summary_std])\n",
    "    result_summary.to_csv(output_summary_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_files = os.listdir(output_dir)\n",
    "output_summary_files = [f for f in output_files if f.endswith('summary.csv')]\n",
    "other_files = [f for f in output_files if f not in output_summary_files]\n",
    "\n",
    "all_res = []\n",
    "\n",
    "for f in output_summary_files:\n",
    "    print(f)\n",
    "    model_name = f.split('_summary.csv')[0]\n",
    "    res = pd.read_csv(os.path.join(output_dir, f), index_col=0)\n",
    "    res.columns = [model_name]\n",
    "    all_res.append(res)\n",
    "    \n",
    "# delete the other files to save room\n",
    "if delete_other_files:\n",
    "    for f in other_files:\n",
    "        os.remove(os.path.join(output_dir, f))    \n",
    "\n",
    "res_summary = pd.concat(all_res, axis=1)    \n",
    "res_summary = res_summary.round(4)\n",
    "res_summary.to_csv(os.path.join(task_dir, 'classical_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svc_default</th>\n",
       "      <th>logistic_regression_default</th>\n",
       "      <th>logistic_regression_optimal</th>\n",
       "      <th>random_forest_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AVG train AUROC</th>\n",
       "      <td>0.980</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG CV AUROC</th>\n",
       "      <td>0.646</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD train AUROC</th>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD CV AUROC</th>\n",
       "      <td>0.090</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 svc_default  logistic_regression_default  \\\n",
       "AVG train AUROC        0.980                        1.000   \n",
       "AVG CV AUROC           0.646                        0.655   \n",
       "STD train AUROC        0.141                        0.000   \n",
       "STD CV AUROC           0.090                        0.074   \n",
       "\n",
       "                 logistic_regression_optimal  random_forest_default  \n",
       "AVG train AUROC                        0.998                  1.000  \n",
       "AVG CV AUROC                           0.634                  0.629  \n",
       "STD train AUROC                        0.008                  0.000  \n",
       "STD CV AUROC                           0.080                  0.074  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_summary.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzlearn_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
