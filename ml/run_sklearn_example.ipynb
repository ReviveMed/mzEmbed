{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Sklearn models to train and predict RCC3 labels using Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn_models import run_train_sklearn_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to download data from dropbox and unzip it\n",
    "\n",
    "def download_data_dir(dropbox_url, save_dir='data'):\n",
    "    # Parse the file name from the URL\n",
    "    file_name = dropbox_url.split(\"/\")[-1]\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Send a GET request to the Dropbox URL\n",
    "    response = requests.get(dropbox_url, stream=True)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        zip_path = os.path.join(save_dir, file_name)\n",
    "        # Write the contents of the response to a file\n",
    "        with open(zip_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "    else:\n",
    "        print(f\"Failed to download data from {dropbox_url}. Status code: {response.status_code}\")\n",
    "\n",
    "    # delete the zip file\n",
    "    os.remove(zip_path)\n",
    "    return\n",
    "\n",
    "\n",
    "# pytorch Dataset class for the data\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (and explain) the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_dir = '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/alignment_RCC_2024_Feb_27/March_8_Data'\n",
    "subset_dir = '/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/alignment_RCC_2024_Feb_27/March_6_Data'\n",
    "# subset_dir = 'example_data'\n",
    "example_data_url = 'https://www.dropbox.com/scl/fo/d1yqlmyafvu8tzujlg4nk/h?rlkey=zrxfapacmgb6yxsmonw4yt986&dl=1'\n",
    "# Download the data\n",
    "\n",
    "# download_data_dir(example_data_url, save_dir=subset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nivo Benefit BINARY finetune_folds', '.DS_Store', 'MSKCC BINARY finetune_folds', 'Cohort ID Expanded pretrain_folds', 'readme.txt', 'X.csv', 'y.csv', 'nans.csv']\n"
     ]
    }
   ],
   "source": [
    "#what is inside the example directory?\n",
    "print(os.listdir(subset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X data\n",
    "- the input peak matrix, columns are peaks, rows are samples\n",
    "- nans are already imputed using the mean of the peak within its corresponding study\n",
    "- in this example, the peaks are those determined by alignment\n",
    "- also in this example, we are including a lot of pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = pd.read_csv(os.path.join(subset_dir, 'X.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nan mask\n",
    "- nans is a binary matrix indicating which peaks were originally missing in which samples\n",
    "- columns are the peaks, rows are the samples\n",
    "- should have the exact same size and organization as X data\n",
    "- Since nans were already imputed, this is not used in this example\n",
    "- is useful if you want to change the imputation method, or calculate peak frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_mask = pd.read_csv(os.path.join(subset_dir, 'nans.csv'), index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Y data\n",
    "- the metadata associated with the samples\n",
    "- includes metadata from alignment with pre-training data AND metadata associated with the RCC studies\n",
    "- RCC related metadata has been cleaned and simplified. combines the information between genomics dataset, the the Nature communication dataset. A lot of superfolous information has been removed\n",
    "- the \"Matt Set\" column indicates whether a sample belongs in Matt's Train/Val/Split\n",
    "- the \"Set\" column corresponds to the \"Matt Set\" for the RCC3 *baseline* samples, all other samples are labeled as pretrai\n",
    "- columns have been created specifically for the Binary classification task \"Nivo Benefit BINARY\" and \"MSKCC BINARY\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.read_csv(os.path.join(subset_dir, 'y.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matt Set\n",
       "Train    746\n",
       "Other    425\n",
       "Test     244\n",
       "Val      229\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data['Matt Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set\n",
       "Pretrain      16944\n",
       "Finetune        449\n",
       "Test            149\n",
       "Validation      143\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data['Set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  17685\n",
      "number of features:  2736\n"
     ]
    }
   ],
   "source": [
    "print('number of samples: ', X_data.shape[0])\n",
    "print('number of features: ', X_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_files = y_data[y_data['Set']=='Pretrain'].index.to_list()\n",
    "finetune_files = y_data[y_data['Set']=='Finetune'].index.to_list()\n",
    "holdout_test_files = y_data[y_data['Set']=='Test'].index.to_list()\n",
    "holdout_val_files = y_data[y_data['Set']=='Validation'].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Stratified Split of the Training data, using single column\n",
    "this can be ignored since the data in the dropbox already has the splits defined\n",
    "Creates a repeated stratified split of the training data to use cross-validation, saved in the \"splits.csv\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf_params = {\n",
    "        'n_splits': 5,\n",
    "        'n_repeats': 10,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "\n",
    "metadata_subset = y_data.loc[finetune_files].copy()\n",
    "yes_remove_nans = True\n",
    "\n",
    "stratify_col = 'Nivo Benefit BINARY'\n",
    "# stratify_col = 'MSKCC BINARY'\n",
    "\n",
    "splits_dir = os.path.join(subset_dir, f'{stratify_col} finetune_folds')\n",
    "os.makedirs(splits_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = RepeatedStratifiedKFold(**rskf_params)\n",
    "\n",
    "rskf_list = []\n",
    "\n",
    "# check for nans\n",
    "if metadata_subset[stratify_col].isna().any():\n",
    "    if yes_remove_nans:\n",
    "        metadata_subset = metadata_subset[~metadata_subset[stratify_col].isna()]\n",
    "    else:\n",
    "        print('There are nans in the stratify column')\n",
    "        fill_val = metadata_subset[stratify_col].max() + 1\n",
    "        metadata_subset[stratify_col] = metadata_subset[stratify_col].fillna(fill_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(rskf.split(metadata_subset.index, metadata_subset[stratify_col])):\n",
    "\n",
    "    test_bool = np.zeros(metadata_subset.shape[0], dtype=bool)\n",
    "    test_bool[test_idx] = True\n",
    "    rskf_list.append(test_bool)\n",
    "\n",
    "rskf_df = pd.DataFrame(index=metadata_subset.index)\n",
    "for i, test_idx in enumerate(rskf_list):\n",
    "    rskf_df[f'fold_{i}'] = test_idx\n",
    "\n",
    "rskf_df.to_csv(os.path.join(splits_dir, 'splits.csv'))\n",
    "\n",
    "rskf_info = {\n",
    "    'rskf_params': rskf_params,\n",
    "    'stratify_col': stratify_col,\n",
    "    'size': rskf_df.shape[0],\n",
    "    'Train Bool' : False,\n",
    "    'Test Bool' : True,\n",
    "    'remove nans': yes_remove_nans\n",
    "}\n",
    "\n",
    "with open(os.path.join(splits_dir, 'splits_info.json'), 'w') as f:\n",
    "    json.dump(rskf_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classical Models on the desired tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nivo Benefit BINARY finetune_folds', 'MSKCC BINARY finetune_folds', 'Cohort ID Expanded pretrain_folds']\n"
     ]
    }
   ],
   "source": [
    "# what are the available splits?\n",
    "\n",
    "subset_dir_files = os.listdir(subset_dir)\n",
    "split_dirs = [f for f in subset_dir_files if '_folds' in f]\n",
    "print(split_dirs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the task directory and properly format the X and y for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc3_baseline = y_data.loc[finetune_files].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSKCC BINARY\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "yes_dropna = True #drops nan values from the label column\n",
    "\n",
    "# finetune_label_col = 'Benefit'\n",
    "# finetune_label_mapper  = {'CB': 1, 'NCB': 0, 'ICB': np.nan}\n",
    "# finetune_filter = rcc3_baseline[rcc3_baseline['Treatment'].isin(['NIVOLUMAB'])].index.to_list()\n",
    "\n",
    "\n",
    "# finetune_label_col = 'MSKCC'\n",
    "# finetune_label_mapper  = {'FAVORABLE': 1, 'POOR': 0, 'INTERMEDIATE': np.nan}\n",
    "# finetune_filter = rcc3_baseline.index.to_list()\n",
    "\n",
    "finetune_label_col = 'MSKCC BINARY'\n",
    "finetune_label_mapper = {}\n",
    "finetune_filter = []\n",
    "\n",
    "\n",
    "# finetune_label_col = 'Nivo Benefit BINARY'\n",
    "# finetune_label_mapper = {}\n",
    "# finetune_filter = []\n",
    "\n",
    "# desc_str = 'Nivo Benefit BINARY'\n",
    "desc_str = 'MSKCC BINARY'\n",
    "desc_str = finetune_label_col\n",
    "\n",
    "splits_dir = os.path.join(subset_dir, f'{desc_str} finetune_folds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(finetune_label_col)\n",
    "print(finetune_label_mapper)\n",
    "\n",
    "\n",
    "task_dir = os.path.join(splits_dir, finetune_label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jonaheaton/ReviveMed Dropbox/Jonah Eaton/development_CohortCombination/alignment_RCC_2024_Feb_27/March_6_Data/MSKCC BINARY finetune_folds/MSKCC BINARY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits_df = pd.read_csv(os.path.join(splits_dir, 'splits.csv'), index_col=0)\n",
    "if len(finetune_filter) > 0:\n",
    "    print('number of samples after the finetune filter:' , len(finetune_filter))\n",
    "    splits_df = splits_df.loc[finetune_filter].copy()\n",
    "\n",
    "X = X_data.loc[splits_df.index]\n",
    "y = y_data.loc[splits_df.index]\n",
    "# nans = nan_mask.loc[splits_df.index].astype(bool)\n",
    "\n",
    "n_folds = splits_df.shape[1]\n",
    "\n",
    "task_info = {\n",
    "    'label_col': finetune_label_col,\n",
    "    'label_mapper': finetune_label_mapper,\n",
    "    'filter': finetune_filter,\n",
    "    'desc_str': desc_str,\n",
    "    'size': splits_df.shape[0],\n",
    "    'folds': splits_df.shape[1],\n",
    "    'dropna': yes_dropna,\n",
    "}\n",
    "\n",
    "if finetune_label_mapper:\n",
    "    task_info['size by label'] =  y[finetune_label_col].map(finetune_label_mapper).value_counts().to_dict(),\n",
    "else:\n",
    "    task_info['size by label'] =  y[finetune_label_col].value_counts().to_dict()\n",
    "\n",
    "os.makedirs(task_dir, exist_ok=True)\n",
    "with open(os.path.join(task_dir, 'task_info.json'), 'w') as f:\n",
    "    json.dump(task_info, f, indent=4)\n",
    "\n",
    "print(task_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cycle over the different models and run repeated cross-validation\n",
    "\n",
    "NOTE: We are computed a weighted AUROC by default, weighted by class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hold_idx = holdout_val_files\n",
    "Xval = X_data.loc[val_hold_idx]\n",
    "yval = y_data.loc[val_hold_idx, finetune_label_col]\n",
    "if yes_dropna:\n",
    "    yval = yval.dropna()\n",
    "    Xval = Xval.loc[yval.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping nan values in the y column\n",
      "number of samples after dropping nan values in the y column:  240\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(task_dir, 'classical_models')\n",
    "if finetune_label_mapper:\n",
    "    y_values = y[finetune_label_col].map(finetune_label_mapper)\n",
    "else:\n",
    "    y_values = y[finetune_label_col]\n",
    "    \n",
    "if yes_dropna:\n",
    "    print('dropping nan values in the y column')\n",
    "    y_values = y_values.dropna()\n",
    "    X = X.loc[y_values.index]\n",
    "    splits_df = splits_df.loc[y_values.index]\n",
    "\n",
    "    print('number of samples after dropping nan values in the y column: ', y_values.shape[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping nan values in the y column\n",
      "number of samples after dropping nan values in the y column:  156\n",
      "svc_default_summary.csv\n",
      "logistic_regression_default_summary.csv\n",
      "random_forest_default_summary.csv\n"
     ]
    }
   ],
   "source": [
    "delete_other_files = False #delete the files generated from each fitting in the repeated Cross-validation\n",
    "optimization_desc = 'default' #use sklearn's default hyperparameters\n",
    "# optimization_desc = 'optimal' # use the default hyperparameter gridsearch specified in run_train_sklearn_model\n",
    "\n",
    "which_models = ['logistic_regression', 'random_forest', 'svc']\n",
    "# which_models = ['logistic_regression']\n",
    "\n",
    "\n",
    "for model_kind in which_models:\n",
    "    gather_output = []\n",
    "    model_name = model_kind + '_' + optimization_desc\n",
    "    output_summary_file = os.path.join(output_dir, f'{model_name}_summary.csv')\n",
    "    \n",
    "    if optimization_desc == 'default':\n",
    "        param_grid = {}\n",
    "    elif optimization_desc == 'optimal':\n",
    "        param_grid = None # use the default hyperparameter gridsearch specified in run_train_sklearn_model\n",
    "\n",
    "    for subset_id in range(n_folds):\n",
    "\n",
    "\n",
    "\n",
    "        train_idx = splits_df.index[splits_df[f'fold_{subset_id}'] == False]\n",
    "        test_idx = splits_df.index[splits_df[f'fold_{subset_id}'] == True]\n",
    "        finetune_dataset = SimpleDataset(X.loc[train_idx], y_values.loc[train_idx])\n",
    "        \n",
    "        class_weights = 1 / torch.bincount(finetune_dataset.y.long())\n",
    "\n",
    "        test_dataset = SimpleDataset(X.loc[test_idx], y_values.loc[test_idx])\n",
    "\n",
    "        data_dict = {'train': finetune_dataset, 'CV': test_dataset}\n",
    "\n",
    "        #if you want to add the validation set to the final evaluation\n",
    "        data_dict['val'] = SimpleDataset(Xval, yval)\n",
    "\n",
    "        ##### ##### ##### ##### ##### ##### ##### ##### ##### #####\n",
    "        ##### Here is the code that actually runs the model optimization #####\n",
    "        output_data = run_train_sklearn_model(data_dict,output_dir,\n",
    "                                model_kind = model_kind,\n",
    "                                model_name=f'{model_name}_{subset_id}',\n",
    "                                param_grid=param_grid\n",
    "                                )\n",
    "        ##### ##### ##### ##### ##### ##### ##### ##### ##### #####\n",
    "        \n",
    "\n",
    "        gather_output.append(output_data)\n",
    "\n",
    "    ## Summarize the important results\n",
    "    # best_epoch_list = [output['best_epoch'] for output in gather_output]\n",
    "        \n",
    "    auc_summary_dct = {}\n",
    "    for phase in data_dict.keys():\n",
    "        auc_summary_dct[phase + ' AUROC'] = [output['end_state_auroc'][phase] for output in gather_output]\n",
    "\n",
    "    # cv_auroc_list = [output['end_state_auroc']['CV'] for output in gather_output]\n",
    "    # train_auroc_list = [output['end_state_auroc']['train'] for output in gather_output]\n",
    "    # model_name = gather_output[0]['model_name']\n",
    "    auc_summary = pd.DataFrame(auc_summary_dct)\n",
    "\n",
    "\n",
    "    result_summary_avg = auc_summary.mean()\n",
    "    result_summary_std = auc_summary.std()\n",
    "    result_summary_avg.index = [f'AVG {col}' for col in result_summary_avg.index]\n",
    "    result_summary_std.index = [f'STD {col}' for col in result_summary_std.index]\n",
    "    result_summary = pd.concat([result_summary_avg, result_summary_std])\n",
    "    result_summary.to_csv(output_summary_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_files = os.listdir(output_dir)\n",
    "output_summary_files = [f for f in output_files if f.endswith('summary.csv')]\n",
    "other_files = [f for f in output_files if f not in output_summary_files]\n",
    "\n",
    "all_res = []\n",
    "\n",
    "for f in output_summary_files:\n",
    "    print(f)\n",
    "    model_name = f.split('_summary.csv')[0]\n",
    "    res = pd.read_csv(os.path.join(output_dir, f), index_col=0)\n",
    "    res.columns = [model_name]\n",
    "    all_res.append(res)\n",
    "    \n",
    "# delete the other files to save room\n",
    "if delete_other_files:\n",
    "    for f in other_files:\n",
    "        os.remove(os.path.join(output_dir, f))    \n",
    "\n",
    "res_summary = pd.concat(all_res, axis=1)    \n",
    "res_summary = res_summary.round(4)\n",
    "res_summary.to_csv(os.path.join(task_dir, 'classical_summary.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression_default val val auc: 0.8607871532440186\n",
      "random_forest_default val val auc: 0.931851327419281\n",
      "svc_default val val auc: 0.9256560802459717\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "data_dict['train'] = SimpleDataset(X, y_values)\n",
    "data_dict['test'] = SimpleDataset(Xval, yval)\n",
    "\n",
    "for model_kind in which_models:\n",
    "\n",
    "        model_name = model_kind + '_' + optimization_desc +' val'\n",
    "\n",
    "        output_data = run_train_sklearn_model(data_dict,output_dir,\n",
    "                                model_kind = model_kind,\n",
    "                                model_name=f'{model_name}',\n",
    "                                param_grid=param_grid\n",
    "                                )\n",
    "        \n",
    "        valauc = output_data['end_state_auroc']['test']\n",
    "        print(f'{model_name} val auc: {valauc}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SimpleDataset at 0x7fc8fd402070>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svc_default</th>\n",
       "      <th>logistic_regression_default</th>\n",
       "      <th>logistic_regression_optimal</th>\n",
       "      <th>random_forest_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AVG train AUROC</th>\n",
       "      <td>0.980</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG CV AUROC</th>\n",
       "      <td>0.646</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD train AUROC</th>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD CV AUROC</th>\n",
       "      <td>0.090</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 svc_default  logistic_regression_default  \\\n",
       "AVG train AUROC        0.980                        1.000   \n",
       "AVG CV AUROC           0.646                        0.655   \n",
       "STD train AUROC        0.141                        0.000   \n",
       "STD CV AUROC           0.090                        0.074   \n",
       "\n",
       "                 logistic_regression_optimal  random_forest_default  \n",
       "AVG train AUROC                        0.998                  1.000  \n",
       "AVG CV AUROC                           0.634                  0.629  \n",
       "STD train AUROC                        0.008                  0.000  \n",
       "STD CV AUROC                           0.080                  0.074  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_summary.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train AUROC</th>\n",
       "      <th>CV AUROC</th>\n",
       "      <th>val AUROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.891841</td>\n",
       "      <td>0.908163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857685</td>\n",
       "      <td>0.919096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973435</td>\n",
       "      <td>0.924198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.884250</td>\n",
       "      <td>0.934402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.930029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.927114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920304</td>\n",
       "      <td>0.910350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.861480</td>\n",
       "      <td>0.931487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872865</td>\n",
       "      <td>0.917638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845703</td>\n",
       "      <td>0.928572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.924927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891841</td>\n",
       "      <td>0.924198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950664</td>\n",
       "      <td>0.922741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897533</td>\n",
       "      <td>0.934402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.917639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874763</td>\n",
       "      <td>0.922741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.880455</td>\n",
       "      <td>0.912536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933586</td>\n",
       "      <td>0.924927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.767578</td>\n",
       "      <td>0.916910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.918406</td>\n",
       "      <td>0.932945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.833017</td>\n",
       "      <td>0.915452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.920304</td>\n",
       "      <td>0.911808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.878558</td>\n",
       "      <td>0.913265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.927843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914611</td>\n",
       "      <td>0.911079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.917639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950664</td>\n",
       "      <td>0.923469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899431</td>\n",
       "      <td>0.922012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.806641</td>\n",
       "      <td>0.935860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.943074</td>\n",
       "      <td>0.917639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787476</td>\n",
       "      <td>0.919096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.905123</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.919096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.892578</td>\n",
       "      <td>0.924198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908918</td>\n",
       "      <td>0.930029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795066</td>\n",
       "      <td>0.924927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891841</td>\n",
       "      <td>0.916910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922201</td>\n",
       "      <td>0.920554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.910156</td>\n",
       "      <td>0.918367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927894</td>\n",
       "      <td>0.920554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.914611</td>\n",
       "      <td>0.922012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.848197</td>\n",
       "      <td>0.918367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.844402</td>\n",
       "      <td>0.920554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893555</td>\n",
       "      <td>0.920554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893738</td>\n",
       "      <td>0.911808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>0.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.999642</td>\n",
       "      <td>0.878558</td>\n",
       "      <td>0.924927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922201</td>\n",
       "      <td>0.922012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900391</td>\n",
       "      <td>0.935131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train AUROC  CV AUROC  val AUROC\n",
       "0      0.999881  0.891841   0.908163\n",
       "1      1.000000  0.857685   0.919096\n",
       "2      1.000000  0.973435   0.924198\n",
       "3      0.999761  0.884250   0.934402\n",
       "4      0.999881  0.882812   0.930029\n",
       "5      1.000000  0.941176   0.927114\n",
       "6      1.000000  0.920304   0.910350\n",
       "7      0.999761  0.861480   0.931487\n",
       "8      1.000000  0.872865   0.917638\n",
       "9      1.000000  0.845703   0.928572\n",
       "10     1.000000  0.882353   0.924927\n",
       "11     1.000000  0.891841   0.924198\n",
       "12     1.000000  0.950664   0.922741\n",
       "13     1.000000  0.897533   0.934402\n",
       "14     1.000000  0.791016   0.917639\n",
       "15     1.000000  0.874763   0.922741\n",
       "16     0.999881  0.880455   0.912536\n",
       "17     0.999881  0.876660   0.928571\n",
       "18     1.000000  0.933586   0.924927\n",
       "19     1.000000  0.767578   0.916910\n",
       "20     0.999761  0.918406   0.932945\n",
       "21     0.999881  0.833017   0.915452\n",
       "22     0.999881  0.920304   0.911808\n",
       "23     1.000000  0.878558   0.913265\n",
       "24     1.000000  0.851562   0.927843\n",
       "25     1.000000  0.914611   0.911079\n",
       "26     1.000000  0.838710   0.917639\n",
       "27     1.000000  0.950664   0.923469\n",
       "28     1.000000  0.899431   0.922012\n",
       "29     0.999644  0.806641   0.935860\n",
       "30     0.999881  0.943074   0.917639\n",
       "31     1.000000  0.787476   0.919096\n",
       "32     0.999881  0.905123   0.928571\n",
       "33     1.000000  0.882353   0.919096\n",
       "34     0.999881  0.892578   0.924198\n",
       "35     1.000000  0.908918   0.930029\n",
       "36     1.000000  0.795066   0.924927\n",
       "37     1.000000  0.891841   0.916910\n",
       "38     1.000000  0.922201   0.920554\n",
       "39     0.999763  0.910156   0.918367\n",
       "40     1.000000  0.927894   0.920554\n",
       "41     0.999761  0.914611   0.922012\n",
       "42     0.999761  0.848197   0.918367\n",
       "43     0.999881  0.844402   0.920554\n",
       "44     1.000000  0.893555   0.920554\n",
       "45     1.000000  0.893738   0.911808\n",
       "46     1.000000  0.876660   0.929300\n",
       "47     0.999642  0.878558   0.924927\n",
       "48     1.000000  0.922201   0.922012\n",
       "49     1.000000  0.900391   0.935131"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to check if a variable is a function\n",
    "callable(run_train_sklearn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(subset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17685, 2736)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FT10031   -0.002666\n",
       "FT10035   -0.005164\n",
       "FT10037   -0.003603\n",
       "FT10039   -0.004585\n",
       "FT10041   -0.007062\n",
       "             ...   \n",
       "FT9985    -0.007561\n",
       "FT9988    -0.006673\n",
       "FT9989    -0.004516\n",
       "FT9997    -0.006612\n",
       "FT9999    -0.008210\n",
       "Length: 2736, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.sum(axis=0)/X_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzlearn_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
